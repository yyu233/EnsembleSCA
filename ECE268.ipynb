{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyu233/EnsembleSCA/blob/268%2Ffeature%2Fyyz/ECE268.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GPYNTwhBDO6Q",
        "outputId": "be55479d-47fb-4e8c-c6f7-7ed1a6fc93d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX9Xc20ZEHUM",
        "outputId": "25e9956c-3a1c-4539-8980-2473ae06c6c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4080673294754445210\n",
              " xla_global_id: -1, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14385217536\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 2543479344270352745\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMZ2sUpGFhDB",
        "outputId": "3ad2ff7d-bbf1-418c-8700-d1c479de3aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia_smi\n",
            "  Downloading nvidia_smi-0.1.3-py36-none-any.whl (11 kB)\n",
            "Collecting sorcery>=0.1.0\n",
            "  Downloading sorcery-0.2.2-py3-none-any.whl (16 kB)\n",
            "Collecting pytest>=4.3.1\n",
            "  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nvidia_smi) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from nvidia_smi) (1.15.0)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytest>=4.3.1->nvidia_smi) (23.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=4.3.1->nvidia_smi) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=4.3.1->nvidia_smi) (22.2.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting littleutils>=0.2.1\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting asttokens\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting executing\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/dist-packages (from sorcery>=0.1.0->nvidia_smi) (1.15.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=c541c9999bb3ad49ede3d422c01b7f750f2f943fde13c5815cf85f9e420caa73\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, executing, pluggy, iniconfig, exceptiongroup, asttokens, sorcery, pytest, nvidia_smi\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed asttokens-2.2.1 exceptiongroup-1.1.1 executing-1.2.0 iniconfig-2.0.0 littleutils-0.2.2 nvidia_smi-0.1.3 pluggy-1.0.0 pytest-7.2.2 sorcery-0.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install nvidia_smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQSnpgH8FsYH",
        "outputId": "a3e02c38-1260-445e-84a6-5909fb73fcb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Mar 17 06:16:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    33W /  70W |    333MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW_Sh_BKF7m9",
        "outputId": "86b27c65-bf1f-49ac-af77-15b1a96fc0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ASCAD'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 161 (delta 46), reused 48 (delta 28), pack-reused 88\u001b[K\n",
            "Receiving objects: 100% (161/161), 181.72 KiB | 15.14 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yyu233/ASCAD.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHf-IgzMGQQg",
        "outputId": "50a94e78-8901-47c5-ecae-e97cb7167275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASCAD\t     ensemble_aes.py   neural_networks.py  sca_metrics.py\n",
            "datasets.py  load_datasets.py  __pycache__\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI-uWQlyGz5r",
        "outputId": "6e909dde-e80d-4df5-dcf8-119fb0ebd4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/EnsembleSCA/commons/ASCAD\n",
            "/content/EnsembleSCA/commons/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key\n",
            "--2023-03-17 06:16:27--  https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip\n",
            "Resolving www.data.gouv.fr (www.data.gouv.fr)... 37.59.183.93\n",
            "Connecting to www.data.gouv.fr (www.data.gouv.fr)|37.59.183.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4435199469 (4.1G) [application/zip]\n",
            "Saving to: ‘ASCAD_data.zip’\n",
            "\n",
            "ASCAD_data.zip        2%[                    ] 116.98M  11.3MB/s    eta 6m 58s ^C\n",
            "Archive:  ASCAD_data.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ASCAD_data.zip or\n",
            "        ASCAD_data.zip.zip, and cannot find ASCAD_data.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "%cd ASCAD/\n",
        "%cd ATMEGA_AES_v1/ATM_AES_v1_fixed_key/\n",
        "!wget https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip\n",
        "!unzip ASCAD_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqS438GxH5t6",
        "outputId": "a6d3275e-b8b2-40b4-b9df-8aeede76431f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASCAD_data.zip\t\t example_test_models_params   Readme.md\n",
            "example_generate_params  example_train_models_params\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcU7htjfKhz2",
        "outputId": "de10816e-e49b-4ee9-f7d5-e09d3d40d062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.11.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqaMM3yRMNLw",
        "outputId": "32ccdc26-8489-46e4-b8b9-0eda1eb81d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: keras\n",
            "Version: 2.11.0\n",
            "Summary: Deep learning for humans.\n",
            "Home-page: https://keras.io/\n",
            "Author: Keras team\n",
            "Author-email: keras-users@googlegroups.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: \n",
            "Required-by: keras-vis, tensorflow\n"
          ]
        }
      ],
      "source": [
        "!pip show keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvCxnfSJMvo1",
        "outputId": "5f63313e-0d55-4793-c7a4-9370f988538a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy h5py matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DO9zsxmNuR5",
        "outputId": "4c6ee7ef-0ae7-4cb9-ef54-9eda83d1a05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfoeL7lpNvfe"
      },
      "outputs": [],
      "source": [
        "!pip show h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxoZiaR-O80Z"
      },
      "outputs": [],
      "source": [
        "!pip show numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0G-tLU1Ys4i"
      },
      "outputs": [],
      "source": [
        "%cd ../../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boirL3IDZNFj"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEtWBzTdEA65"
      },
      "outputs": [],
      "source": [
        "%cd ASCAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax6eDeMIhTl2"
      },
      "outputs": [],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsDSba0ehwB7"
      },
      "outputs": [],
      "source": [
        "!git checkout -b feature/268"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onzIIJweDGJD"
      },
      "outputs": [],
      "source": [
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XPOtxhOG35E"
      },
      "outputs": [],
      "source": [
        "!git checkout master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsyuoxhjDKRI"
      },
      "outputs": [],
      "source": [
        "!git fetch\n",
        "!git branch -v -a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMxCKJg3DPGX"
      },
      "outputs": [],
      "source": [
        "!git switch feature/268"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5vqFPiChAQ3"
      },
      "outputs": [],
      "source": [
        "%%writefile .gitignore\n",
        "**/*.zip\n",
        "**/.h5\n",
        "**/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hdRHgCxh3Ib"
      },
      "outputs": [],
      "source": [
        "!git status\n",
        "!git add .\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95npnm9SiA1H"
      },
      "outputs": [],
      "source": [
        "!git config user.name yyz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKuIocKUiO1i"
      },
      "outputs": [],
      "source": [
        "!git config user.email yalex778@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5626nJEIiSui"
      },
      "outputs": [],
      "source": [
        "!git commit -m \"Added plot save file path in the test model param file\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaFIa96ZiZw2"
      },
      "outputs": [],
      "source": [
        "!git push -u origin feature/268"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXfxWGtDi7uU"
      },
      "outputs": [],
      "source": [
        "!git push https://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@github.com/yyu233/ASCAD.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ706i__ZW8o"
      },
      "outputs": [],
      "source": [
        "!python ASCAD_generate.py ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_generate_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCXJe56xaUan"
      },
      "outputs": [],
      "source": [
        "!ls ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZomWjWula1Pg"
      },
      "outputs": [],
      "source": [
        "!ls ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EImW0n-0emLj"
      },
      "outputs": [],
      "source": [
        "%%writefile ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_test_models_params\n",
        "{\n",
        "\"ascad_database\" : \"ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/ASCAD_fixed.h5\",\n",
        "\"model_file\": \"ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/yyz_cnn_best_ascad_desync0_epochs75_classes256_batchsize200.h5\",\n",
        "\"num_traces\": 1000,\n",
        " \"save_file\": \"yyz_test_plot\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFMmcvYGfdWt"
      },
      "outputs": [],
      "source": [
        "%pycat ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_test_models_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i8bOscLle72"
      },
      "outputs": [],
      "source": [
        "!git branch --set-upstream-to=origin/feature/268 feature/268"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlmKHvuBltYu"
      },
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhYnDLl_mdwu"
      },
      "outputs": [],
      "source": [
        "!git diff HEAD~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASxyNFpYbFzb"
      },
      "outputs": [],
      "source": [
        "!python ASCAD_test_models.py ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_test_models_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDPubUgibNhx"
      },
      "outputs": [],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKlmTDIvpYn1"
      },
      "outputs": [],
      "source": [
        "!ls ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtBLx32sfo0t"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image('yyz_test_plot.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-caeAtDtfjWU"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub9gXU5_7cwg"
      },
      "outputs": [],
      "source": [
        "!python ASCAD_train_models.py ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_train_models_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQlkTI9X7nUV"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5cekoaP_9iR"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bV8mh7tABV8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKdhe6dpdUuz"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HWZMsBqej1_"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my9_0AbjelpW"
      },
      "outputs": [],
      "source": [
        "!cp -v /content/gdrive/MyDrive/268/*  /content/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z1sAlCxiIBG"
      },
      "outputs": [],
      "source": [
        "!mv /content/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/Copy of mlp_2_epoch20_batch100_50k.h5 /content/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/268_mlp_2_epoch20_batch100_50k.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acDiBQMpiwo2"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax_NipffnmF9"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ-fhNq1nnn7"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "file_name = \"/content/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/mlp_best_ascad_desync0_node200_layernb6_epochs200_classes256_batchsize100.h5\"\n",
        "model_file = h5py.File(file_name, \"r\")\n",
        "\n",
        "model_baseline = load_model(model_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gh2tISYoIE4"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "file_name = \"/content/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/268_mlp_epoch100_batch50_50k.h5\"\n",
        "model_file = h5py.File(file_name, \"r\")\n",
        "\n",
        "model_268_epoch100_batch50 = load_model(model_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F2EFdHHqRfy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Get the weights of each layer from both models\n",
        "model_baseline_weights = model_baseline.get_weights()\n",
        "model_268_weights = model_268_epoch100_batch50.get_weights()\n",
        "\n",
        "print(len(model_baseline_weights))\n",
        "print(len(model_268_weights))\n",
        "# Calculate the absolute difference between the weights of each layer in the two models\n",
        "weight_diff = []\n",
        "print(f\"Differene between \\n mlp_best_ascad_desync0_node200_layernb6_epochs200_classes256_batchsize100.h5\\n and \\n our model 268_mlp_epoch100_batch50_50k.h5 layer weights\")\n",
        "for i in range(len(model_baseline_weights)):\n",
        "    diff = np.abs(model_268_weights[i] - model_baseline_weights[i])\n",
        "    print(\"loop\")\n",
        "    mae = np.mean(diff)\n",
        "    rmse = np.sqrt(np.mean(np.square(diff)))\n",
        "    print('MAE of the weights between the two models:', mae)\n",
        "    print('RMSE of the weights between the two models:', rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf2svloPzsir"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "file_name = \"/content/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/268_mlp_epoch50_batch50.h5\"\n",
        "model_file = h5py.File(file_name, \"r\")\n",
        "\n",
        "model_268_epoch50_batch50 = load_model(model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKVsCoNsz2q5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Get the weights of each layer from both models\n",
        "model_baseline_weights = model_baseline.get_weights()\n",
        "model_268_weights = model_268_epoch50_batch50.get_weights()\n",
        "\n",
        "print(len(model_baseline_weights))\n",
        "print(len(model_268_weights))\n",
        "# Calculate the absolute difference between the weights of each layer in the two models\n",
        "weight_diff = []\n",
        "print(f\"Differene between \\n mlp_best_ascad_desync0_node200_layernb6_epochs200_classes256_batchsize100.h5\\n and \\n our model 268_mlp_epoch50_batch50.h5 layer weights\")\n",
        "for i in range(len(model_baseline_weights)):\n",
        "    diff = np.abs(model_268_weights[i] - model_baseline_weights[i])\n",
        "    print(\"loop\")\n",
        "    mae = np.mean(diff)\n",
        "    rmse = np.sqrt(np.mean(np.square(diff)))\n",
        "    print('MAE of the weights between the two models:', mae)\n",
        "    print('RMSE of the weights between the two models:', rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ezBZ_RX22gc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Get the weights of each layer from both models\n",
        "model_268_weights_e100 = model_268_epoch100_batch50.get_weights()\n",
        "model_268_weights_e50 = model_268_epoch50_batch50.get_weights()\n",
        "\n",
        "print(len(model_268_weights_e100))\n",
        "print(len(model_268_weights_e50))\n",
        "# Calculate the absolute difference between the weights of each layer in the two models\n",
        "weight_diff = []\n",
        "print(f\"Differene between \\n 268_mlp_epoch100_batch50_50k.h5\\n and \\n our model 268_mlp_epoch50_batch50.h5 layer weights\")\n",
        "for i in range(len(model_268_weights_e100)):\n",
        "    diff = np.abs(model_268_weights_e50[i] - model_268_weights_e100[i])\n",
        "    print(\"loop\")\n",
        "    mae = np.mean(diff)\n",
        "    rmse = np.sqrt(np.mean(np.square(diff)))\n",
        "    print('MAE of the weights between the two models:', mae)\n",
        "    print('RMSE of the weights between the two models:', rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjWnJ3-n1GSN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKe7cOlPsmDn"
      },
      "outputs": [],
      "source": [
        "%%writefile ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_train_models_params\n",
        "{\n",
        "\"ascad_database\" : \"ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/ASCAD_fixed.h5\",\n",
        "\"training_model\": \"ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_trained_models/yyz_cnn_best_ascad_desync0_epochs75_classes256_batchsize200.h5\",\n",
        "\"network_type\": \"cnn\",\n",
        "\"epochs\": 200,\n",
        "\"batch_size\": 100\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCwT9fDxwUbZ"
      },
      "outputs": [],
      "source": [
        "!python ASCAD_train_models.py ATMEGA_AES_v1/ATM_AES_v1_fixed_key/example_train_models_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9NlBFqawek4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki3SfQjYD3gN",
        "outputId": "fe26055e-fe66-4060-9adb-27841a0f6e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-9-6a995d6dc172>\", line 1, in <module>\n",
            "    get_ipython().run_line_magic('cd', '/content')\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2314, in run_line_magic\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"<decorator-gen-85>\", line 2, in cd\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py\", line 334, in cd\n",
            "    oldcwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "rlxNQOSeuuz5",
        "outputId": "3f090f4e-e556-4392-968d-e02e29d5bbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-10-c10d86e90129>\", line 1, in <module>\n",
            "    get_ipython().run_line_magic('cd', '/content/drive/MyDrive/268')\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2314, in run_line_magic\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"<decorator-gen-85>\", line 2, in cd\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py\", line 334, in cd\n",
            "    oldcwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/268"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m76qIamr-2YC",
        "outputId": "1d98554a-90eb-41e3-d635-2ad0efdb92a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "pwd: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb1bhz2kD7tN",
        "outputId": "4e581f22-f45d-40af-d06c-99531ecc13fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'EnsembleSCA'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 52 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), 23.81 KiB | 48.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yyu233/EnsembleSCA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l6GkX51-59j"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fN2II_hECaJ"
      },
      "outputs": [],
      "source": [
        "!mkdir ASCAD\n",
        "!mv ../ASCAD/* ASCAD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QzBzv_SudDF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WFlCWUEEg6x"
      },
      "outputs": [],
      "source": [
        "!rm -rf ASCAD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdqchfeZEThw"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/EnsembleSCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_-86uYu-9cu",
        "outputId": "60c644d3-d4d4-4429-a36a-b1d99aa78ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/268/EnsembleSCA\n"
          ]
        }
      ],
      "source": [
        "%cd EnsembleSCA/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKInfsGF_TCp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_GHw_R_Xzy",
        "outputId": "315a6595-f7f2-498a-b172-70d976806404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/268/EnsembleSCA/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key\n",
            "--2023-03-18 03:21:14--  https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip\n",
            "Resolving www.data.gouv.fr (www.data.gouv.fr)... 37.59.183.93\n",
            "Connecting to www.data.gouv.fr (www.data.gouv.fr)|37.59.183.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4435199469 (4.1G) [application/zip]\n",
            "Saving to: ‘ASCAD_data.zip’\n",
            "\n",
            "ASCAD_data.zip      100%[===================>]   4.13G  9.16MB/s    in 11m 37s \n",
            "\n",
            "2023-03-18 03:32:52 (6.07 MB/s) - ‘ASCAD_data.zip’ saved [4435199469/4435199469]\n",
            "\n",
            "Archive:  ASCAD_data.zip\n",
            "   creating: ASCAD_data/\n",
            "   creating: ASCAD_data/ASCAD_databases/\n",
            "  inflating: ASCAD_data/ASCAD_databases/ASCAD.h5  \n",
            "  inflating: ASCAD_data/ASCAD_databases/ASCAD_desync100.h5  \n",
            "  inflating: ASCAD_data/ASCAD_databases/ASCAD_desync50.h5  \n",
            "  inflating: ASCAD_data/ASCAD_databases/ATMega8515_raw_traces.h5  \n",
            "   creating: ASCAD_data/ASCAD_trained_models/\n",
            "  inflating: ASCAD_data/ASCAD_trained_models/cnn_best_ascad_desync0_epochs75_classes256_batchsize200.h5  \n",
            "  inflating: ASCAD_data/ASCAD_trained_models/cnn_best_ascad_desync100_epochs75_classes256_batchsize200.h5  \n",
            "  inflating: ASCAD_data/ASCAD_trained_models/cnn_best_ascad_desync50_epochs75_classes256_batchsize200.h5  \n",
            "  inflating: ASCAD_data/ASCAD_trained_models/mlp_best_ascad_desync0_node200_layernb6_epochs200_classes256_batchsize100.h5  \n",
            "  inflating: ASCAD_data/ASCAD_trained_models/mlp_best_ascad_desync100_node200_layernb6_epochs200_classes256_batchsize100.h5  \n",
            "  inflating: ASCAD_data/ASCAD_trained_models/mlp_best_ascad_desync50_node200_layernb6_epochs200_classes256_batchsize100.h5  \n"
          ]
        }
      ],
      "source": [
        "%cd ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/\n",
        "!wget https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip\n",
        "!unzip ASCAD_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "DEDlkVQu_gJU",
        "outputId": "2cb42cf6-150a-4c03-fcea-660a86298abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-5-21ac2c322a5f>\", line 1, in <module>\n",
            "    get_ipython().run_line_magic('cd', '/content/drive/MyDrive/268/EnsembleSCA/')\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2314, in run_line_magic\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"<decorator-gen-85>\", line 2, in cd\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py\", line 334, in cd\n",
            "    oldcwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/268/EnsembleSCA/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aNxWasKEmXk",
        "outputId": "51b2dbfc-e03e-4ad4-977c-9109caea2042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: A branch named 'feature/268/yyz' already exists.\n"
          ]
        }
      ],
      "source": [
        "!git checkout -b feature/268/yyz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hNdB7njGzs1"
      },
      "outputs": [],
      "source": [
        "!ls ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/ASCAD.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq61IZRmCtOw"
      },
      "outputs": [],
      "source": [
        "%pycat run_ensemble.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IEd8q9KDGn9"
      },
      "outputs": [],
      "source": [
        "%pycat commons/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2hBnBNHC1eb",
        "outputId": "abcadb05-e9b3-40e1-8b27-a98e48762446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting run_ensemble.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_ensemble.py\n",
        "\n",
        "from commons.ensemble_aes import EnsembleAES\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "ensemble_aes = EnsembleAES()\n",
        "ensemble_aes.set_dataset(\"ascad_fixed_key\")  # \"ascad_fixed_key\", \"ascad_random_key\" or \"ches_ctf\"\n",
        "ensemble_aes.set_leakage_model(\"HW\")\n",
        "ensemble_aes.set_target_byte(2)\n",
        "ensemble_aes.set_mini_batch(400)\n",
        "ensemble_aes.set_epochs(200)\n",
        "ensemble_aes.run_ensemble(\n",
        "    number_of_models=5,\n",
        "    number_of_best_models=3\n",
        ")\n",
        "\n",
        "# plotting GE and SR\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ensemble_aes.get_ge_best_model_validation(), label=\"GE best validation\")\n",
        "plt.plot(ensemble_aes.get_ge_best_model_attack(), label=\"GE best attack\")\n",
        "plt.plot(ensemble_aes.get_ge_ensemble(), label=\"GE Ensemble All Models\")\n",
        "plt.plot(ensemble_aes.get_ge_ensemble_best_models(), label=\"GE Ensemble Best Models\")\n",
        "plt.xlabel(\"Traces\")\n",
        "plt.ylabel(\"Guessing Entropy\")\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ensemble_aes.get_sr_best_model_validation(), label=\"SR best validation\")\n",
        "plt.plot(ensemble_aes.get_sr_best_model_attack(), label=\"SR best attack\")\n",
        "plt.plot(ensemble_aes.get_sr_ensemble(), label=\"SR Ensemble All Models\")\n",
        "plt.plot(ensemble_aes.get_sr_ensemble_best_models(), label=\"SR Ensemble Best Models\")\n",
        "plt.xlabel(\"Traces\")\n",
        "plt.ylabel(\"Success Rate\")\n",
        "plt.legend()\n",
        "plt.savefig(\"yyz_cnn_plot\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPAktHpAEwWP"
      },
      "outputs": [],
      "source": [
        "%pycat commons/ensemble_aes.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZo2ju0OE77I",
        "outputId": "0c175bff-1072-4c13-f085-81aabab998aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/268/EnsembleSCA/commons/ensemble_aes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/268/EnsembleSCA/commons/ensemble_aes.py\n",
        "\n",
        "from tensorflow.keras import backend as backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from commons.neural_networks import NeuralNetwork\n",
        "from commons.sca_metrics import SCAMetrics\n",
        "from commons.datasets import SCADatasets\n",
        "from commons.load_datasets import LoadDatasets\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class EnsembleAES:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.number_of_models = 50\n",
        "        self.number_of_best_models = 10\n",
        "        self.ge_all_validation = []\n",
        "        self.ge_all_attack = []\n",
        "        self.sr_all_validation = []\n",
        "        self.sr_all_attack = []\n",
        "        self.k_ps_all = []\n",
        "        self.ge_ensemble = None\n",
        "        self.ge_ensemble_best_models = None\n",
        "        self.ge_best_model_validation = None\n",
        "        self.ge_best_model_attack = None\n",
        "        self.sr_ensemble = None\n",
        "        self.sr_ensemble_best_models = None\n",
        "        self.sr_best_model_validation = None\n",
        "        self.sr_best_model_attack = None\n",
        "        self.target_dataset = None\n",
        "        self.l_model = None\n",
        "        self.target_byte = None\n",
        "        self.classes = None\n",
        "        self.epochs = None\n",
        "        self.mini_batch = None\n",
        "\n",
        "    def set_dataset(self, target):\n",
        "        self.target_dataset = target\n",
        "\n",
        "    def set_leakage_model(self, leakage_model):\n",
        "        self.l_model = leakage_model\n",
        "        if leakage_model == \"HW\":\n",
        "            self.classes = 9\n",
        "        else:\n",
        "            self.classes = 256\n",
        "\n",
        "    def set_target_byte(self, target_byte):\n",
        "        self.target_byte = target_byte\n",
        "\n",
        "    def set_epochs(self, epochs):\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def set_mini_batch(self, mini_batch):\n",
        "        self.mini_batch = mini_batch\n",
        "\n",
        "    def __add_if_one(self, value):\n",
        "        return 1 if value == 1 else 0\n",
        "\n",
        "    def get_best_models(self, n_models, result_models_validation, n_traces):\n",
        "        result_number_of_traces_val = []\n",
        "        for model_index in range(n_models):\n",
        "            if result_models_validation[model_index][n_traces - 1] == 1:\n",
        "                for index in range(n_traces - 1, -1, -1):\n",
        "                    if result_models_validation[model_index][index] != 1:\n",
        "                        result_number_of_traces_val.append(\n",
        "                            [result_models_validation[model_index][n_traces - 1], index + 1,\n",
        "                             model_index])\n",
        "                        break\n",
        "            else:\n",
        "                result_number_of_traces_val.append(\n",
        "                    [result_models_validation[model_index][n_traces - 1], n_traces,\n",
        "                     model_index])\n",
        "\n",
        "        sorted_models = sorted(result_number_of_traces_val, key=lambda l: l[:])\n",
        "\n",
        "        list_of_best_models = []\n",
        "        for model_index in range(n_models):\n",
        "            list_of_best_models.append(sorted_models[model_index][2])\n",
        "\n",
        "        return list_of_best_models\n",
        "\n",
        "    def run_mlp(self, X_profiling, Y_profiling, X_validation, Y_validation, X_attack, Y_attack, plt_validation, plt_attack, params,\n",
        "                step, fraction):\n",
        "        mini_batch = random.randrange(500, 1000, 100)\n",
        "        learning_rate = random.uniform(0.0001, 0.001)\n",
        "        activation = ['relu', 'tanh', 'elu', 'selu'][random.randint(0, 3)]\n",
        "        layers = random.randrange(2, 8, 1)\n",
        "        neurons = random.randrange(500, 800, 100)\n",
        "\n",
        "        model = NeuralNetwork().mlp_random(self.classes, params[\"number_of_samples\"], activation, neurons, layers, learning_rate)\n",
        "        model.fit(\n",
        "            x=X_profiling,\n",
        "            y=Y_profiling,\n",
        "            batch_size=self.mini_batch,\n",
        "            verbose=1,\n",
        "            epochs=self.epochs,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_validation, Y_validation),\n",
        "            callbacks=[])\n",
        "\n",
        "        ge_validation, sr_validation, kp_krs = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte,\n",
        "                                                                      X_validation, plt_validation, step, fraction)\n",
        "        ge_attack, sr_attack, _ = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte, X_attack, plt_attack, step,\n",
        "                                                         fraction)\n",
        "\n",
        "        backend.clear_session()\n",
        "\n",
        "        return ge_validation, ge_attack, sr_validation, sr_attack, kp_krs\n",
        "\n",
        "    def run_cnn(self, X_profiling, Y_profiling, X_validation, Y_validation, X_attack, Y_attack, plt_validation, plt_attack, params,\n",
        "                step, fraction):\n",
        "        X_profiling = X_profiling.reshape((X_profiling.shape[0], X_profiling.shape[1], 1))\n",
        "        X_validation = X_validation.reshape((X_validation.shape[0], X_validation.shape[1], 1))\n",
        "        X_attack = X_attack.reshape((X_attack.shape[0], X_attack.shape[1], 1))\n",
        "\n",
        "        mini_batch = random.randrange(500, 1000, 100)\n",
        "        learning_rate = random.uniform(0.0001, 0.001)\n",
        "        activation = ['relu', 'tanh', 'elu', 'selu'][random.randint(0, 3)]\n",
        "        dense_layers = random.randrange(2, 8, 1)\n",
        "        neurons = random.randrange(500, 800, 100)\n",
        "        conv_layers = random.randrange(1, 2, 1)\n",
        "        filters = random.randrange(8, 32, 4)\n",
        "        kernel_size = random.randrange(10, 20, 2)\n",
        "        stride = random.randrange(5, 10, 5)\n",
        "\n",
        "        model = NeuralNetwork().cnn_random(self.classes, params[\"number_of_samples\"], activation, neurons, conv_layers, filters,\n",
        "                                           kernel_size, stride, dense_layers, learning_rate)\n",
        "        model.fit(\n",
        "            x=X_profiling,\n",
        "            y=Y_profiling,\n",
        "            batch_size=self.mini_batch,\n",
        "            verbose=1,\n",
        "            epochs=self.epochs,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_validation, Y_validation),\n",
        "            callbacks=[])\n",
        "\n",
        "        ge_validation, sr_validation, kp_krs = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte,\n",
        "                                                                      X_validation, plt_validation,\n",
        "                                                                      step, fraction)\n",
        "        ge_attack, sr_attack, _ = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte, X_attack, plt_attack, step,\n",
        "                                                         fraction)\n",
        "\n",
        "        backend.clear_session()\n",
        "\n",
        "        return ge_validation, ge_attack, sr_validation, sr_attack, kp_krs\n",
        "\n",
        "    def compute_ensembles(self, kr_nt, correct_key):\n",
        "\n",
        "        list_of_best_models = self.get_best_models(self.number_of_models, self.ge_all_validation, kr_nt)\n",
        "\n",
        "        self.ge_best_model_validation = self.ge_all_validation[list_of_best_models[0]]\n",
        "        self.ge_best_model_attack = self.ge_all_attack[list_of_best_models[0]]\n",
        "        self.sr_best_model_validation = self.sr_all_validation[list_of_best_models[0]]\n",
        "        self.sr_best_model_attack = self.sr_all_attack[list_of_best_models[0]]\n",
        "\n",
        "        kr_ensemble = np.zeros(kr_nt)\n",
        "        krs_ensemble = np.zeros((100, kr_nt))\n",
        "        kr_ensemble_best_models = np.zeros(kr_nt)\n",
        "        krs_ensemble_best_models = np.zeros((100, kr_nt))\n",
        "\n",
        "        for run in range(100):\n",
        "\n",
        "            key_p_ensemble = np.zeros(256)\n",
        "            key_p_ensemble_best_models = np.zeros(256)\n",
        "\n",
        "            for index in range(kr_nt):\n",
        "                for model_index in range(self.number_of_models):\n",
        "                    key_p_ensemble += np.log(self.k_ps_all[list_of_best_models[model_index]][run][index] + 1e-36)\n",
        "                for model_index in range(self.number_of_best_models):\n",
        "                    key_p_ensemble_best_models += np.log(self.k_ps_all[list_of_best_models[model_index]][run][index] + 1e-36)\n",
        "\n",
        "                key_p_ensemble_sorted = np.argsort(key_p_ensemble)[::-1]\n",
        "                key_p_ensemble_best_models_sorted = np.argsort(key_p_ensemble_best_models)[::-1]\n",
        "\n",
        "                kr_position = list(key_p_ensemble_sorted).index(correct_key) + 1\n",
        "                kr_ensemble[index] += kr_position\n",
        "                krs_ensemble[run][index] = kr_position\n",
        "\n",
        "                kr_position = list(key_p_ensemble_best_models_sorted).index(correct_key) + 1\n",
        "                kr_ensemble_best_models[index] += kr_position\n",
        "                krs_ensemble_best_models[run][index] = kr_position\n",
        "\n",
        "            print(\"Run {} - GE {} models: {} | GE {} models: {} | \".format(run, self.number_of_models,\n",
        "                                                                           int(kr_ensemble[kr_nt - 1] / (run + 1)),\n",
        "                                                                           self.number_of_best_models,\n",
        "                                                                           int(kr_ensemble_best_models[kr_nt - 1] / (run + 1))))\n",
        "\n",
        "        ge_ensemble = kr_ensemble / 100\n",
        "        ge_ensemble_best_models = kr_ensemble_best_models / 100\n",
        "\n",
        "        sr_ensemble = np.zeros(kr_nt)\n",
        "        sr_ensemble_best_models = np.zeros(kr_nt)\n",
        "\n",
        "        for index in range(kr_nt):\n",
        "            for run in range(100):\n",
        "                sr_ensemble[index] += self.__add_if_one(krs_ensemble[run][index])\n",
        "                sr_ensemble_best_models[index] += self.__add_if_one(krs_ensemble_best_models[run][index])\n",
        "\n",
        "        return ge_ensemble, ge_ensemble_best_models, sr_ensemble/100, sr_ensemble_best_models/100\n",
        "\n",
        "    def create_z_score_norm(self, dataset):\n",
        "        z_score_mean = np.mean(dataset, axis=0)\n",
        "        z_score_std = np.std(dataset, axis=0)\n",
        "        return z_score_mean, z_score_std\n",
        "\n",
        "    def apply_z_score_norm(self, dataset, z_score_mean, z_score_std):\n",
        "        for index in range(len(dataset)):\n",
        "            dataset[index] = (dataset[index] - z_score_mean) / z_score_std\n",
        "\n",
        "    def run_ensemble(self, number_of_models, number_of_best_models):\n",
        "\n",
        "        self.number_of_models = number_of_models\n",
        "        self.number_of_best_models = number_of_best_models\n",
        "\n",
        "        target_params = SCADatasets().get_trace_set(self.target_dataset)\n",
        "\n",
        "        root_folder = \"/content/drive/MyDrive/268/EnsembleSCA/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/\"\n",
        "\n",
        "        (X_profiling, Y_profiling), (X_validation, Y_validation), (X_attack, Y_attack), (\n",
        "            _, plt_validation, plt_attack) = LoadDatasets().load_dataset(\n",
        "            root_folder + target_params[\"file\"], target_params[\"n_profiling\"], target_params[\"n_attack\"], self.target_byte, self.l_model)\n",
        "\n",
        "        # normalize with z-score\n",
        "        z_score_mean, z_score_std = self.create_z_score_norm(X_profiling)\n",
        "        self.apply_z_score_norm(X_profiling, z_score_mean, z_score_std)\n",
        "        self.apply_z_score_norm(X_validation, z_score_mean, z_score_std)\n",
        "        self.apply_z_score_norm(X_attack, z_score_mean, z_score_std)\n",
        "\n",
        "        # convert labels to categorical labels\n",
        "        Y_profiling = to_categorical(Y_profiling, num_classes=self.classes)\n",
        "        Y_validation = to_categorical(Y_validation, num_classes=self.classes)\n",
        "        Y_attack = to_categorical(Y_attack, num_classes=self.classes)\n",
        "\n",
        "        X_profiling = X_profiling.astype('float32')\n",
        "        X_validation = X_validation.astype('float32')\n",
        "        X_attack = X_attack.astype('float32')\n",
        "\n",
        "        kr_step = 10  # key rank processed for each kr_step traces\n",
        "        kr_fraction = 1  # validation or attack sets are divided by kr_fraction before computing key rank\n",
        "\n",
        "        self.ge_all_validation = []\n",
        "        self.sr_all_validation = []\n",
        "        self.ge_all_attack = []\n",
        "        self.k_ps_all = []\n",
        "\n",
        "        kr_nt = int(len(X_validation) / (kr_step * kr_fraction))\n",
        "\n",
        "        # train random MLP\n",
        "        #for model_index in range(self.number_of_models):\n",
        "        #    ge_validation, ge_attack, sr_validation, sr_attack, kp_krs = self.run_mlp(X_profiling, Y_profiling,\n",
        "        #                                                                              X_validation, Y_validation,\n",
        "        #                                                                              X_attack, Y_attack,\n",
        "        #                                                                              plt_validation, plt_attack,\n",
        "        #                                                                              target_params, kr_step, kr_fraction)\n",
        "        #    self.ge_all_validation.append(ge_validation)\n",
        "        #    self.ge_all_attack.append(ge_attack)\n",
        "        #    self.sr_all_validation.append(sr_validation)\n",
        "        #    self.sr_all_attack.append(sr_attack)\n",
        "        #    self.k_ps_all.append(kp_krs)\n",
        "\n",
        "        # train random CNN\n",
        "        for model_index in range(self.number_of_models):\n",
        "             ge_validation, ge_attack, sr_validation, sr_attack, kp_krs = self.run_cnn(X_profiling, Y_profiling,\n",
        "                                                                                       X_validation, Y_validation,\n",
        "                                                                                       X_attack, Y_attack,\n",
        "                                                                                       plt_validation, plt_attack,\n",
        "                                                                                       target_params, kr_step, kr_fraction)\n",
        "             self.ge_all_validation.append(ge_validation)\n",
        "             self.ge_all_attack.append(ge_attack)\n",
        "             self.sr_all_validation.append(sr_validation)\n",
        "             self.sr_all_attack.append(sr_attack)\n",
        "             self.k_ps_all.append(kp_krs)\n",
        "\n",
        "        ge_ensemble, ge_ensemble_best_models, sr_ensemble, sr_ensemble_best_models = self.compute_ensembles(kr_nt,\n",
        "                                                                                                            target_params[\"good_key\"])\n",
        "\n",
        "        self.ge_ensemble = ge_ensemble\n",
        "        self.ge_ensemble_best_models = ge_ensemble_best_models\n",
        "        self.sr_ensemble = sr_ensemble\n",
        "        self.sr_ensemble_best_models = sr_ensemble_best_models\n",
        "\n",
        "    def get_ge_ensemble(self):\n",
        "        return self.ge_ensemble\n",
        "\n",
        "    def get_ge_ensemble_best_models(self):\n",
        "        return self.ge_ensemble_best_models\n",
        "\n",
        "    def get_ge_best_model_validation(self):\n",
        "        return self.ge_best_model_validation\n",
        "\n",
        "    def get_ge_best_model_attack(self):\n",
        "        return self.ge_best_model_attack\n",
        "\n",
        "    def get_sr_ensemble(self):\n",
        "        return self.sr_ensemble\n",
        "\n",
        "    def get_sr_ensemble_best_models(self):\n",
        "        return self.sr_ensemble_best_models\n",
        "\n",
        "    def get_sr_best_model_validation(self):\n",
        "        return self.sr_best_model_validation\n",
        "\n",
        "    def get_sr_best_model_attack(self):\n",
        "        return self.sr_best_model_attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc8uaQPhHbgy",
        "outputId": "89a5c032-f64d-4942-d25a-8446876343b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-19 21:04:03.446515: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-19 21:04:04.899488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-19 21:04:04.899645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-19 21:04:04.899675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-19 21:04:10.153992: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Num GPUs Available:  0\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 137, 16)           272       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 600)               1315800   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 5409      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,682,081\n",
            "Trainable params: 1,682,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/200\n",
            "125/125 [==============================] - 15s 113ms/step - loss: 1.9486 - accuracy: 0.2475 - val_loss: 1.7967 - val_accuracy: 0.2678\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.7672 - accuracy: 0.2681 - val_loss: 1.8024 - val_accuracy: 0.2652\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 15s 120ms/step - loss: 1.7599 - accuracy: 0.2690 - val_loss: 1.7969 - val_accuracy: 0.2678\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.7536 - accuracy: 0.2731 - val_loss: 1.8027 - val_accuracy: 0.2520\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.7439 - accuracy: 0.2784 - val_loss: 1.8123 - val_accuracy: 0.2514\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.7338 - accuracy: 0.2845 - val_loss: 1.8062 - val_accuracy: 0.2664\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.7215 - accuracy: 0.2913 - val_loss: 1.8191 - val_accuracy: 0.2386\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.7117 - accuracy: 0.2939 - val_loss: 1.8231 - val_accuracy: 0.2482\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.6967 - accuracy: 0.3009 - val_loss: 1.8183 - val_accuracy: 0.2432\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.6843 - accuracy: 0.3095 - val_loss: 1.8198 - val_accuracy: 0.2482\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.6733 - accuracy: 0.3103 - val_loss: 1.8543 - val_accuracy: 0.2386\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 1.6608 - accuracy: 0.3159 - val_loss: 1.8518 - val_accuracy: 0.2300\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 1.6509 - accuracy: 0.3189 - val_loss: 1.8543 - val_accuracy: 0.2494\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 1.6425 - accuracy: 0.3238 - val_loss: 1.8657 - val_accuracy: 0.2382\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.6262 - accuracy: 0.3308 - val_loss: 1.8739 - val_accuracy: 0.2538\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.6179 - accuracy: 0.3342 - val_loss: 1.8794 - val_accuracy: 0.2390\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.6055 - accuracy: 0.3396 - val_loss: 1.8824 - val_accuracy: 0.2392\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.5987 - accuracy: 0.3408 - val_loss: 1.8888 - val_accuracy: 0.2432\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.5870 - accuracy: 0.3470 - val_loss: 1.9324 - val_accuracy: 0.2484\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.5799 - accuracy: 0.3515 - val_loss: 1.9344 - val_accuracy: 0.2418\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.5684 - accuracy: 0.3583 - val_loss: 1.9400 - val_accuracy: 0.2338\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.5580 - accuracy: 0.3641 - val_loss: 1.9606 - val_accuracy: 0.2304\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.5409 - accuracy: 0.3701 - val_loss: 1.9426 - val_accuracy: 0.2402\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.5322 - accuracy: 0.3754 - val_loss: 1.9659 - val_accuracy: 0.2366\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 1.5194 - accuracy: 0.3830 - val_loss: 2.0040 - val_accuracy: 0.2352\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 16s 129ms/step - loss: 1.5024 - accuracy: 0.3932 - val_loss: 1.9938 - val_accuracy: 0.2402\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 1.4864 - accuracy: 0.4027 - val_loss: 2.0470 - val_accuracy: 0.2530\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 1.4651 - accuracy: 0.4108 - val_loss: 2.0614 - val_accuracy: 0.2218\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 1.4453 - accuracy: 0.4238 - val_loss: 2.0726 - val_accuracy: 0.2382\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 1.4221 - accuracy: 0.4343 - val_loss: 2.1290 - val_accuracy: 0.2268\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 1.4033 - accuracy: 0.4456 - val_loss: 2.1391 - val_accuracy: 0.2306\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.3767 - accuracy: 0.4531 - val_loss: 2.1790 - val_accuracy: 0.2152\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.3452 - accuracy: 0.4693 - val_loss: 2.2165 - val_accuracy: 0.2258\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.3211 - accuracy: 0.4813 - val_loss: 2.2335 - val_accuracy: 0.2298\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.2870 - accuracy: 0.4989 - val_loss: 2.3285 - val_accuracy: 0.2158\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.2561 - accuracy: 0.5141 - val_loss: 2.3598 - val_accuracy: 0.2180\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.2125 - accuracy: 0.5331 - val_loss: 2.3716 - val_accuracy: 0.2186\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 1.1820 - accuracy: 0.5458 - val_loss: 2.4400 - val_accuracy: 0.2152\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.1333 - accuracy: 0.5692 - val_loss: 2.5538 - val_accuracy: 0.2234\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.0861 - accuracy: 0.5895 - val_loss: 2.5980 - val_accuracy: 0.2122\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 1.0449 - accuracy: 0.6080 - val_loss: 2.6874 - val_accuracy: 0.2122\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.9814 - accuracy: 0.6337 - val_loss: 2.8236 - val_accuracy: 0.2134\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 0.9354 - accuracy: 0.6516 - val_loss: 2.8633 - val_accuracy: 0.2152\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 13s 108ms/step - loss: 0.8744 - accuracy: 0.6792 - val_loss: 3.0227 - val_accuracy: 0.2100\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.8186 - accuracy: 0.7018 - val_loss: 3.0832 - val_accuracy: 0.2126\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.7611 - accuracy: 0.7253 - val_loss: 3.1972 - val_accuracy: 0.2088\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.6965 - accuracy: 0.7530 - val_loss: 3.3986 - val_accuracy: 0.2192\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 0.6322 - accuracy: 0.7777 - val_loss: 3.6073 - val_accuracy: 0.2162\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.5700 - accuracy: 0.8039 - val_loss: 3.7134 - val_accuracy: 0.2210\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.5142 - accuracy: 0.8254 - val_loss: 3.9110 - val_accuracy: 0.2178\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.4461 - accuracy: 0.8544 - val_loss: 4.1368 - val_accuracy: 0.2106\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.3957 - accuracy: 0.8728 - val_loss: 4.3715 - val_accuracy: 0.2188\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.3470 - accuracy: 0.8917 - val_loss: 4.5368 - val_accuracy: 0.2136\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.2922 - accuracy: 0.9150 - val_loss: 4.8468 - val_accuracy: 0.2032\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.2510 - accuracy: 0.9290 - val_loss: 5.1095 - val_accuracy: 0.2066\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.2144 - accuracy: 0.9424 - val_loss: 5.2945 - val_accuracy: 0.2028\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 0.1660 - accuracy: 0.9620 - val_loss: 5.6532 - val_accuracy: 0.2054\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.1273 - accuracy: 0.9755 - val_loss: 5.7927 - val_accuracy: 0.2086\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0959 - accuracy: 0.9860 - val_loss: 6.0682 - val_accuracy: 0.2038\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 0.0690 - accuracy: 0.9936 - val_loss: 6.3217 - val_accuracy: 0.2096\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0463 - accuracy: 0.9984 - val_loss: 6.5711 - val_accuracy: 0.2102\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0295 - accuracy: 0.9997 - val_loss: 6.7959 - val_accuracy: 0.2076\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 6.9934 - val_accuracy: 0.2076\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 7.1627 - val_accuracy: 0.2064\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 7.2996 - val_accuracy: 0.2090\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 7.4435 - val_accuracy: 0.2078\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 7.5868 - val_accuracy: 0.2042\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 7.7140 - val_accuracy: 0.2074\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 7.8275 - val_accuracy: 0.2066\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 7.9571 - val_accuracy: 0.2084\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 8.0925 - val_accuracy: 0.2084\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 8.2012 - val_accuracy: 0.2056\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 8.3001 - val_accuracy: 0.2076\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 8.4061 - val_accuracy: 0.2048\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 8.5177 - val_accuracy: 0.2068\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 8.6210 - val_accuracy: 0.2064\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 8.7163 - val_accuracy: 0.2082\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 8.8224 - val_accuracy: 0.2058\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 8.9347 - val_accuracy: 0.2084\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 9.0049 - val_accuracy: 0.2048\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 9.1052 - val_accuracy: 0.2060\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 9.2195 - val_accuracy: 0.2062\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 9.3095 - val_accuracy: 0.2062\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 9.3916 - val_accuracy: 0.2056\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 9.4908 - val_accuracy: 0.2046\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5629 - val_accuracy: 0.2046\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.6847 - val_accuracy: 0.2018\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.7704 - val_accuracy: 0.2066\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.8571 - val_accuracy: 0.2076\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 9.3922e-04 - accuracy: 1.0000 - val_loss: 9.9380 - val_accuracy: 0.2050\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 8.6533e-04 - accuracy: 1.0000 - val_loss: 10.0256 - val_accuracy: 0.2056\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 7.9967e-04 - accuracy: 1.0000 - val_loss: 10.1133 - val_accuracy: 0.2040\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 7.3414e-04 - accuracy: 1.0000 - val_loss: 10.2152 - val_accuracy: 0.2052\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 6.7591e-04 - accuracy: 1.0000 - val_loss: 10.3028 - val_accuracy: 0.2058\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 6.2533e-04 - accuracy: 1.0000 - val_loss: 10.3835 - val_accuracy: 0.2056\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 5.7405e-04 - accuracy: 1.0000 - val_loss: 10.4680 - val_accuracy: 0.2066\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 5.2920e-04 - accuracy: 1.0000 - val_loss: 10.5518 - val_accuracy: 0.2044\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 4.8984e-04 - accuracy: 1.0000 - val_loss: 10.6452 - val_accuracy: 0.2052\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.5089e-04 - accuracy: 1.0000 - val_loss: 10.7347 - val_accuracy: 0.2064\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.1655e-04 - accuracy: 1.0000 - val_loss: 10.8029 - val_accuracy: 0.2066\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.8473e-04 - accuracy: 1.0000 - val_loss: 10.9127 - val_accuracy: 0.2038\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.5743e-04 - accuracy: 1.0000 - val_loss: 10.9882 - val_accuracy: 0.2050\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.2905e-04 - accuracy: 1.0000 - val_loss: 11.0494 - val_accuracy: 0.2064\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.0413e-04 - accuracy: 1.0000 - val_loss: 11.1518 - val_accuracy: 0.2046\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 2.8098e-04 - accuracy: 1.0000 - val_loss: 11.2350 - val_accuracy: 0.2024\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 2.6054e-04 - accuracy: 1.0000 - val_loss: 11.3067 - val_accuracy: 0.2048\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 2.4015e-04 - accuracy: 1.0000 - val_loss: 11.4091 - val_accuracy: 0.2050\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 2.2235e-04 - accuracy: 1.0000 - val_loss: 11.4848 - val_accuracy: 0.2058\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 13s 108ms/step - loss: 2.0627e-04 - accuracy: 1.0000 - val_loss: 11.5638 - val_accuracy: 0.2050\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.9180e-04 - accuracy: 1.0000 - val_loss: 11.6198 - val_accuracy: 0.2072\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 1.7730e-04 - accuracy: 1.0000 - val_loss: 11.7362 - val_accuracy: 0.2052\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.6390e-04 - accuracy: 1.0000 - val_loss: 11.8048 - val_accuracy: 0.2046\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.5212e-04 - accuracy: 1.0000 - val_loss: 11.8756 - val_accuracy: 0.2046\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.4107e-04 - accuracy: 1.0000 - val_loss: 11.9681 - val_accuracy: 0.2054\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.3062e-04 - accuracy: 1.0000 - val_loss: 12.0443 - val_accuracy: 0.2050\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.2120e-04 - accuracy: 1.0000 - val_loss: 12.1226 - val_accuracy: 0.2052\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.1236e-04 - accuracy: 1.0000 - val_loss: 12.2047 - val_accuracy: 0.2050\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.0395e-04 - accuracy: 1.0000 - val_loss: 12.2748 - val_accuracy: 0.2040\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 9.6553e-05 - accuracy: 1.0000 - val_loss: 12.3721 - val_accuracy: 0.2042\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 8.9678e-05 - accuracy: 1.0000 - val_loss: 12.4389 - val_accuracy: 0.2034\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 8.3036e-05 - accuracy: 1.0000 - val_loss: 12.5233 - val_accuracy: 0.2048\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 7.7206e-05 - accuracy: 1.0000 - val_loss: 12.6038 - val_accuracy: 0.2056\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 7.1422e-05 - accuracy: 1.0000 - val_loss: 12.6818 - val_accuracy: 0.2032\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 6.6414e-05 - accuracy: 1.0000 - val_loss: 12.7859 - val_accuracy: 0.2036\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 6.1771e-05 - accuracy: 1.0000 - val_loss: 12.8415 - val_accuracy: 0.2044\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 5.7321e-05 - accuracy: 1.0000 - val_loss: 12.9009 - val_accuracy: 0.2032\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 5.3188e-05 - accuracy: 1.0000 - val_loss: 12.9882 - val_accuracy: 0.2026\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 4.9320e-05 - accuracy: 1.0000 - val_loss: 13.0644 - val_accuracy: 0.2034\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 4.5856e-05 - accuracy: 1.0000 - val_loss: 13.1495 - val_accuracy: 0.2042\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.2666e-05 - accuracy: 1.0000 - val_loss: 13.2289 - val_accuracy: 0.2056\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 3.9585e-05 - accuracy: 1.0000 - val_loss: 13.3110 - val_accuracy: 0.2040\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 3.6841e-05 - accuracy: 1.0000 - val_loss: 13.3879 - val_accuracy: 0.2036\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 3.4269e-05 - accuracy: 1.0000 - val_loss: 13.4690 - val_accuracy: 0.2044\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 3.1848e-05 - accuracy: 1.0000 - val_loss: 13.5420 - val_accuracy: 0.2038\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 2.9586e-05 - accuracy: 1.0000 - val_loss: 13.6118 - val_accuracy: 0.2034\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 2.7493e-05 - accuracy: 1.0000 - val_loss: 13.6950 - val_accuracy: 0.2028\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 2.5687e-05 - accuracy: 1.0000 - val_loss: 13.7709 - val_accuracy: 0.2046\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 2.3911e-05 - accuracy: 1.0000 - val_loss: 13.8469 - val_accuracy: 0.2034\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 2.2146e-05 - accuracy: 1.0000 - val_loss: 13.9183 - val_accuracy: 0.2032\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 2.0567e-05 - accuracy: 1.0000 - val_loss: 14.0032 - val_accuracy: 0.2032\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.9125e-05 - accuracy: 1.0000 - val_loss: 14.0733 - val_accuracy: 0.2036\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.7765e-05 - accuracy: 1.0000 - val_loss: 14.1497 - val_accuracy: 0.2040\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.6528e-05 - accuracy: 1.0000 - val_loss: 14.2278 - val_accuracy: 0.2028\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.5410e-05 - accuracy: 1.0000 - val_loss: 14.2878 - val_accuracy: 0.2044\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.4366e-05 - accuracy: 1.0000 - val_loss: 14.3825 - val_accuracy: 0.2036\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.3278e-05 - accuracy: 1.0000 - val_loss: 14.4317 - val_accuracy: 0.2026\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.2383e-05 - accuracy: 1.0000 - val_loss: 14.5243 - val_accuracy: 0.2028\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.1588e-05 - accuracy: 1.0000 - val_loss: 14.6032 - val_accuracy: 0.2042\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.0751e-05 - accuracy: 1.0000 - val_loss: 14.6688 - val_accuracy: 0.2036\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.0021e-05 - accuracy: 1.0000 - val_loss: 14.7456 - val_accuracy: 0.2044\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 9.3322e-06 - accuracy: 1.0000 - val_loss: 14.8257 - val_accuracy: 0.2028\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 8.7081e-06 - accuracy: 1.0000 - val_loss: 14.8888 - val_accuracy: 0.2038\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 8.1125e-06 - accuracy: 1.0000 - val_loss: 14.9711 - val_accuracy: 0.2028\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 7.5638e-06 - accuracy: 1.0000 - val_loss: 15.0463 - val_accuracy: 0.2040\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 7.0326e-06 - accuracy: 1.0000 - val_loss: 15.1238 - val_accuracy: 0.2042\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 6.5522e-06 - accuracy: 1.0000 - val_loss: 15.1940 - val_accuracy: 0.2034\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 6.0990e-06 - accuracy: 1.0000 - val_loss: 15.2656 - val_accuracy: 0.2034\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 5.6648e-06 - accuracy: 1.0000 - val_loss: 15.3410 - val_accuracy: 0.2036\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 5.2963e-06 - accuracy: 1.0000 - val_loss: 15.4063 - val_accuracy: 0.2032\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 4.9565e-06 - accuracy: 1.0000 - val_loss: 15.4823 - val_accuracy: 0.2042\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 4.6089e-06 - accuracy: 1.0000 - val_loss: 15.5721 - val_accuracy: 0.2040\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 4.3012e-06 - accuracy: 1.0000 - val_loss: 15.6305 - val_accuracy: 0.2044\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.0152e-06 - accuracy: 1.0000 - val_loss: 15.7036 - val_accuracy: 0.2032\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.7353e-06 - accuracy: 1.0000 - val_loss: 15.7849 - val_accuracy: 0.2034\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.4861e-06 - accuracy: 1.0000 - val_loss: 15.8535 - val_accuracy: 0.2022\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.2563e-06 - accuracy: 1.0000 - val_loss: 15.9249 - val_accuracy: 0.2034\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 3.0378e-06 - accuracy: 1.0000 - val_loss: 15.9902 - val_accuracy: 0.2036\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 2.8327e-06 - accuracy: 1.0000 - val_loss: 16.0623 - val_accuracy: 0.2044\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 2.6496e-06 - accuracy: 1.0000 - val_loss: 16.1434 - val_accuracy: 0.2040\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 2.4702e-06 - accuracy: 1.0000 - val_loss: 16.1953 - val_accuracy: 0.2046\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 2.3048e-06 - accuracy: 1.0000 - val_loss: 16.2842 - val_accuracy: 0.2028\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 2.1563e-06 - accuracy: 1.0000 - val_loss: 16.3329 - val_accuracy: 0.2020\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 2.0092e-06 - accuracy: 1.0000 - val_loss: 16.4191 - val_accuracy: 0.2038\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.8754e-06 - accuracy: 1.0000 - val_loss: 16.4955 - val_accuracy: 0.2038\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.7555e-06 - accuracy: 1.0000 - val_loss: 16.5739 - val_accuracy: 0.2038\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 1.6431e-06 - accuracy: 1.0000 - val_loss: 16.6441 - val_accuracy: 0.2036\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.5354e-06 - accuracy: 1.0000 - val_loss: 16.7060 - val_accuracy: 0.2036\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 1.4371e-06 - accuracy: 1.0000 - val_loss: 16.7741 - val_accuracy: 0.2040\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 1.3383e-06 - accuracy: 1.0000 - val_loss: 16.8490 - val_accuracy: 0.2042\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.2604e-06 - accuracy: 1.0000 - val_loss: 16.9147 - val_accuracy: 0.2018\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.1746e-06 - accuracy: 1.0000 - val_loss: 16.9787 - val_accuracy: 0.2034\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 1.0953e-06 - accuracy: 1.0000 - val_loss: 17.0531 - val_accuracy: 0.2026\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 1.0300e-06 - accuracy: 1.0000 - val_loss: 17.1218 - val_accuracy: 0.2040\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 9.6461e-07 - accuracy: 1.0000 - val_loss: 17.1900 - val_accuracy: 0.2046\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 9.0170e-07 - accuracy: 1.0000 - val_loss: 17.2670 - val_accuracy: 0.2040\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 8.4326e-07 - accuracy: 1.0000 - val_loss: 17.3299 - val_accuracy: 0.2046\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 7.9120e-07 - accuracy: 1.0000 - val_loss: 17.3933 - val_accuracy: 0.2028\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 7.4117e-07 - accuracy: 1.0000 - val_loss: 17.4610 - val_accuracy: 0.2022\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 6.9628e-07 - accuracy: 1.0000 - val_loss: 17.5350 - val_accuracy: 0.2022\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 6.5061e-07 - accuracy: 1.0000 - val_loss: 17.5907 - val_accuracy: 0.2038\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 6.1233e-07 - accuracy: 1.0000 - val_loss: 17.6596 - val_accuracy: 0.2032\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 5.7421e-07 - accuracy: 1.0000 - val_loss: 17.7297 - val_accuracy: 0.2032\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 5.3808e-07 - accuracy: 1.0000 - val_loss: 17.7976 - val_accuracy: 0.2034\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 5.0551e-07 - accuracy: 1.0000 - val_loss: 17.8666 - val_accuracy: 0.2034\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.7337e-07 - accuracy: 1.0000 - val_loss: 17.9371 - val_accuracy: 0.2028\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.4506e-07 - accuracy: 1.0000 - val_loss: 18.0040 - val_accuracy: 0.2036\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 4.1811e-07 - accuracy: 1.0000 - val_loss: 18.0564 - val_accuracy: 0.2036\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.9251e-07 - accuracy: 1.0000 - val_loss: 18.1271 - val_accuracy: 0.2034\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 3.6901e-07 - accuracy: 1.0000 - val_loss: 18.1961 - val_accuracy: 0.2034\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 3.4774e-07 - accuracy: 1.0000 - val_loss: 18.2564 - val_accuracy: 0.2052\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "KR: 0 | GE for correct key (224): 2.0)\n",
            "KR: 1 | GE for correct key (224): 2.0)\n",
            "KR: 2 | GE for correct key (224): 2.0)\n",
            "KR: 3 | GE for correct key (224): 2.0)\n",
            "KR: 4 | GE for correct key (224): 2.0)\n",
            "KR: 5 | GE for correct key (224): 2.0)\n",
            "KR: 6 | GE for correct key (224): 2.0)\n",
            "KR: 7 | GE for correct key (224): 2.0)\n",
            "KR: 8 | GE for correct key (224): 2.0)\n",
            "KR: 9 | GE for correct key (224): 2.0)\n",
            "KR: 10 | GE for correct key (224): 2.0)\n",
            "KR: 11 | GE for correct key (224): 2.0)\n",
            "KR: 12 | GE for correct key (224): 2.0)\n",
            "KR: 13 | GE for correct key (224): 2.0)\n",
            "KR: 14 | GE for correct key (224): 2.0)\n",
            "KR: 15 | GE for correct key (224): 2.0)\n",
            "KR: 16 | GE for correct key (224): 2.0)\n",
            "KR: 17 | GE for correct key (224): 2.0)\n",
            "KR: 18 | GE for correct key (224): 2.0)\n",
            "KR: 19 | GE for correct key (224): 2.0)\n",
            "KR: 20 | GE for correct key (224): 2.0)\n",
            "KR: 21 | GE for correct key (224): 2.0)\n",
            "KR: 22 | GE for correct key (224): 2.0)\n",
            "KR: 23 | GE for correct key (224): 2.0)\n",
            "KR: 24 | GE for correct key (224): 2.0)\n",
            "KR: 25 | GE for correct key (224): 2.0)\n",
            "KR: 26 | GE for correct key (224): 2.0)\n",
            "KR: 27 | GE for correct key (224): 2.0)\n",
            "KR: 28 | GE for correct key (224): 2.0)\n",
            "KR: 29 | GE for correct key (224): 2.0)\n",
            "KR: 30 | GE for correct key (224): 2.0)\n",
            "KR: 31 | GE for correct key (224): 2.0)\n",
            "KR: 32 | GE for correct key (224): 2.0)\n",
            "KR: 33 | GE for correct key (224): 2.0)\n",
            "KR: 34 | GE for correct key (224): 2.0)\n",
            "KR: 35 | GE for correct key (224): 2.0)\n",
            "KR: 36 | GE for correct key (224): 2.0)\n",
            "KR: 37 | GE for correct key (224): 2.0)\n",
            "KR: 38 | GE for correct key (224): 2.0)\n",
            "KR: 39 | GE for correct key (224): 2.0)\n",
            "KR: 40 | GE for correct key (224): 2.0)\n",
            "KR: 41 | GE for correct key (224): 2.0)\n",
            "KR: 42 | GE for correct key (224): 2.0)\n",
            "KR: 43 | GE for correct key (224): 2.0)\n",
            "KR: 44 | GE for correct key (224): 2.0)\n",
            "KR: 45 | GE for correct key (224): 2.0)\n",
            "KR: 46 | GE for correct key (224): 2.0)\n",
            "KR: 47 | GE for correct key (224): 2.0)\n",
            "KR: 48 | GE for correct key (224): 2.0)\n",
            "KR: 49 | GE for correct key (224): 2.0)\n",
            "KR: 50 | GE for correct key (224): 2.0)\n",
            "KR: 51 | GE for correct key (224): 2.0)\n",
            "KR: 52 | GE for correct key (224): 2.0)\n",
            "KR: 53 | GE for correct key (224): 2.0)\n",
            "KR: 54 | GE for correct key (224): 2.0)\n",
            "KR: 55 | GE for correct key (224): 2.0)\n",
            "KR: 56 | GE for correct key (224): 2.0)\n",
            "KR: 57 | GE for correct key (224): 2.0)\n",
            "KR: 58 | GE for correct key (224): 2.0)\n",
            "KR: 59 | GE for correct key (224): 2.0)\n",
            "KR: 60 | GE for correct key (224): 2.0)\n",
            "KR: 61 | GE for correct key (224): 2.0)\n",
            "KR: 62 | GE for correct key (224): 2.0)\n",
            "KR: 63 | GE for correct key (224): 2.0)\n",
            "KR: 64 | GE for correct key (224): 2.0)\n",
            "KR: 65 | GE for correct key (224): 2.0)\n",
            "KR: 66 | GE for correct key (224): 2.0)\n",
            "KR: 67 | GE for correct key (224): 2.0)\n",
            "KR: 68 | GE for correct key (224): 2.0)\n",
            "KR: 69 | GE for correct key (224): 2.0)\n",
            "KR: 70 | GE for correct key (224): 2.0)\n",
            "KR: 71 | GE for correct key (224): 2.0)\n",
            "KR: 72 | GE for correct key (224): 2.0)\n",
            "KR: 73 | GE for correct key (224): 2.0)\n",
            "KR: 74 | GE for correct key (224): 2.0)\n",
            "KR: 75 | GE for correct key (224): 2.0)\n",
            "KR: 76 | GE for correct key (224): 2.0)\n",
            "KR: 77 | GE for correct key (224): 2.0)\n",
            "KR: 78 | GE for correct key (224): 2.0)\n",
            "KR: 79 | GE for correct key (224): 2.0)\n",
            "KR: 80 | GE for correct key (224): 2.0)\n",
            "KR: 81 | GE for correct key (224): 2.0)\n",
            "KR: 82 | GE for correct key (224): 2.0)\n",
            "KR: 83 | GE for correct key (224): 2.0)\n",
            "KR: 84 | GE for correct key (224): 2.0)\n",
            "KR: 85 | GE for correct key (224): 2.0)\n",
            "KR: 86 | GE for correct key (224): 2.0)\n",
            "KR: 87 | GE for correct key (224): 2.0)\n",
            "KR: 88 | GE for correct key (224): 2.0)\n",
            "KR: 89 | GE for correct key (224): 2.0)\n",
            "KR: 90 | GE for correct key (224): 2.0)\n",
            "KR: 91 | GE for correct key (224): 2.0)\n",
            "KR: 92 | GE for correct key (224): 2.0)\n",
            "KR: 93 | GE for correct key (224): 2.0)\n",
            "KR: 94 | GE for correct key (224): 2.0)\n",
            "KR: 95 | GE for correct key (224): 2.0)\n",
            "KR: 96 | GE for correct key (224): 2.0)\n",
            "KR: 97 | GE for correct key (224): 2.0)\n",
            "KR: 98 | GE for correct key (224): 2.0)\n",
            "KR: 99 | GE for correct key (224): 2.0)\n",
            "157/157 [==============================] - 1s 8ms/step\n",
            "KR: 0 | GE for correct key (224): 1.0)\n",
            "KR: 1 | GE for correct key (224): 1.0)\n",
            "KR: 2 | GE for correct key (224): 1.0)\n",
            "KR: 3 | GE for correct key (224): 1.0)\n",
            "KR: 4 | GE for correct key (224): 1.0)\n",
            "KR: 5 | GE for correct key (224): 1.0)\n",
            "KR: 6 | GE for correct key (224): 1.0)\n",
            "KR: 7 | GE for correct key (224): 1.0)\n",
            "KR: 8 | GE for correct key (224): 1.0)\n",
            "KR: 9 | GE for correct key (224): 1.0)\n",
            "KR: 10 | GE for correct key (224): 1.0)\n",
            "KR: 11 | GE for correct key (224): 1.0)\n",
            "KR: 12 | GE for correct key (224): 1.0)\n",
            "KR: 13 | GE for correct key (224): 1.0)\n",
            "KR: 14 | GE for correct key (224): 1.0)\n",
            "KR: 15 | GE for correct key (224): 1.0)\n",
            "KR: 16 | GE for correct key (224): 1.0)\n",
            "KR: 17 | GE for correct key (224): 1.0)\n",
            "KR: 18 | GE for correct key (224): 1.0)\n",
            "KR: 19 | GE for correct key (224): 1.0)\n",
            "KR: 20 | GE for correct key (224): 1.0)\n",
            "KR: 21 | GE for correct key (224): 1.0)\n",
            "KR: 22 | GE for correct key (224): 1.0)\n",
            "KR: 23 | GE for correct key (224): 1.0)\n",
            "KR: 24 | GE for correct key (224): 1.0)\n",
            "KR: 25 | GE for correct key (224): 1.0)\n",
            "KR: 26 | GE for correct key (224): 1.0)\n",
            "KR: 27 | GE for correct key (224): 1.0)\n",
            "KR: 28 | GE for correct key (224): 1.0)\n",
            "KR: 29 | GE for correct key (224): 1.0)\n",
            "KR: 30 | GE for correct key (224): 1.0)\n",
            "KR: 31 | GE for correct key (224): 1.0)\n",
            "KR: 32 | GE for correct key (224): 1.0)\n",
            "KR: 33 | GE for correct key (224): 1.0)\n",
            "KR: 34 | GE for correct key (224): 1.0)\n",
            "KR: 35 | GE for correct key (224): 1.0)\n",
            "KR: 36 | GE for correct key (224): 1.0)\n",
            "KR: 37 | GE for correct key (224): 1.0)\n",
            "KR: 38 | GE for correct key (224): 1.0)\n",
            "KR: 39 | GE for correct key (224): 1.0)\n",
            "KR: 40 | GE for correct key (224): 1.0)\n",
            "KR: 41 | GE for correct key (224): 1.0)\n",
            "KR: 42 | GE for correct key (224): 1.0)\n",
            "KR: 43 | GE for correct key (224): 1.0)\n",
            "KR: 44 | GE for correct key (224): 1.0)\n",
            "KR: 45 | GE for correct key (224): 1.0)\n",
            "KR: 46 | GE for correct key (224): 1.0)\n",
            "KR: 47 | GE for correct key (224): 1.0)\n",
            "KR: 48 | GE for correct key (224): 1.0)\n",
            "KR: 49 | GE for correct key (224): 1.0)\n",
            "KR: 50 | GE for correct key (224): 1.0)\n",
            "KR: 51 | GE for correct key (224): 1.0)\n",
            "KR: 52 | GE for correct key (224): 1.0)\n",
            "KR: 53 | GE for correct key (224): 1.0)\n",
            "KR: 54 | GE for correct key (224): 1.0)\n",
            "KR: 55 | GE for correct key (224): 1.0)\n",
            "KR: 56 | GE for correct key (224): 1.0)\n",
            "KR: 57 | GE for correct key (224): 1.0)\n",
            "KR: 58 | GE for correct key (224): 1.0)\n",
            "KR: 59 | GE for correct key (224): 1.0)\n",
            "KR: 60 | GE for correct key (224): 1.0)\n",
            "KR: 61 | GE for correct key (224): 1.0)\n",
            "KR: 62 | GE for correct key (224): 1.0)\n",
            "KR: 63 | GE for correct key (224): 1.0)\n",
            "KR: 64 | GE for correct key (224): 1.0)\n",
            "KR: 65 | GE for correct key (224): 1.0)\n",
            "KR: 66 | GE for correct key (224): 1.0)\n",
            "KR: 67 | GE for correct key (224): 1.0)\n",
            "KR: 68 | GE for correct key (224): 1.0)\n",
            "KR: 69 | GE for correct key (224): 1.0)\n",
            "KR: 70 | GE for correct key (224): 1.0)\n",
            "KR: 71 | GE for correct key (224): 1.0)\n",
            "KR: 72 | GE for correct key (224): 1.0)\n",
            "KR: 73 | GE for correct key (224): 1.0)\n",
            "KR: 74 | GE for correct key (224): 1.0)\n",
            "KR: 75 | GE for correct key (224): 1.0)\n",
            "KR: 76 | GE for correct key (224): 1.0)\n",
            "KR: 77 | GE for correct key (224): 1.0)\n",
            "KR: 78 | GE for correct key (224): 1.0)\n",
            "KR: 79 | GE for correct key (224): 1.0)\n",
            "KR: 80 | GE for correct key (224): 1.0)\n",
            "KR: 81 | GE for correct key (224): 1.0)\n",
            "KR: 82 | GE for correct key (224): 1.0)\n",
            "KR: 83 | GE for correct key (224): 1.0)\n",
            "KR: 84 | GE for correct key (224): 1.0)\n",
            "KR: 85 | GE for correct key (224): 1.0)\n",
            "KR: 86 | GE for correct key (224): 1.0)\n",
            "KR: 87 | GE for correct key (224): 1.0)\n",
            "KR: 88 | GE for correct key (224): 1.0)\n",
            "KR: 89 | GE for correct key (224): 1.0)\n",
            "KR: 90 | GE for correct key (224): 1.0)\n",
            "KR: 91 | GE for correct key (224): 1.0)\n",
            "KR: 92 | GE for correct key (224): 1.0)\n",
            "KR: 93 | GE for correct key (224): 1.0)\n",
            "KR: 94 | GE for correct key (224): 1.0)\n",
            "KR: 95 | GE for correct key (224): 1.0)\n",
            "KR: 96 | GE for correct key (224): 1.0)\n",
            "KR: 97 | GE for correct key (224): 1.0)\n",
            "KR: 98 | GE for correct key (224): 1.0)\n",
            "KR: 99 | GE for correct key (224): 1.0)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 137, 24)           456       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3288)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 600)               1973400   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 9)                 5409      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,061,065\n",
            "Trainable params: 3,061,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/200\n",
            "125/125 [==============================] - 25s 195ms/step - loss: 1.8413 - accuracy: 0.2493 - val_loss: 1.7935 - val_accuracy: 0.2612\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 1.7649 - accuracy: 0.2702 - val_loss: 1.7970 - val_accuracy: 0.2128\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 1.7611 - accuracy: 0.2696 - val_loss: 1.8004 - val_accuracy: 0.2696\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 1.7568 - accuracy: 0.2702 - val_loss: 1.7948 - val_accuracy: 0.2688\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.7501 - accuracy: 0.2768 - val_loss: 1.7976 - val_accuracy: 0.2638\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 1.7420 - accuracy: 0.2811 - val_loss: 1.8086 - val_accuracy: 0.2612\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 1.7333 - accuracy: 0.2826 - val_loss: 1.8087 - val_accuracy: 0.2446\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 24s 193ms/step - loss: 1.7214 - accuracy: 0.2884 - val_loss: 1.8226 - val_accuracy: 0.2472\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 1.7100 - accuracy: 0.2962 - val_loss: 1.8325 - val_accuracy: 0.2368\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.6992 - accuracy: 0.2999 - val_loss: 1.8435 - val_accuracy: 0.2344\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 1.6859 - accuracy: 0.3027 - val_loss: 1.8648 - val_accuracy: 0.2346\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 1.6746 - accuracy: 0.3104 - val_loss: 1.8741 - val_accuracy: 0.2236\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.6659 - accuracy: 0.3134 - val_loss: 1.8657 - val_accuracy: 0.2474\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 23s 180ms/step - loss: 1.6522 - accuracy: 0.3211 - val_loss: 1.8996 - val_accuracy: 0.2306\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.6433 - accuracy: 0.3258 - val_loss: 1.9086 - val_accuracy: 0.2496\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 23s 188ms/step - loss: 1.6315 - accuracy: 0.3287 - val_loss: 1.9128 - val_accuracy: 0.2382\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 24s 188ms/step - loss: 1.6210 - accuracy: 0.3356 - val_loss: 1.9383 - val_accuracy: 0.2212\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 1.6128 - accuracy: 0.3363 - val_loss: 1.9540 - val_accuracy: 0.2348\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 1.6043 - accuracy: 0.3417 - val_loss: 1.9439 - val_accuracy: 0.2316\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 1.5931 - accuracy: 0.3473 - val_loss: 1.9730 - val_accuracy: 0.2230\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 1.5877 - accuracy: 0.3495 - val_loss: 1.9959 - val_accuracy: 0.2218\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 1.5817 - accuracy: 0.3521 - val_loss: 1.9972 - val_accuracy: 0.2154\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.5666 - accuracy: 0.3599 - val_loss: 2.0282 - val_accuracy: 0.2118\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 1.5619 - accuracy: 0.3619 - val_loss: 2.0344 - val_accuracy: 0.2128\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 1.5467 - accuracy: 0.3667 - val_loss: 2.0503 - val_accuracy: 0.2030\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.5324 - accuracy: 0.3773 - val_loss: 2.0997 - val_accuracy: 0.2122\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 22s 180ms/step - loss: 1.5214 - accuracy: 0.3824 - val_loss: 2.0749 - val_accuracy: 0.2202\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.4988 - accuracy: 0.3910 - val_loss: 2.1001 - val_accuracy: 0.2202\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 1.4746 - accuracy: 0.4002 - val_loss: 2.1506 - val_accuracy: 0.2102\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 1.4511 - accuracy: 0.4138 - val_loss: 2.1510 - val_accuracy: 0.2142\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 1.4182 - accuracy: 0.4288 - val_loss: 2.2698 - val_accuracy: 0.2000\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 1.3779 - accuracy: 0.4490 - val_loss: 2.2938 - val_accuracy: 0.2138\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.3416 - accuracy: 0.4656 - val_loss: 2.3296 - val_accuracy: 0.1928\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 1.2899 - accuracy: 0.4878 - val_loss: 2.4183 - val_accuracy: 0.2028\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 23s 184ms/step - loss: 1.2381 - accuracy: 0.5141 - val_loss: 2.4664 - val_accuracy: 0.2152\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.1849 - accuracy: 0.5370 - val_loss: 2.5754 - val_accuracy: 0.1990\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 1.1111 - accuracy: 0.5686 - val_loss: 2.6649 - val_accuracy: 0.1934\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 1.0407 - accuracy: 0.6004 - val_loss: 2.7764 - val_accuracy: 0.1990\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.9726 - accuracy: 0.6293 - val_loss: 2.9743 - val_accuracy: 0.1912\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.8896 - accuracy: 0.6628 - val_loss: 3.2001 - val_accuracy: 0.1914\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.8152 - accuracy: 0.6940 - val_loss: 3.2425 - val_accuracy: 0.1886\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 24s 188ms/step - loss: 0.7389 - accuracy: 0.7254 - val_loss: 3.4457 - val_accuracy: 0.1926\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 24s 188ms/step - loss: 0.6365 - accuracy: 0.7667 - val_loss: 3.6151 - val_accuracy: 0.1978\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.5628 - accuracy: 0.7951 - val_loss: 3.8494 - val_accuracy: 0.1954\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 22s 180ms/step - loss: 0.4908 - accuracy: 0.8218 - val_loss: 4.1205 - val_accuracy: 0.1984\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.4364 - accuracy: 0.8432 - val_loss: 4.4213 - val_accuracy: 0.1970\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 0.3629 - accuracy: 0.8725 - val_loss: 4.6528 - val_accuracy: 0.1920\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 0.3270 - accuracy: 0.8851 - val_loss: 4.9022 - val_accuracy: 0.1982\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.2627 - accuracy: 0.9086 - val_loss: 5.2095 - val_accuracy: 0.1928\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 0.2140 - accuracy: 0.9284 - val_loss: 5.5297 - val_accuracy: 0.1950\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 0.1752 - accuracy: 0.9430 - val_loss: 5.7633 - val_accuracy: 0.2028\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.1657 - accuracy: 0.9442 - val_loss: 6.0251 - val_accuracy: 0.1942\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.1710 - accuracy: 0.9425 - val_loss: 6.0106 - val_accuracy: 0.1926\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.1811 - accuracy: 0.9367 - val_loss: 6.0996 - val_accuracy: 0.1936\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 0.1565 - accuracy: 0.9476 - val_loss: 6.2643 - val_accuracy: 0.2002\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 0.1692 - accuracy: 0.9424 - val_loss: 6.3254 - val_accuracy: 0.2022\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.1319 - accuracy: 0.9568 - val_loss: 6.4929 - val_accuracy: 0.2038\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 0.1011 - accuracy: 0.9683 - val_loss: 6.7229 - val_accuracy: 0.1982\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 0.1118 - accuracy: 0.9636 - val_loss: 6.7209 - val_accuracy: 0.1992\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 0.1315 - accuracy: 0.9555 - val_loss: 6.8931 - val_accuracy: 0.1934\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 0.1376 - accuracy: 0.9538 - val_loss: 6.8071 - val_accuracy: 0.1992\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 0.1159 - accuracy: 0.9609 - val_loss: 7.0428 - val_accuracy: 0.1952\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.1139 - accuracy: 0.9617 - val_loss: 7.0387 - val_accuracy: 0.2060\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.1015 - accuracy: 0.9656 - val_loss: 7.1674 - val_accuracy: 0.1916\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.1041 - accuracy: 0.9650 - val_loss: 7.0704 - val_accuracy: 0.1968\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 0.1515 - accuracy: 0.9480 - val_loss: 7.0363 - val_accuracy: 0.2030\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 0.1220 - accuracy: 0.9592 - val_loss: 7.0017 - val_accuracy: 0.2082\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.0830 - accuracy: 0.9737 - val_loss: 7.3240 - val_accuracy: 0.1992\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 22s 180ms/step - loss: 0.0707 - accuracy: 0.9768 - val_loss: 7.4207 - val_accuracy: 0.1974\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 0.0580 - accuracy: 0.9818 - val_loss: 7.4687 - val_accuracy: 0.2028\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 24s 188ms/step - loss: 0.0539 - accuracy: 0.9831 - val_loss: 7.6550 - val_accuracy: 0.2036\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 0.0998 - accuracy: 0.9663 - val_loss: 7.6071 - val_accuracy: 0.1982\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 0.1364 - accuracy: 0.9524 - val_loss: 7.3075 - val_accuracy: 0.1992\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.1596 - accuracy: 0.9443 - val_loss: 7.1807 - val_accuracy: 0.1982\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.1401 - accuracy: 0.9520 - val_loss: 7.2913 - val_accuracy: 0.1972\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 0.0768 - accuracy: 0.9742 - val_loss: 7.4207 - val_accuracy: 0.2024\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 24s 187ms/step - loss: 0.0514 - accuracy: 0.9838 - val_loss: 7.6293 - val_accuracy: 0.1920\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.0331 - accuracy: 0.9901 - val_loss: 7.8924 - val_accuracy: 0.1890\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.0232 - accuracy: 0.9940 - val_loss: 8.1342 - val_accuracy: 0.1906\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 0.0311 - accuracy: 0.9909 - val_loss: 8.1205 - val_accuracy: 0.1914\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 0.0676 - accuracy: 0.9775 - val_loss: 7.9515 - val_accuracy: 0.1868\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 0.2524 - accuracy: 0.9169 - val_loss: 7.1321 - val_accuracy: 0.1900\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 0.2247 - accuracy: 0.9220 - val_loss: 7.1423 - val_accuracy: 0.1966\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 0.1042 - accuracy: 0.9645 - val_loss: 7.3167 - val_accuracy: 0.1942\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 0.0416 - accuracy: 0.9874 - val_loss: 7.5719 - val_accuracy: 0.1968\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.0171 - accuracy: 0.9961 - val_loss: 7.8690 - val_accuracy: 0.1970\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 7.8664 - val_accuracy: 0.1946\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 7.9985 - val_accuracy: 0.1998\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 24s 193ms/step - loss: 9.1244e-04 - accuracy: 1.0000 - val_loss: 8.0741 - val_accuracy: 0.1998\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 7.5822e-04 - accuracy: 1.0000 - val_loss: 8.1363 - val_accuracy: 0.1996\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 6.5215e-04 - accuracy: 1.0000 - val_loss: 8.1993 - val_accuracy: 0.1988\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 5.7223e-04 - accuracy: 1.0000 - val_loss: 8.2550 - val_accuracy: 0.1984\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 5.0820e-04 - accuracy: 1.0000 - val_loss: 8.3100 - val_accuracy: 0.1982\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 4.5525e-04 - accuracy: 1.0000 - val_loss: 8.3645 - val_accuracy: 0.1968\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 22s 180ms/step - loss: 4.1057e-04 - accuracy: 1.0000 - val_loss: 8.4179 - val_accuracy: 0.1968\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 3.7203e-04 - accuracy: 1.0000 - val_loss: 8.4691 - val_accuracy: 0.1964\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 3.3859e-04 - accuracy: 1.0000 - val_loss: 8.5211 - val_accuracy: 0.1966\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 3.0919e-04 - accuracy: 1.0000 - val_loss: 8.5692 - val_accuracy: 0.1958\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 2.8315e-04 - accuracy: 1.0000 - val_loss: 8.6205 - val_accuracy: 0.1974\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 23s 180ms/step - loss: 2.5978e-04 - accuracy: 1.0000 - val_loss: 8.6707 - val_accuracy: 0.1972\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 2.3889e-04 - accuracy: 1.0000 - val_loss: 8.7175 - val_accuracy: 0.1972\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 2.1999e-04 - accuracy: 1.0000 - val_loss: 8.7667 - val_accuracy: 0.1972\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 2.0295e-04 - accuracy: 1.0000 - val_loss: 8.8167 - val_accuracy: 0.1968\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.8743e-04 - accuracy: 1.0000 - val_loss: 8.8636 - val_accuracy: 0.1970\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 1.7323e-04 - accuracy: 1.0000 - val_loss: 8.9123 - val_accuracy: 0.1964\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.6039e-04 - accuracy: 1.0000 - val_loss: 8.9612 - val_accuracy: 0.1968\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 1.4849e-04 - accuracy: 1.0000 - val_loss: 9.0110 - val_accuracy: 0.1960\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 1.3757e-04 - accuracy: 1.0000 - val_loss: 9.0592 - val_accuracy: 0.1956\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 1.2757e-04 - accuracy: 1.0000 - val_loss: 9.1075 - val_accuracy: 0.1956\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 1.1841e-04 - accuracy: 1.0000 - val_loss: 9.1577 - val_accuracy: 0.1964\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.0988e-04 - accuracy: 1.0000 - val_loss: 9.2055 - val_accuracy: 0.1964\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.0204e-04 - accuracy: 1.0000 - val_loss: 9.2558 - val_accuracy: 0.1958\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 9.4752e-05 - accuracy: 1.0000 - val_loss: 9.3052 - val_accuracy: 0.1966\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 8.8075e-05 - accuracy: 1.0000 - val_loss: 9.3533 - val_accuracy: 0.1966\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 8.1850e-05 - accuracy: 1.0000 - val_loss: 9.4024 - val_accuracy: 0.1976\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 7.6115e-05 - accuracy: 1.0000 - val_loss: 9.4515 - val_accuracy: 0.1982\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 7.0787e-05 - accuracy: 1.0000 - val_loss: 9.5018 - val_accuracy: 0.1982\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 6.5829e-05 - accuracy: 1.0000 - val_loss: 9.5497 - val_accuracy: 0.1982\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 6.1250e-05 - accuracy: 1.0000 - val_loss: 9.6020 - val_accuracy: 0.1978\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 5.7000e-05 - accuracy: 1.0000 - val_loss: 9.6520 - val_accuracy: 0.1988\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 5.3024e-05 - accuracy: 1.0000 - val_loss: 9.7021 - val_accuracy: 0.1976\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 4.9345e-05 - accuracy: 1.0000 - val_loss: 9.7521 - val_accuracy: 0.1980\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 4.5941e-05 - accuracy: 1.0000 - val_loss: 9.8016 - val_accuracy: 0.1976\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 4.2762e-05 - accuracy: 1.0000 - val_loss: 9.8531 - val_accuracy: 0.1978\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 3.9800e-05 - accuracy: 1.0000 - val_loss: 9.9032 - val_accuracy: 0.1988\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 3.7068e-05 - accuracy: 1.0000 - val_loss: 9.9548 - val_accuracy: 0.1984\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 3.4522e-05 - accuracy: 1.0000 - val_loss: 10.0060 - val_accuracy: 0.1990\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 3.2135e-05 - accuracy: 1.0000 - val_loss: 10.0566 - val_accuracy: 0.1998\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 2.9924e-05 - accuracy: 1.0000 - val_loss: 10.1067 - val_accuracy: 0.1994\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 2.7873e-05 - accuracy: 1.0000 - val_loss: 10.1594 - val_accuracy: 0.1992\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 2.5957e-05 - accuracy: 1.0000 - val_loss: 10.2110 - val_accuracy: 0.1990\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 2.4184e-05 - accuracy: 1.0000 - val_loss: 10.2600 - val_accuracy: 0.1986\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 2.2524e-05 - accuracy: 1.0000 - val_loss: 10.3125 - val_accuracy: 0.1986\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 2.0984e-05 - accuracy: 1.0000 - val_loss: 10.3640 - val_accuracy: 0.1986\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.9545e-05 - accuracy: 1.0000 - val_loss: 10.4151 - val_accuracy: 0.1978\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 1.8213e-05 - accuracy: 1.0000 - val_loss: 10.4668 - val_accuracy: 0.1976\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 1.6962e-05 - accuracy: 1.0000 - val_loss: 10.5195 - val_accuracy: 0.1982\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.5811e-05 - accuracy: 1.0000 - val_loss: 10.5719 - val_accuracy: 0.1982\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 1.4728e-05 - accuracy: 1.0000 - val_loss: 10.6240 - val_accuracy: 0.1982\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.3726e-05 - accuracy: 1.0000 - val_loss: 10.6761 - val_accuracy: 0.1982\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.2791e-05 - accuracy: 1.0000 - val_loss: 10.7290 - val_accuracy: 0.1988\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 1.1922e-05 - accuracy: 1.0000 - val_loss: 10.7805 - val_accuracy: 0.1976\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.1106e-05 - accuracy: 1.0000 - val_loss: 10.8299 - val_accuracy: 0.1978\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 23s 188ms/step - loss: 1.0352e-05 - accuracy: 1.0000 - val_loss: 10.8823 - val_accuracy: 0.1980\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 24s 194ms/step - loss: 9.6468e-06 - accuracy: 1.0000 - val_loss: 10.9351 - val_accuracy: 0.1984\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 25s 201ms/step - loss: 8.9933e-06 - accuracy: 1.0000 - val_loss: 10.9867 - val_accuracy: 0.1982\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 8.3857e-06 - accuracy: 1.0000 - val_loss: 11.0403 - val_accuracy: 0.1984\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 7.8149e-06 - accuracy: 1.0000 - val_loss: 11.0931 - val_accuracy: 0.1992\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 7.2864e-06 - accuracy: 1.0000 - val_loss: 11.1428 - val_accuracy: 0.1982\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 6.7938e-06 - accuracy: 1.0000 - val_loss: 11.1951 - val_accuracy: 0.1984\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 6.3339e-06 - accuracy: 1.0000 - val_loss: 11.2479 - val_accuracy: 0.1984\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 5.9059e-06 - accuracy: 1.0000 - val_loss: 11.2981 - val_accuracy: 0.1982\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 5.5072e-06 - accuracy: 1.0000 - val_loss: 11.3506 - val_accuracy: 0.1984\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 5.1341e-06 - accuracy: 1.0000 - val_loss: 11.4016 - val_accuracy: 0.1988\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 4.7903e-06 - accuracy: 1.0000 - val_loss: 11.4553 - val_accuracy: 0.1990\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 4.4664e-06 - accuracy: 1.0000 - val_loss: 11.5076 - val_accuracy: 0.1988\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 4.1672e-06 - accuracy: 1.0000 - val_loss: 11.5567 - val_accuracy: 0.1988\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 3.8858e-06 - accuracy: 1.0000 - val_loss: 11.6095 - val_accuracy: 0.1988\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 3.6251e-06 - accuracy: 1.0000 - val_loss: 11.6615 - val_accuracy: 0.1990\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 3.3828e-06 - accuracy: 1.0000 - val_loss: 11.7125 - val_accuracy: 0.1994\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 3.1571e-06 - accuracy: 1.0000 - val_loss: 11.7625 - val_accuracy: 0.1988\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 2.9463e-06 - accuracy: 1.0000 - val_loss: 11.8152 - val_accuracy: 0.1990\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 2.7485e-06 - accuracy: 1.0000 - val_loss: 11.8659 - val_accuracy: 0.1986\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 2.5649e-06 - accuracy: 1.0000 - val_loss: 11.9185 - val_accuracy: 0.1992\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 2.3944e-06 - accuracy: 1.0000 - val_loss: 11.9706 - val_accuracy: 0.1992\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 2.2348e-06 - accuracy: 1.0000 - val_loss: 12.0200 - val_accuracy: 0.1984\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 2.0870e-06 - accuracy: 1.0000 - val_loss: 12.0742 - val_accuracy: 0.1992\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 1.9485e-06 - accuracy: 1.0000 - val_loss: 12.1234 - val_accuracy: 0.1994\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 1.8198e-06 - accuracy: 1.0000 - val_loss: 12.1752 - val_accuracy: 0.1994\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 1.7002e-06 - accuracy: 1.0000 - val_loss: 12.2258 - val_accuracy: 0.2000\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 1.5876e-06 - accuracy: 1.0000 - val_loss: 12.2764 - val_accuracy: 0.1998\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.4829e-06 - accuracy: 1.0000 - val_loss: 12.3274 - val_accuracy: 0.1998\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 1.3853e-06 - accuracy: 1.0000 - val_loss: 12.3769 - val_accuracy: 0.1998\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 24s 193ms/step - loss: 1.2952e-06 - accuracy: 1.0000 - val_loss: 12.4277 - val_accuracy: 0.2006\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 1.2103e-06 - accuracy: 1.0000 - val_loss: 12.4769 - val_accuracy: 0.2008\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 1.1315e-06 - accuracy: 1.0000 - val_loss: 12.5268 - val_accuracy: 0.2004\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 1.0585e-06 - accuracy: 1.0000 - val_loss: 12.5765 - val_accuracy: 0.2002\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 24s 193ms/step - loss: 9.8890e-07 - accuracy: 1.0000 - val_loss: 12.6260 - val_accuracy: 0.2000\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 9.2451e-07 - accuracy: 1.0000 - val_loss: 12.6734 - val_accuracy: 0.2004\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 8.6516e-07 - accuracy: 1.0000 - val_loss: 12.7263 - val_accuracy: 0.2002\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 8.0932e-07 - accuracy: 1.0000 - val_loss: 12.7731 - val_accuracy: 0.2002\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 7.5768e-07 - accuracy: 1.0000 - val_loss: 12.8236 - val_accuracy: 0.2010\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 7.0885e-07 - accuracy: 1.0000 - val_loss: 12.8751 - val_accuracy: 0.2002\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 6.6303e-07 - accuracy: 1.0000 - val_loss: 12.9202 - val_accuracy: 0.2002\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 25s 199ms/step - loss: 6.2095e-07 - accuracy: 1.0000 - val_loss: 12.9691 - val_accuracy: 0.2002\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 5.8133e-07 - accuracy: 1.0000 - val_loss: 13.0168 - val_accuracy: 0.2004\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 5.4440e-07 - accuracy: 1.0000 - val_loss: 13.0650 - val_accuracy: 0.1996\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 5.1005e-07 - accuracy: 1.0000 - val_loss: 13.1123 - val_accuracy: 0.1998\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 4.7823e-07 - accuracy: 1.0000 - val_loss: 13.1605 - val_accuracy: 0.2002\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 4.4788e-07 - accuracy: 1.0000 - val_loss: 13.2081 - val_accuracy: 0.1994\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 4.1973e-07 - accuracy: 1.0000 - val_loss: 13.2577 - val_accuracy: 0.1994\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 3.9371e-07 - accuracy: 1.0000 - val_loss: 13.3034 - val_accuracy: 0.1990\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 3.6912e-07 - accuracy: 1.0000 - val_loss: 13.3477 - val_accuracy: 0.1992\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 3.4636e-07 - accuracy: 1.0000 - val_loss: 13.3944 - val_accuracy: 0.1996\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 25s 196ms/step - loss: 3.2503e-07 - accuracy: 1.0000 - val_loss: 13.4403 - val_accuracy: 0.1990\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 3.0493e-07 - accuracy: 1.0000 - val_loss: 13.4873 - val_accuracy: 0.1994\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 2.8607e-07 - accuracy: 1.0000 - val_loss: 13.5334 - val_accuracy: 0.1994\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 2.6870e-07 - accuracy: 1.0000 - val_loss: 13.5795 - val_accuracy: 0.1990\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 23s 188ms/step - loss: 2.5244e-07 - accuracy: 1.0000 - val_loss: 13.6244 - val_accuracy: 0.1992\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 2.3700e-07 - accuracy: 1.0000 - val_loss: 13.6706 - val_accuracy: 0.1990\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "KR: 0 | GE for correct key (224): 222.0)\n",
            "KR: 1 | GE for correct key (224): 222.0)\n",
            "KR: 2 | GE for correct key (224): 222.0)\n",
            "KR: 3 | GE for correct key (224): 222.0)\n",
            "KR: 4 | GE for correct key (224): 222.0)\n",
            "KR: 5 | GE for correct key (224): 222.0)\n",
            "KR: 6 | GE for correct key (224): 222.0)\n",
            "KR: 7 | GE for correct key (224): 222.0)\n",
            "KR: 8 | GE for correct key (224): 222.0)\n",
            "KR: 9 | GE for correct key (224): 222.0)\n",
            "KR: 10 | GE for correct key (224): 222.0)\n",
            "KR: 11 | GE for correct key (224): 222.0)\n",
            "KR: 12 | GE for correct key (224): 222.0)\n",
            "KR: 13 | GE for correct key (224): 222.0)\n",
            "KR: 14 | GE for correct key (224): 222.0)\n",
            "KR: 15 | GE for correct key (224): 222.0)\n",
            "KR: 16 | GE for correct key (224): 222.0)\n",
            "KR: 17 | GE for correct key (224): 222.0)\n",
            "KR: 18 | GE for correct key (224): 222.0)\n",
            "KR: 19 | GE for correct key (224): 222.0)\n",
            "KR: 20 | GE for correct key (224): 222.0)\n",
            "KR: 21 | GE for correct key (224): 222.0)\n",
            "KR: 22 | GE for correct key (224): 222.0)\n",
            "KR: 23 | GE for correct key (224): 222.0)\n",
            "KR: 24 | GE for correct key (224): 222.0)\n",
            "KR: 25 | GE for correct key (224): 222.0)\n",
            "KR: 26 | GE for correct key (224): 222.0)\n",
            "KR: 27 | GE for correct key (224): 222.0)\n",
            "KR: 28 | GE for correct key (224): 222.0)\n",
            "KR: 29 | GE for correct key (224): 222.0)\n",
            "KR: 30 | GE for correct key (224): 222.0)\n",
            "KR: 31 | GE for correct key (224): 222.0)\n",
            "KR: 32 | GE for correct key (224): 222.0)\n",
            "KR: 33 | GE for correct key (224): 222.0)\n",
            "KR: 34 | GE for correct key (224): 222.0)\n",
            "KR: 35 | GE for correct key (224): 222.0)\n",
            "KR: 36 | GE for correct key (224): 222.0)\n",
            "KR: 37 | GE for correct key (224): 222.0)\n",
            "KR: 38 | GE for correct key (224): 222.0)\n",
            "KR: 39 | GE for correct key (224): 222.0)\n",
            "KR: 40 | GE for correct key (224): 222.0)\n",
            "KR: 41 | GE for correct key (224): 222.0)\n",
            "KR: 42 | GE for correct key (224): 222.0)\n",
            "KR: 43 | GE for correct key (224): 222.0)\n",
            "KR: 44 | GE for correct key (224): 222.0)\n",
            "KR: 45 | GE for correct key (224): 222.0)\n",
            "KR: 46 | GE for correct key (224): 222.0)\n",
            "KR: 47 | GE for correct key (224): 222.0)\n",
            "KR: 48 | GE for correct key (224): 222.0)\n",
            "KR: 49 | GE for correct key (224): 222.0)\n",
            "KR: 50 | GE for correct key (224): 222.0)\n",
            "KR: 51 | GE for correct key (224): 222.0)\n",
            "KR: 52 | GE for correct key (224): 222.0)\n",
            "KR: 53 | GE for correct key (224): 222.0)\n",
            "KR: 54 | GE for correct key (224): 222.0)\n",
            "KR: 55 | GE for correct key (224): 222.0)\n",
            "KR: 56 | GE for correct key (224): 222.0)\n",
            "KR: 57 | GE for correct key (224): 222.0)\n",
            "KR: 58 | GE for correct key (224): 222.0)\n",
            "KR: 59 | GE for correct key (224): 222.0)\n",
            "KR: 60 | GE for correct key (224): 222.0)\n",
            "KR: 61 | GE for correct key (224): 222.0)\n",
            "KR: 62 | GE for correct key (224): 222.0)\n",
            "KR: 63 | GE for correct key (224): 222.0)\n",
            "KR: 64 | GE for correct key (224): 222.0)\n",
            "KR: 65 | GE for correct key (224): 222.0)\n",
            "KR: 66 | GE for correct key (224): 222.0)\n",
            "KR: 67 | GE for correct key (224): 222.0)\n",
            "KR: 68 | GE for correct key (224): 222.0)\n",
            "KR: 69 | GE for correct key (224): 222.0)\n",
            "KR: 70 | GE for correct key (224): 222.0)\n",
            "KR: 71 | GE for correct key (224): 222.0)\n",
            "KR: 72 | GE for correct key (224): 222.0)\n",
            "KR: 73 | GE for correct key (224): 222.0)\n",
            "KR: 74 | GE for correct key (224): 222.0)\n",
            "KR: 75 | GE for correct key (224): 222.0)\n",
            "KR: 76 | GE for correct key (224): 222.0)\n",
            "KR: 77 | GE for correct key (224): 222.0)\n",
            "KR: 78 | GE for correct key (224): 222.0)\n",
            "KR: 79 | GE for correct key (224): 222.0)\n",
            "KR: 80 | GE for correct key (224): 222.0)\n",
            "KR: 81 | GE for correct key (224): 222.0)\n",
            "KR: 82 | GE for correct key (224): 222.0)\n",
            "KR: 83 | GE for correct key (224): 222.0)\n",
            "KR: 84 | GE for correct key (224): 222.0)\n",
            "KR: 85 | GE for correct key (224): 222.0)\n",
            "KR: 86 | GE for correct key (224): 222.0)\n",
            "KR: 87 | GE for correct key (224): 222.0)\n",
            "KR: 88 | GE for correct key (224): 222.0)\n",
            "KR: 89 | GE for correct key (224): 222.0)\n",
            "KR: 90 | GE for correct key (224): 222.0)\n",
            "KR: 91 | GE for correct key (224): 222.0)\n",
            "KR: 92 | GE for correct key (224): 222.0)\n",
            "KR: 93 | GE for correct key (224): 222.0)\n",
            "KR: 94 | GE for correct key (224): 222.0)\n",
            "KR: 95 | GE for correct key (224): 222.0)\n",
            "KR: 96 | GE for correct key (224): 222.0)\n",
            "KR: 97 | GE for correct key (224): 222.0)\n",
            "KR: 98 | GE for correct key (224): 222.0)\n",
            "KR: 99 | GE for correct key (224): 222.0)\n",
            "157/157 [==============================] - 1s 8ms/step\n",
            "KR: 0 | GE for correct key (224): 106.0)\n",
            "KR: 1 | GE for correct key (224): 106.0)\n",
            "KR: 2 | GE for correct key (224): 106.0)\n",
            "KR: 3 | GE for correct key (224): 106.0)\n",
            "KR: 4 | GE for correct key (224): 106.0)\n",
            "KR: 5 | GE for correct key (224): 106.0)\n",
            "KR: 6 | GE for correct key (224): 106.0)\n",
            "KR: 7 | GE for correct key (224): 106.0)\n",
            "KR: 8 | GE for correct key (224): 106.0)\n",
            "KR: 9 | GE for correct key (224): 106.0)\n",
            "KR: 10 | GE for correct key (224): 106.0)\n",
            "KR: 11 | GE for correct key (224): 106.0)\n",
            "KR: 12 | GE for correct key (224): 106.0)\n",
            "KR: 13 | GE for correct key (224): 106.0)\n",
            "KR: 14 | GE for correct key (224): 106.0)\n",
            "KR: 15 | GE for correct key (224): 106.0)\n",
            "KR: 16 | GE for correct key (224): 106.0)\n",
            "KR: 17 | GE for correct key (224): 106.0)\n",
            "KR: 18 | GE for correct key (224): 106.0)\n",
            "KR: 19 | GE for correct key (224): 106.0)\n",
            "KR: 20 | GE for correct key (224): 106.0)\n",
            "KR: 21 | GE for correct key (224): 106.0)\n",
            "KR: 22 | GE for correct key (224): 106.0)\n",
            "KR: 23 | GE for correct key (224): 106.0)\n",
            "KR: 24 | GE for correct key (224): 106.0)\n",
            "KR: 25 | GE for correct key (224): 106.0)\n",
            "KR: 26 | GE for correct key (224): 106.0)\n",
            "KR: 27 | GE for correct key (224): 106.0)\n",
            "KR: 28 | GE for correct key (224): 106.0)\n",
            "KR: 29 | GE for correct key (224): 106.0)\n",
            "KR: 30 | GE for correct key (224): 106.0)\n",
            "KR: 31 | GE for correct key (224): 106.0)\n",
            "KR: 32 | GE for correct key (224): 106.0)\n",
            "KR: 33 | GE for correct key (224): 106.0)\n",
            "KR: 34 | GE for correct key (224): 106.0)\n",
            "KR: 35 | GE for correct key (224): 106.0)\n",
            "KR: 36 | GE for correct key (224): 106.0)\n",
            "KR: 37 | GE for correct key (224): 106.0)\n",
            "KR: 38 | GE for correct key (224): 106.0)\n",
            "KR: 39 | GE for correct key (224): 106.0)\n",
            "KR: 40 | GE for correct key (224): 106.0)\n",
            "KR: 41 | GE for correct key (224): 106.0)\n",
            "KR: 42 | GE for correct key (224): 106.0)\n",
            "KR: 43 | GE for correct key (224): 106.0)\n",
            "KR: 44 | GE for correct key (224): 106.0)\n",
            "KR: 45 | GE for correct key (224): 106.0)\n",
            "KR: 46 | GE for correct key (224): 106.0)\n",
            "KR: 47 | GE for correct key (224): 106.0)\n",
            "KR: 48 | GE for correct key (224): 106.0)\n",
            "KR: 49 | GE for correct key (224): 106.0)\n",
            "KR: 50 | GE for correct key (224): 106.0)\n",
            "KR: 51 | GE for correct key (224): 106.0)\n",
            "KR: 52 | GE for correct key (224): 106.0)\n",
            "KR: 53 | GE for correct key (224): 106.0)\n",
            "KR: 54 | GE for correct key (224): 106.0)\n",
            "KR: 55 | GE for correct key (224): 106.0)\n",
            "KR: 56 | GE for correct key (224): 106.0)\n",
            "KR: 57 | GE for correct key (224): 106.0)\n",
            "KR: 58 | GE for correct key (224): 106.0)\n",
            "KR: 59 | GE for correct key (224): 106.0)\n",
            "KR: 60 | GE for correct key (224): 106.0)\n",
            "KR: 61 | GE for correct key (224): 106.0)\n",
            "KR: 62 | GE for correct key (224): 106.0)\n",
            "KR: 63 | GE for correct key (224): 106.0)\n",
            "KR: 64 | GE for correct key (224): 106.0)\n",
            "KR: 65 | GE for correct key (224): 106.0)\n",
            "KR: 66 | GE for correct key (224): 106.0)\n",
            "KR: 67 | GE for correct key (224): 106.0)\n",
            "KR: 68 | GE for correct key (224): 106.0)\n",
            "KR: 69 | GE for correct key (224): 106.0)\n",
            "KR: 70 | GE for correct key (224): 106.0)\n",
            "KR: 71 | GE for correct key (224): 106.0)\n",
            "KR: 72 | GE for correct key (224): 106.0)\n",
            "KR: 73 | GE for correct key (224): 106.0)\n",
            "KR: 74 | GE for correct key (224): 106.0)\n",
            "KR: 75 | GE for correct key (224): 106.0)\n",
            "KR: 76 | GE for correct key (224): 106.0)\n",
            "KR: 77 | GE for correct key (224): 106.0)\n",
            "KR: 78 | GE for correct key (224): 106.0)\n",
            "KR: 79 | GE for correct key (224): 106.0)\n",
            "KR: 80 | GE for correct key (224): 106.0)\n",
            "KR: 81 | GE for correct key (224): 106.0)\n",
            "KR: 82 | GE for correct key (224): 106.0)\n",
            "KR: 83 | GE for correct key (224): 106.0)\n",
            "KR: 84 | GE for correct key (224): 106.0)\n",
            "KR: 85 | GE for correct key (224): 106.0)\n",
            "KR: 86 | GE for correct key (224): 106.0)\n",
            "KR: 87 | GE for correct key (224): 106.0)\n",
            "KR: 88 | GE for correct key (224): 106.0)\n",
            "KR: 89 | GE for correct key (224): 106.0)\n",
            "KR: 90 | GE for correct key (224): 106.0)\n",
            "KR: 91 | GE for correct key (224): 106.0)\n",
            "KR: 92 | GE for correct key (224): 106.0)\n",
            "KR: 93 | GE for correct key (224): 106.0)\n",
            "KR: 94 | GE for correct key (224): 106.0)\n",
            "KR: 95 | GE for correct key (224): 106.0)\n",
            "KR: 96 | GE for correct key (224): 106.0)\n",
            "KR: 97 | GE for correct key (224): 106.0)\n",
            "KR: 98 | GE for correct key (224): 106.0)\n",
            "KR: 99 | GE for correct key (224): 106.0)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 139, 8)            88        \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1112)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               556500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 500)               250500    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 4509      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 811,597\n",
            "Trainable params: 811,597\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/200\n",
            "125/125 [==============================] - 10s 70ms/step - loss: 1.7756 - accuracy: 0.2677 - val_loss: 1.7952 - val_accuracy: 0.2646\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 7s 54ms/step - loss: 1.7455 - accuracy: 0.2742 - val_loss: 1.7652 - val_accuracy: 0.2740\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 8s 67ms/step - loss: 1.7331 - accuracy: 0.2781 - val_loss: 1.7576 - val_accuracy: 0.2572\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 1.7241 - accuracy: 0.2802 - val_loss: 1.7589 - val_accuracy: 0.2696\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 1.7159 - accuracy: 0.2856 - val_loss: 1.7535 - val_accuracy: 0.2672\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 8s 61ms/step - loss: 1.7054 - accuracy: 0.2908 - val_loss: 1.7565 - val_accuracy: 0.2728\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 8s 60ms/step - loss: 1.6905 - accuracy: 0.3009 - val_loss: 1.7685 - val_accuracy: 0.2572\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 8s 66ms/step - loss: 1.6684 - accuracy: 0.3154 - val_loss: 1.7850 - val_accuracy: 0.2612\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 1.6397 - accuracy: 0.3292 - val_loss: 1.8040 - val_accuracy: 0.2420\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.5917 - accuracy: 0.3546 - val_loss: 1.8462 - val_accuracy: 0.2322\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.5357 - accuracy: 0.3821 - val_loss: 1.8916 - val_accuracy: 0.2338\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.4552 - accuracy: 0.4225 - val_loss: 1.9758 - val_accuracy: 0.2288\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.3574 - accuracy: 0.4658 - val_loss: 2.0712 - val_accuracy: 0.2306\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.2490 - accuracy: 0.5138 - val_loss: 2.2453 - val_accuracy: 0.2132\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.1287 - accuracy: 0.5684 - val_loss: 2.3967 - val_accuracy: 0.2222\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.9999 - accuracy: 0.6177 - val_loss: 2.6478 - val_accuracy: 0.2084\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 0.8821 - accuracy: 0.6664 - val_loss: 2.9301 - val_accuracy: 0.2196\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 0.7633 - accuracy: 0.7172 - val_loss: 3.2313 - val_accuracy: 0.2198\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 0.6593 - accuracy: 0.7565 - val_loss: 3.4963 - val_accuracy: 0.2068\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 0.5664 - accuracy: 0.7934 - val_loss: 3.9677 - val_accuracy: 0.2120\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.4898 - accuracy: 0.8215 - val_loss: 4.3129 - val_accuracy: 0.2124\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.4194 - accuracy: 0.8484 - val_loss: 4.7131 - val_accuracy: 0.2102\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 0.3631 - accuracy: 0.8694 - val_loss: 5.1918 - val_accuracy: 0.2092\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 0.3093 - accuracy: 0.8917 - val_loss: 5.6339 - val_accuracy: 0.2112\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.2670 - accuracy: 0.9059 - val_loss: 5.9173 - val_accuracy: 0.2032\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.2241 - accuracy: 0.9237 - val_loss: 6.4426 - val_accuracy: 0.2046\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.1993 - accuracy: 0.9325 - val_loss: 6.8172 - val_accuracy: 0.2108\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.1813 - accuracy: 0.9394 - val_loss: 7.0224 - val_accuracy: 0.2060\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 0.1459 - accuracy: 0.9533 - val_loss: 7.5543 - val_accuracy: 0.2088\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.1331 - accuracy: 0.9571 - val_loss: 7.8334 - val_accuracy: 0.2136\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.1364 - accuracy: 0.9547 - val_loss: 7.9450 - val_accuracy: 0.2012\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 0.1238 - accuracy: 0.9605 - val_loss: 8.2562 - val_accuracy: 0.2106\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 0.1165 - accuracy: 0.9627 - val_loss: 8.5465 - val_accuracy: 0.2042\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 0.0996 - accuracy: 0.9694 - val_loss: 8.7093 - val_accuracy: 0.2090\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0705 - accuracy: 0.9802 - val_loss: 9.2007 - val_accuracy: 0.2142\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 0.0623 - accuracy: 0.9821 - val_loss: 9.3507 - val_accuracy: 0.2052\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 0.0431 - accuracy: 0.9893 - val_loss: 9.7667 - val_accuracy: 0.2102\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0469 - accuracy: 0.9872 - val_loss: 9.9827 - val_accuracy: 0.2082\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 8s 66ms/step - loss: 0.0522 - accuracy: 0.9848 - val_loss: 10.1679 - val_accuracy: 0.2102\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 7s 55ms/step - loss: 0.0971 - accuracy: 0.9669 - val_loss: 9.6242 - val_accuracy: 0.2012\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.1324 - accuracy: 0.9545 - val_loss: 9.4908 - val_accuracy: 0.1974\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.0941 - accuracy: 0.9690 - val_loss: 9.6011 - val_accuracy: 0.2088\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 9s 71ms/step - loss: 0.0522 - accuracy: 0.9847 - val_loss: 10.0482 - val_accuracy: 0.2102\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.0285 - accuracy: 0.9931 - val_loss: 10.4585 - val_accuracy: 0.2130\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.0161 - accuracy: 0.9972 - val_loss: 10.8053 - val_accuracy: 0.2090\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.0050 - accuracy: 0.9999 - val_loss: 11.1222 - val_accuracy: 0.2098\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 11.4202 - val_accuracy: 0.2110\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 11.6356 - val_accuracy: 0.2114\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 11.7825 - val_accuracy: 0.2104\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 11.9209 - val_accuracy: 0.2106\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 12.0506 - val_accuracy: 0.2092\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 9.1173e-04 - accuracy: 1.0000 - val_loss: 12.1776 - val_accuracy: 0.2106\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 9s 71ms/step - loss: 8.1324e-04 - accuracy: 1.0000 - val_loss: 12.3040 - val_accuracy: 0.2106\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 7.2803e-04 - accuracy: 1.0000 - val_loss: 12.4216 - val_accuracy: 0.2102\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 6.5341e-04 - accuracy: 1.0000 - val_loss: 12.5371 - val_accuracy: 0.2100\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 6s 52ms/step - loss: 5.9281e-04 - accuracy: 1.0000 - val_loss: 12.6436 - val_accuracy: 0.2100\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 5.3808e-04 - accuracy: 1.0000 - val_loss: 12.7548 - val_accuracy: 0.2102\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 6s 52ms/step - loss: 4.9125e-04 - accuracy: 1.0000 - val_loss: 12.8665 - val_accuracy: 0.2102\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 4.4780e-04 - accuracy: 1.0000 - val_loss: 12.9769 - val_accuracy: 0.2114\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 4.1052e-04 - accuracy: 1.0000 - val_loss: 13.0691 - val_accuracy: 0.2108\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 3.7376e-04 - accuracy: 1.0000 - val_loss: 13.1660 - val_accuracy: 0.2102\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 3.4421e-04 - accuracy: 1.0000 - val_loss: 13.2709 - val_accuracy: 0.2106\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 8s 66ms/step - loss: 3.1555e-04 - accuracy: 1.0000 - val_loss: 13.3747 - val_accuracy: 0.2108\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 2.9149e-04 - accuracy: 1.0000 - val_loss: 13.4568 - val_accuracy: 0.2114\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 2.6870e-04 - accuracy: 1.0000 - val_loss: 13.5547 - val_accuracy: 0.2088\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 2.4766e-04 - accuracy: 1.0000 - val_loss: 13.6592 - val_accuracy: 0.2090\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 2.2840e-04 - accuracy: 1.0000 - val_loss: 13.7562 - val_accuracy: 0.2090\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 8s 68ms/step - loss: 2.1184e-04 - accuracy: 1.0000 - val_loss: 13.8497 - val_accuracy: 0.2088\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 7s 55ms/step - loss: 1.9574e-04 - accuracy: 1.0000 - val_loss: 13.9404 - val_accuracy: 0.2106\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.8079e-04 - accuracy: 1.0000 - val_loss: 14.0280 - val_accuracy: 0.2086\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 6s 52ms/step - loss: 1.6797e-04 - accuracy: 1.0000 - val_loss: 14.1243 - val_accuracy: 0.2094\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.5607e-04 - accuracy: 1.0000 - val_loss: 14.2192 - val_accuracy: 0.2100\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.4408e-04 - accuracy: 1.0000 - val_loss: 14.3063 - val_accuracy: 0.2104\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.3383e-04 - accuracy: 1.0000 - val_loss: 14.3958 - val_accuracy: 0.2102\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.2439e-04 - accuracy: 1.0000 - val_loss: 14.4791 - val_accuracy: 0.2094\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.1515e-04 - accuracy: 1.0000 - val_loss: 14.5842 - val_accuracy: 0.2100\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 6s 52ms/step - loss: 1.0708e-04 - accuracy: 1.0000 - val_loss: 14.6631 - val_accuracy: 0.2086\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 9.9744e-05 - accuracy: 1.0000 - val_loss: 14.7498 - val_accuracy: 0.2088\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 9.2726e-05 - accuracy: 1.0000 - val_loss: 14.8430 - val_accuracy: 0.2100\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 10s 84ms/step - loss: 8.6136e-05 - accuracy: 1.0000 - val_loss: 14.9299 - val_accuracy: 0.2086\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 7.9775e-05 - accuracy: 1.0000 - val_loss: 15.0166 - val_accuracy: 0.2090\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 7.4072e-05 - accuracy: 1.0000 - val_loss: 15.1037 - val_accuracy: 0.2078\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 6.9014e-05 - accuracy: 1.0000 - val_loss: 15.1943 - val_accuracy: 0.2096\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 6.4202e-05 - accuracy: 1.0000 - val_loss: 15.2784 - val_accuracy: 0.2094\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 5.9770e-05 - accuracy: 1.0000 - val_loss: 15.3713 - val_accuracy: 0.2104\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 5.5546e-05 - accuracy: 1.0000 - val_loss: 15.4579 - val_accuracy: 0.2082\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 5.1654e-05 - accuracy: 1.0000 - val_loss: 15.5351 - val_accuracy: 0.2092\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 4.7989e-05 - accuracy: 1.0000 - val_loss: 15.6275 - val_accuracy: 0.2080\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 4.4660e-05 - accuracy: 1.0000 - val_loss: 15.7088 - val_accuracy: 0.2098\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 4.1723e-05 - accuracy: 1.0000 - val_loss: 15.8044 - val_accuracy: 0.2094\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 3.8657e-05 - accuracy: 1.0000 - val_loss: 15.8855 - val_accuracy: 0.2088\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 3.5978e-05 - accuracy: 1.0000 - val_loss: 15.9743 - val_accuracy: 0.2074\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 3.3538e-05 - accuracy: 1.0000 - val_loss: 16.0617 - val_accuracy: 0.2092\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 8s 67ms/step - loss: 3.1261e-05 - accuracy: 1.0000 - val_loss: 16.1428 - val_accuracy: 0.2080\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 2.9076e-05 - accuracy: 1.0000 - val_loss: 16.2469 - val_accuracy: 0.2088\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 2.7011e-05 - accuracy: 1.0000 - val_loss: 16.3299 - val_accuracy: 0.2094\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 8s 64ms/step - loss: 2.5208e-05 - accuracy: 1.0000 - val_loss: 16.4092 - val_accuracy: 0.2094\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 2.3345e-05 - accuracy: 1.0000 - val_loss: 16.5029 - val_accuracy: 0.2080\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 8s 67ms/step - loss: 2.1864e-05 - accuracy: 1.0000 - val_loss: 16.5833 - val_accuracy: 0.2082\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 7s 55ms/step - loss: 2.0238e-05 - accuracy: 1.0000 - val_loss: 16.6795 - val_accuracy: 0.2088\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 9s 72ms/step - loss: 1.8847e-05 - accuracy: 1.0000 - val_loss: 16.7682 - val_accuracy: 0.2096\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.7598e-05 - accuracy: 1.0000 - val_loss: 16.8476 - val_accuracy: 0.2090\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.6345e-05 - accuracy: 1.0000 - val_loss: 16.9380 - val_accuracy: 0.2090\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.5179e-05 - accuracy: 1.0000 - val_loss: 17.0320 - val_accuracy: 0.2088\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.4171e-05 - accuracy: 1.0000 - val_loss: 17.1078 - val_accuracy: 0.2088\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.3179e-05 - accuracy: 1.0000 - val_loss: 17.2014 - val_accuracy: 0.2096\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.2249e-05 - accuracy: 1.0000 - val_loss: 17.2949 - val_accuracy: 0.2094\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 6s 52ms/step - loss: 1.1402e-05 - accuracy: 1.0000 - val_loss: 17.3775 - val_accuracy: 0.2096\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.0614e-05 - accuracy: 1.0000 - val_loss: 17.4571 - val_accuracy: 0.2102\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 9.8546e-06 - accuracy: 1.0000 - val_loss: 17.5627 - val_accuracy: 0.2094\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 9.1943e-06 - accuracy: 1.0000 - val_loss: 17.6392 - val_accuracy: 0.2082\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 8.5481e-06 - accuracy: 1.0000 - val_loss: 17.7231 - val_accuracy: 0.2088\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 7.9713e-06 - accuracy: 1.0000 - val_loss: 17.8131 - val_accuracy: 0.2094\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 7.3959e-06 - accuracy: 1.0000 - val_loss: 17.8999 - val_accuracy: 0.2094\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 6.8755e-06 - accuracy: 1.0000 - val_loss: 17.9867 - val_accuracy: 0.2106\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 6.4095e-06 - accuracy: 1.0000 - val_loss: 18.0726 - val_accuracy: 0.2098\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 5.9758e-06 - accuracy: 1.0000 - val_loss: 18.1642 - val_accuracy: 0.2100\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 5.5588e-06 - accuracy: 1.0000 - val_loss: 18.2553 - val_accuracy: 0.2100\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 5.1743e-06 - accuracy: 1.0000 - val_loss: 18.3234 - val_accuracy: 0.2088\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 4.8215e-06 - accuracy: 1.0000 - val_loss: 18.4236 - val_accuracy: 0.2106\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 4.4757e-06 - accuracy: 1.0000 - val_loss: 18.5099 - val_accuracy: 0.2102\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 4.1685e-06 - accuracy: 1.0000 - val_loss: 18.5897 - val_accuracy: 0.2098\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 3.8824e-06 - accuracy: 1.0000 - val_loss: 18.6755 - val_accuracy: 0.2104\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 3.6182e-06 - accuracy: 1.0000 - val_loss: 18.7647 - val_accuracy: 0.2110\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 8s 66ms/step - loss: 3.3753e-06 - accuracy: 1.0000 - val_loss: 18.8505 - val_accuracy: 0.2098\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 3.1463e-06 - accuracy: 1.0000 - val_loss: 18.9277 - val_accuracy: 0.2108\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 2.9220e-06 - accuracy: 1.0000 - val_loss: 19.0090 - val_accuracy: 0.2106\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 2.7189e-06 - accuracy: 1.0000 - val_loss: 19.0949 - val_accuracy: 0.2108\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 8s 60ms/step - loss: 2.5526e-06 - accuracy: 1.0000 - val_loss: 19.1755 - val_accuracy: 0.2098\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 2.3733e-06 - accuracy: 1.0000 - val_loss: 19.2587 - val_accuracy: 0.2098\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 2.2134e-06 - accuracy: 1.0000 - val_loss: 19.3423 - val_accuracy: 0.2108\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 2.0679e-06 - accuracy: 1.0000 - val_loss: 19.4240 - val_accuracy: 0.2102\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.9245e-06 - accuracy: 1.0000 - val_loss: 19.5028 - val_accuracy: 0.2110\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.7943e-06 - accuracy: 1.0000 - val_loss: 19.5848 - val_accuracy: 0.2098\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.6764e-06 - accuracy: 1.0000 - val_loss: 19.6741 - val_accuracy: 0.2112\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.5620e-06 - accuracy: 1.0000 - val_loss: 19.7469 - val_accuracy: 0.2104\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.4606e-06 - accuracy: 1.0000 - val_loss: 19.8472 - val_accuracy: 0.2116\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.3681e-06 - accuracy: 1.0000 - val_loss: 19.9181 - val_accuracy: 0.2108\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.2729e-06 - accuracy: 1.0000 - val_loss: 19.9980 - val_accuracy: 0.2108\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.1902e-06 - accuracy: 1.0000 - val_loss: 20.0734 - val_accuracy: 0.2114\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.1110e-06 - accuracy: 1.0000 - val_loss: 20.1634 - val_accuracy: 0.2116\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.0398e-06 - accuracy: 1.0000 - val_loss: 20.2420 - val_accuracy: 0.2108\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 9.6992e-07 - accuracy: 1.0000 - val_loss: 20.3222 - val_accuracy: 0.2106\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 9.0574e-07 - accuracy: 1.0000 - val_loss: 20.3913 - val_accuracy: 0.2112\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 8.4422e-07 - accuracy: 1.0000 - val_loss: 20.4851 - val_accuracy: 0.2110\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 7.9169e-07 - accuracy: 1.0000 - val_loss: 20.5599 - val_accuracy: 0.2096\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 7.4312e-07 - accuracy: 1.0000 - val_loss: 20.6301 - val_accuracy: 0.2112\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 6.9570e-07 - accuracy: 1.0000 - val_loss: 20.7115 - val_accuracy: 0.2106\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 6.4815e-07 - accuracy: 1.0000 - val_loss: 20.7955 - val_accuracy: 0.2104\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 6.0662e-07 - accuracy: 1.0000 - val_loss: 20.8676 - val_accuracy: 0.2104\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 5.6661e-07 - accuracy: 1.0000 - val_loss: 20.9466 - val_accuracy: 0.2108\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 5.3013e-07 - accuracy: 1.0000 - val_loss: 21.0259 - val_accuracy: 0.2108\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 4.9894e-07 - accuracy: 1.0000 - val_loss: 21.1007 - val_accuracy: 0.2108\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 4.6675e-07 - accuracy: 1.0000 - val_loss: 21.1826 - val_accuracy: 0.2096\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 4.3795e-07 - accuracy: 1.0000 - val_loss: 21.2464 - val_accuracy: 0.2106\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 8s 65ms/step - loss: 4.0892e-07 - accuracy: 1.0000 - val_loss: 21.3342 - val_accuracy: 0.2110\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 3.8395e-07 - accuracy: 1.0000 - val_loss: 21.4005 - val_accuracy: 0.2114\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 3.6003e-07 - accuracy: 1.0000 - val_loss: 21.4744 - val_accuracy: 0.2104\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 3.3712e-07 - accuracy: 1.0000 - val_loss: 21.5504 - val_accuracy: 0.2110\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 3.1717e-07 - accuracy: 1.0000 - val_loss: 21.6321 - val_accuracy: 0.2114\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 2.9689e-07 - accuracy: 1.0000 - val_loss: 21.6961 - val_accuracy: 0.2096\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 2.7844e-07 - accuracy: 1.0000 - val_loss: 21.7747 - val_accuracy: 0.2114\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 8s 68ms/step - loss: 2.6247e-07 - accuracy: 1.0000 - val_loss: 21.8461 - val_accuracy: 0.2114\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 7s 55ms/step - loss: 2.4507e-07 - accuracy: 1.0000 - val_loss: 21.9245 - val_accuracy: 0.2114\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 2.3020e-07 - accuracy: 1.0000 - val_loss: 21.9982 - val_accuracy: 0.2102\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 2.1573e-07 - accuracy: 1.0000 - val_loss: 22.0560 - val_accuracy: 0.2100\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 2.0314e-07 - accuracy: 1.0000 - val_loss: 22.1308 - val_accuracy: 0.2106\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.9038e-07 - accuracy: 1.0000 - val_loss: 22.2073 - val_accuracy: 0.2100\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.7976e-07 - accuracy: 1.0000 - val_loss: 22.2690 - val_accuracy: 0.2112\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.6880e-07 - accuracy: 1.0000 - val_loss: 22.3458 - val_accuracy: 0.2112\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.5833e-07 - accuracy: 1.0000 - val_loss: 22.4093 - val_accuracy: 0.2112\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.4888e-07 - accuracy: 1.0000 - val_loss: 22.4814 - val_accuracy: 0.2102\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.3999e-07 - accuracy: 1.0000 - val_loss: 22.5385 - val_accuracy: 0.2110\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.3169e-07 - accuracy: 1.0000 - val_loss: 22.6082 - val_accuracy: 0.2110\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 1.2388e-07 - accuracy: 1.0000 - val_loss: 22.6658 - val_accuracy: 0.2100\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 1.1725e-07 - accuracy: 1.0000 - val_loss: 22.7404 - val_accuracy: 0.2096\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.0974e-07 - accuracy: 1.0000 - val_loss: 22.7988 - val_accuracy: 0.2104\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.0356e-07 - accuracy: 1.0000 - val_loss: 22.8666 - val_accuracy: 0.2102\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 9.7473e-08 - accuracy: 1.0000 - val_loss: 22.9275 - val_accuracy: 0.2100\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 9.1844e-08 - accuracy: 1.0000 - val_loss: 22.9924 - val_accuracy: 0.2106\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 8.6575e-08 - accuracy: 1.0000 - val_loss: 23.0543 - val_accuracy: 0.2110\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 7s 55ms/step - loss: 8.1596e-08 - accuracy: 1.0000 - val_loss: 23.1173 - val_accuracy: 0.2096\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 10s 80ms/step - loss: 7.7040e-08 - accuracy: 1.0000 - val_loss: 23.1703 - val_accuracy: 0.2102\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 8s 64ms/step - loss: 7.2453e-08 - accuracy: 1.0000 - val_loss: 23.2364 - val_accuracy: 0.2100\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 6.8204e-08 - accuracy: 1.0000 - val_loss: 23.2918 - val_accuracy: 0.2096\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 6.4509e-08 - accuracy: 1.0000 - val_loss: 23.3519 - val_accuracy: 0.2104\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 8s 60ms/step - loss: 6.0673e-08 - accuracy: 1.0000 - val_loss: 23.4179 - val_accuracy: 0.2100\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 8s 67ms/step - loss: 5.7192e-08 - accuracy: 1.0000 - val_loss: 23.4650 - val_accuracy: 0.2106\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 5.3859e-08 - accuracy: 1.0000 - val_loss: 23.5177 - val_accuracy: 0.2112\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 5.0552e-08 - accuracy: 1.0000 - val_loss: 23.5765 - val_accuracy: 0.2094\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 4.7877e-08 - accuracy: 1.0000 - val_loss: 23.6312 - val_accuracy: 0.2112\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 9s 69ms/step - loss: 4.4847e-08 - accuracy: 1.0000 - val_loss: 23.6847 - val_accuracy: 0.2098\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 4.2226e-08 - accuracy: 1.0000 - val_loss: 23.7447 - val_accuracy: 0.2100\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 3.9783e-08 - accuracy: 1.0000 - val_loss: 23.7868 - val_accuracy: 0.2102\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 3.7224e-08 - accuracy: 1.0000 - val_loss: 23.8333 - val_accuracy: 0.2098\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 3.5062e-08 - accuracy: 1.0000 - val_loss: 23.8880 - val_accuracy: 0.2104\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 3.3183e-08 - accuracy: 1.0000 - val_loss: 23.9394 - val_accuracy: 0.2104\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 3.1054e-08 - accuracy: 1.0000 - val_loss: 23.9914 - val_accuracy: 0.2104\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 7s 53ms/step - loss: 2.9404e-08 - accuracy: 1.0000 - val_loss: 24.0336 - val_accuracy: 0.2108\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 2.7378e-08 - accuracy: 1.0000 - val_loss: 24.0861 - val_accuracy: 0.2110\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "KR: 0 | GE for correct key (224): 1.0)\n",
            "KR: 1 | GE for correct key (224): 1.0)\n",
            "KR: 2 | GE for correct key (224): 1.0)\n",
            "KR: 3 | GE for correct key (224): 1.0)\n",
            "KR: 4 | GE for correct key (224): 1.0)\n",
            "KR: 5 | GE for correct key (224): 1.0)\n",
            "KR: 6 | GE for correct key (224): 1.0)\n",
            "KR: 7 | GE for correct key (224): 1.0)\n",
            "KR: 8 | GE for correct key (224): 1.0)\n",
            "KR: 9 | GE for correct key (224): 1.0)\n",
            "KR: 10 | GE for correct key (224): 1.0)\n",
            "KR: 11 | GE for correct key (224): 1.0)\n",
            "KR: 12 | GE for correct key (224): 1.0)\n",
            "KR: 13 | GE for correct key (224): 1.0)\n",
            "KR: 14 | GE for correct key (224): 1.0)\n",
            "KR: 15 | GE for correct key (224): 1.0)\n",
            "KR: 16 | GE for correct key (224): 1.0)\n",
            "KR: 17 | GE for correct key (224): 1.0)\n",
            "KR: 18 | GE for correct key (224): 1.0)\n",
            "KR: 19 | GE for correct key (224): 1.0)\n",
            "KR: 20 | GE for correct key (224): 1.0)\n",
            "KR: 21 | GE for correct key (224): 1.0)\n",
            "KR: 22 | GE for correct key (224): 1.0)\n",
            "KR: 23 | GE for correct key (224): 1.0)\n",
            "KR: 24 | GE for correct key (224): 1.0)\n",
            "KR: 25 | GE for correct key (224): 1.0)\n",
            "KR: 26 | GE for correct key (224): 1.0)\n",
            "KR: 27 | GE for correct key (224): 1.0)\n",
            "KR: 28 | GE for correct key (224): 1.0)\n",
            "KR: 29 | GE for correct key (224): 1.0)\n",
            "KR: 30 | GE for correct key (224): 1.0)\n",
            "KR: 31 | GE for correct key (224): 1.0)\n",
            "KR: 32 | GE for correct key (224): 1.0)\n",
            "KR: 33 | GE for correct key (224): 1.0)\n",
            "KR: 34 | GE for correct key (224): 1.0)\n",
            "KR: 35 | GE for correct key (224): 1.0)\n",
            "KR: 36 | GE for correct key (224): 1.0)\n",
            "KR: 37 | GE for correct key (224): 1.0)\n",
            "KR: 38 | GE for correct key (224): 1.0)\n",
            "KR: 39 | GE for correct key (224): 1.0)\n",
            "KR: 40 | GE for correct key (224): 1.0)\n",
            "KR: 41 | GE for correct key (224): 1.0)\n",
            "KR: 42 | GE for correct key (224): 1.0)\n",
            "KR: 43 | GE for correct key (224): 1.0)\n",
            "KR: 44 | GE for correct key (224): 1.0)\n",
            "KR: 45 | GE for correct key (224): 1.0)\n",
            "KR: 46 | GE for correct key (224): 1.0)\n",
            "KR: 47 | GE for correct key (224): 1.0)\n",
            "KR: 48 | GE for correct key (224): 1.0)\n",
            "KR: 49 | GE for correct key (224): 1.0)\n",
            "KR: 50 | GE for correct key (224): 1.0)\n",
            "KR: 51 | GE for correct key (224): 1.0)\n",
            "KR: 52 | GE for correct key (224): 1.0)\n",
            "KR: 53 | GE for correct key (224): 1.0)\n",
            "KR: 54 | GE for correct key (224): 1.0)\n",
            "KR: 55 | GE for correct key (224): 1.0)\n",
            "KR: 56 | GE for correct key (224): 1.0)\n",
            "KR: 57 | GE for correct key (224): 1.0)\n",
            "KR: 58 | GE for correct key (224): 1.0)\n",
            "KR: 59 | GE for correct key (224): 1.0)\n",
            "KR: 60 | GE for correct key (224): 1.0)\n",
            "KR: 61 | GE for correct key (224): 1.0)\n",
            "KR: 62 | GE for correct key (224): 1.0)\n",
            "KR: 63 | GE for correct key (224): 1.0)\n",
            "KR: 64 | GE for correct key (224): 1.0)\n",
            "KR: 65 | GE for correct key (224): 1.0)\n",
            "KR: 66 | GE for correct key (224): 1.0)\n",
            "KR: 67 | GE for correct key (224): 1.0)\n",
            "KR: 68 | GE for correct key (224): 1.0)\n",
            "KR: 69 | GE for correct key (224): 1.0)\n",
            "KR: 70 | GE for correct key (224): 1.0)\n",
            "KR: 71 | GE for correct key (224): 1.0)\n",
            "KR: 72 | GE for correct key (224): 1.0)\n",
            "KR: 73 | GE for correct key (224): 1.0)\n",
            "KR: 74 | GE for correct key (224): 1.0)\n",
            "KR: 75 | GE for correct key (224): 1.0)\n",
            "KR: 76 | GE for correct key (224): 1.0)\n",
            "KR: 77 | GE for correct key (224): 1.0)\n",
            "KR: 78 | GE for correct key (224): 1.0)\n",
            "KR: 79 | GE for correct key (224): 1.0)\n",
            "KR: 80 | GE for correct key (224): 1.0)\n",
            "KR: 81 | GE for correct key (224): 1.0)\n",
            "KR: 82 | GE for correct key (224): 1.0)\n",
            "KR: 83 | GE for correct key (224): 1.0)\n",
            "KR: 84 | GE for correct key (224): 1.0)\n",
            "KR: 85 | GE for correct key (224): 1.0)\n",
            "KR: 86 | GE for correct key (224): 1.0)\n",
            "KR: 87 | GE for correct key (224): 1.0)\n",
            "KR: 88 | GE for correct key (224): 1.0)\n",
            "KR: 89 | GE for correct key (224): 1.0)\n",
            "KR: 90 | GE for correct key (224): 1.0)\n",
            "KR: 91 | GE for correct key (224): 1.0)\n",
            "KR: 92 | GE for correct key (224): 1.0)\n",
            "KR: 93 | GE for correct key (224): 1.0)\n",
            "KR: 94 | GE for correct key (224): 1.0)\n",
            "KR: 95 | GE for correct key (224): 1.0)\n",
            "KR: 96 | GE for correct key (224): 1.0)\n",
            "KR: 97 | GE for correct key (224): 1.0)\n",
            "KR: 98 | GE for correct key (224): 1.0)\n",
            "KR: 99 | GE for correct key (224): 1.0)\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "KR: 0 | GE for correct key (224): 1.0)\n",
            "KR: 1 | GE for correct key (224): 1.0)\n",
            "KR: 2 | GE for correct key (224): 1.0)\n",
            "KR: 3 | GE for correct key (224): 1.0)\n",
            "KR: 4 | GE for correct key (224): 1.0)\n",
            "KR: 5 | GE for correct key (224): 1.0)\n",
            "KR: 6 | GE for correct key (224): 1.0)\n",
            "KR: 7 | GE for correct key (224): 1.0)\n",
            "KR: 8 | GE for correct key (224): 1.0)\n",
            "KR: 9 | GE for correct key (224): 1.0)\n",
            "KR: 10 | GE for correct key (224): 1.0)\n",
            "KR: 11 | GE for correct key (224): 1.0)\n",
            "KR: 12 | GE for correct key (224): 1.0)\n",
            "KR: 13 | GE for correct key (224): 1.0)\n",
            "KR: 14 | GE for correct key (224): 1.0)\n",
            "KR: 15 | GE for correct key (224): 1.0)\n",
            "KR: 16 | GE for correct key (224): 1.0)\n",
            "KR: 17 | GE for correct key (224): 1.0)\n",
            "KR: 18 | GE for correct key (224): 1.0)\n",
            "KR: 19 | GE for correct key (224): 1.0)\n",
            "KR: 20 | GE for correct key (224): 1.0)\n",
            "KR: 21 | GE for correct key (224): 1.0)\n",
            "KR: 22 | GE for correct key (224): 1.0)\n",
            "KR: 23 | GE for correct key (224): 1.0)\n",
            "KR: 24 | GE for correct key (224): 1.0)\n",
            "KR: 25 | GE for correct key (224): 1.0)\n",
            "KR: 26 | GE for correct key (224): 1.0)\n",
            "KR: 27 | GE for correct key (224): 1.0)\n",
            "KR: 28 | GE for correct key (224): 1.0)\n",
            "KR: 29 | GE for correct key (224): 1.0)\n",
            "KR: 30 | GE for correct key (224): 1.0)\n",
            "KR: 31 | GE for correct key (224): 1.0)\n",
            "KR: 32 | GE for correct key (224): 1.0)\n",
            "KR: 33 | GE for correct key (224): 1.0)\n",
            "KR: 34 | GE for correct key (224): 1.0)\n",
            "KR: 35 | GE for correct key (224): 1.0)\n",
            "KR: 36 | GE for correct key (224): 1.0)\n",
            "KR: 37 | GE for correct key (224): 1.0)\n",
            "KR: 38 | GE for correct key (224): 1.0)\n",
            "KR: 39 | GE for correct key (224): 1.0)\n",
            "KR: 40 | GE for correct key (224): 1.0)\n",
            "KR: 41 | GE for correct key (224): 1.0)\n",
            "KR: 42 | GE for correct key (224): 1.0)\n",
            "KR: 43 | GE for correct key (224): 1.0)\n",
            "KR: 44 | GE for correct key (224): 1.0)\n",
            "KR: 45 | GE for correct key (224): 1.0)\n",
            "KR: 46 | GE for correct key (224): 1.0)\n",
            "KR: 47 | GE for correct key (224): 1.0)\n",
            "KR: 48 | GE for correct key (224): 1.0)\n",
            "KR: 49 | GE for correct key (224): 1.0)\n",
            "KR: 50 | GE for correct key (224): 1.0)\n",
            "KR: 51 | GE for correct key (224): 1.0)\n",
            "KR: 52 | GE for correct key (224): 1.0)\n",
            "KR: 53 | GE for correct key (224): 1.0)\n",
            "KR: 54 | GE for correct key (224): 1.0)\n",
            "KR: 55 | GE for correct key (224): 1.0)\n",
            "KR: 56 | GE for correct key (224): 1.0)\n",
            "KR: 57 | GE for correct key (224): 1.0)\n",
            "KR: 58 | GE for correct key (224): 1.0)\n",
            "KR: 59 | GE for correct key (224): 1.0)\n",
            "KR: 60 | GE for correct key (224): 1.0)\n",
            "KR: 61 | GE for correct key (224): 1.0)\n",
            "KR: 62 | GE for correct key (224): 1.0)\n",
            "KR: 63 | GE for correct key (224): 1.0)\n",
            "KR: 64 | GE for correct key (224): 1.0)\n",
            "KR: 65 | GE for correct key (224): 1.0)\n",
            "KR: 66 | GE for correct key (224): 1.0)\n",
            "KR: 67 | GE for correct key (224): 1.0)\n",
            "KR: 68 | GE for correct key (224): 1.0)\n",
            "KR: 69 | GE for correct key (224): 1.0)\n",
            "KR: 70 | GE for correct key (224): 1.0)\n",
            "KR: 71 | GE for correct key (224): 1.0)\n",
            "KR: 72 | GE for correct key (224): 1.0)\n",
            "KR: 73 | GE for correct key (224): 1.0)\n",
            "KR: 74 | GE for correct key (224): 1.0)\n",
            "KR: 75 | GE for correct key (224): 1.0)\n",
            "KR: 76 | GE for correct key (224): 1.0)\n",
            "KR: 77 | GE for correct key (224): 1.0)\n",
            "KR: 78 | GE for correct key (224): 1.0)\n",
            "KR: 79 | GE for correct key (224): 1.0)\n",
            "KR: 80 | GE for correct key (224): 1.0)\n",
            "KR: 81 | GE for correct key (224): 1.0)\n",
            "KR: 82 | GE for correct key (224): 1.0)\n",
            "KR: 83 | GE for correct key (224): 1.0)\n",
            "KR: 84 | GE for correct key (224): 1.0)\n",
            "KR: 85 | GE for correct key (224): 1.0)\n",
            "KR: 86 | GE for correct key (224): 1.0)\n",
            "KR: 87 | GE for correct key (224): 1.0)\n",
            "KR: 88 | GE for correct key (224): 1.0)\n",
            "KR: 89 | GE for correct key (224): 1.0)\n",
            "KR: 90 | GE for correct key (224): 1.0)\n",
            "KR: 91 | GE for correct key (224): 1.0)\n",
            "KR: 92 | GE for correct key (224): 1.0)\n",
            "KR: 93 | GE for correct key (224): 1.0)\n",
            "KR: 94 | GE for correct key (224): 1.0)\n",
            "KR: 95 | GE for correct key (224): 1.0)\n",
            "KR: 96 | GE for correct key (224): 1.0)\n",
            "KR: 97 | GE for correct key (224): 1.0)\n",
            "KR: 98 | GE for correct key (224): 1.0)\n",
            "KR: 99 | GE for correct key (224): 1.0)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 138, 16)           240       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2208)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 700)               1546300   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 700)               490700    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 700)               490700    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 700)               490700    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 700)               490700    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 700)               490700    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 9)                 6309      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,006,349\n",
            "Trainable params: 4,006,349\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/200\n",
            "125/125 [==============================] - 32s 243ms/step - loss: 1.8319 - accuracy: 0.2557 - val_loss: 1.7960 - val_accuracy: 0.2696\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.7673 - accuracy: 0.2686 - val_loss: 1.7906 - val_accuracy: 0.2696\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.7639 - accuracy: 0.2717 - val_loss: 1.7877 - val_accuracy: 0.2658\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.7597 - accuracy: 0.2726 - val_loss: 1.7945 - val_accuracy: 0.2696\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 1.7570 - accuracy: 0.2742 - val_loss: 1.7963 - val_accuracy: 0.2436\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 32s 254ms/step - loss: 1.7529 - accuracy: 0.2728 - val_loss: 1.7979 - val_accuracy: 0.2618\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 1.7494 - accuracy: 0.2759 - val_loss: 1.8018 - val_accuracy: 0.2412\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.7460 - accuracy: 0.2790 - val_loss: 1.8067 - val_accuracy: 0.2512\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 30s 240ms/step - loss: 1.7415 - accuracy: 0.2816 - val_loss: 1.8065 - val_accuracy: 0.2354\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.7376 - accuracy: 0.2865 - val_loss: 1.8227 - val_accuracy: 0.2668\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 1.7319 - accuracy: 0.2861 - val_loss: 1.8154 - val_accuracy: 0.2524\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.7244 - accuracy: 0.2904 - val_loss: 1.8526 - val_accuracy: 0.2366\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 1.7218 - accuracy: 0.2905 - val_loss: 1.8375 - val_accuracy: 0.2172\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.7156 - accuracy: 0.2945 - val_loss: 1.8385 - val_accuracy: 0.2446\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 1.7098 - accuracy: 0.2985 - val_loss: 1.8370 - val_accuracy: 0.2282\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 1.7021 - accuracy: 0.2990 - val_loss: 1.8518 - val_accuracy: 0.2416\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 1.6959 - accuracy: 0.3040 - val_loss: 1.8615 - val_accuracy: 0.2368\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6902 - accuracy: 0.3063 - val_loss: 1.8591 - val_accuracy: 0.2360\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6881 - accuracy: 0.3070 - val_loss: 1.8703 - val_accuracy: 0.2438\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 34s 274ms/step - loss: 1.6816 - accuracy: 0.3124 - val_loss: 1.8632 - val_accuracy: 0.2292\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6730 - accuracy: 0.3150 - val_loss: 1.8736 - val_accuracy: 0.2380\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6712 - accuracy: 0.3153 - val_loss: 1.8943 - val_accuracy: 0.2278\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 1.6605 - accuracy: 0.3213 - val_loss: 1.8947 - val_accuracy: 0.2386\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 31s 244ms/step - loss: 1.6539 - accuracy: 0.3239 - val_loss: 1.8869 - val_accuracy: 0.2296\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6486 - accuracy: 0.3247 - val_loss: 1.9249 - val_accuracy: 0.2240\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 1.6406 - accuracy: 0.3295 - val_loss: 1.9302 - val_accuracy: 0.2232\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6321 - accuracy: 0.3340 - val_loss: 1.8985 - val_accuracy: 0.2340\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 1.6228 - accuracy: 0.3340 - val_loss: 1.9757 - val_accuracy: 0.2288\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 1.6086 - accuracy: 0.3417 - val_loss: 1.9669 - val_accuracy: 0.2352\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 1.5905 - accuracy: 0.3491 - val_loss: 1.9748 - val_accuracy: 0.2276\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.5803 - accuracy: 0.3522 - val_loss: 2.0005 - val_accuracy: 0.2276\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 1.5588 - accuracy: 0.3621 - val_loss: 2.0630 - val_accuracy: 0.2198\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 1.5366 - accuracy: 0.3719 - val_loss: 2.0419 - val_accuracy: 0.2082\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 32s 257ms/step - loss: 1.5092 - accuracy: 0.3825 - val_loss: 2.1806 - val_accuracy: 0.2060\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 31s 244ms/step - loss: 1.4855 - accuracy: 0.3911 - val_loss: 2.1810 - val_accuracy: 0.2104\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.4487 - accuracy: 0.4066 - val_loss: 2.2195 - val_accuracy: 0.2146\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 1.4084 - accuracy: 0.4260 - val_loss: 2.2002 - val_accuracy: 0.2184\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 31s 249ms/step - loss: 1.3670 - accuracy: 0.4449 - val_loss: 2.3312 - val_accuracy: 0.2126\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 1.3285 - accuracy: 0.4615 - val_loss: 2.3823 - val_accuracy: 0.2104\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.2827 - accuracy: 0.4831 - val_loss: 2.4697 - val_accuracy: 0.2164\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 1.2243 - accuracy: 0.5112 - val_loss: 2.5145 - val_accuracy: 0.2158\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 31s 249ms/step - loss: 1.1611 - accuracy: 0.5372 - val_loss: 2.7246 - val_accuracy: 0.2068\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 1.1039 - accuracy: 0.5634 - val_loss: 2.8343 - val_accuracy: 0.1934\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 1.0474 - accuracy: 0.5870 - val_loss: 2.8496 - val_accuracy: 0.2064\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.9809 - accuracy: 0.6146 - val_loss: 3.2634 - val_accuracy: 0.1994\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.9034 - accuracy: 0.6473 - val_loss: 3.3716 - val_accuracy: 0.2044\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.8210 - accuracy: 0.6801 - val_loss: 3.5750 - val_accuracy: 0.2024\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 31s 248ms/step - loss: 0.7639 - accuracy: 0.7045 - val_loss: 3.8327 - val_accuracy: 0.2052\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 31s 248ms/step - loss: 0.6834 - accuracy: 0.7372 - val_loss: 4.1379 - val_accuracy: 0.2052\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.6001 - accuracy: 0.7703 - val_loss: 4.5456 - val_accuracy: 0.1992\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.5390 - accuracy: 0.7953 - val_loss: 4.9572 - val_accuracy: 0.1982\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.4607 - accuracy: 0.8257 - val_loss: 5.4208 - val_accuracy: 0.2046\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.4054 - accuracy: 0.8492 - val_loss: 5.8557 - val_accuracy: 0.1966\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.3497 - accuracy: 0.8713 - val_loss: 5.8609 - val_accuracy: 0.2032\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.3114 - accuracy: 0.8869 - val_loss: 6.3268 - val_accuracy: 0.1964\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.2351 - accuracy: 0.9141 - val_loss: 7.3829 - val_accuracy: 0.2074\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.2298 - accuracy: 0.9173 - val_loss: 7.1865 - val_accuracy: 0.2046\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.1941 - accuracy: 0.9313 - val_loss: 7.7422 - val_accuracy: 0.2068\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.1851 - accuracy: 0.9344 - val_loss: 7.8977 - val_accuracy: 0.2030\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.1467 - accuracy: 0.9475 - val_loss: 8.2958 - val_accuracy: 0.2104\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 31s 248ms/step - loss: 0.1407 - accuracy: 0.9515 - val_loss: 8.6095 - val_accuracy: 0.2114\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.1545 - accuracy: 0.9458 - val_loss: 8.4940 - val_accuracy: 0.2096\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.1316 - accuracy: 0.9547 - val_loss: 8.9115 - val_accuracy: 0.2064\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.1389 - accuracy: 0.9536 - val_loss: 8.6394 - val_accuracy: 0.2118\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 32s 255ms/step - loss: 0.0967 - accuracy: 0.9668 - val_loss: 9.0325 - val_accuracy: 0.2034\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.1135 - accuracy: 0.9611 - val_loss: 9.1977 - val_accuracy: 0.2110\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.1178 - accuracy: 0.9593 - val_loss: 9.2879 - val_accuracy: 0.2022\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0992 - accuracy: 0.9662 - val_loss: 9.3328 - val_accuracy: 0.2046\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 31s 249ms/step - loss: 0.0944 - accuracy: 0.9677 - val_loss: 9.4957 - val_accuracy: 0.1992\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.1137 - accuracy: 0.9615 - val_loss: 9.3005 - val_accuracy: 0.2000\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.1036 - accuracy: 0.9634 - val_loss: 9.7256 - val_accuracy: 0.2078\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0825 - accuracy: 0.9721 - val_loss: 9.9449 - val_accuracy: 0.2036\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0880 - accuracy: 0.9704 - val_loss: 9.3869 - val_accuracy: 0.2080\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.1116 - accuracy: 0.9630 - val_loss: 9.0775 - val_accuracy: 0.2108\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0884 - accuracy: 0.9692 - val_loss: 9.5545 - val_accuracy: 0.2136\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0812 - accuracy: 0.9731 - val_loss: 9.7829 - val_accuracy: 0.2126\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0819 - accuracy: 0.9728 - val_loss: 9.6713 - val_accuracy: 0.2106\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0952 - accuracy: 0.9678 - val_loss: 9.6168 - val_accuracy: 0.2020\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 31s 250ms/step - loss: 0.0872 - accuracy: 0.9701 - val_loss: 9.5239 - val_accuracy: 0.2074\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 32s 252ms/step - loss: 0.0755 - accuracy: 0.9736 - val_loss: 9.8473 - val_accuracy: 0.2072\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0902 - accuracy: 0.9703 - val_loss: 9.8239 - val_accuracy: 0.2092\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0750 - accuracy: 0.9751 - val_loss: 10.1819 - val_accuracy: 0.1994\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0762 - accuracy: 0.9742 - val_loss: 9.8542 - val_accuracy: 0.2144\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0780 - accuracy: 0.9748 - val_loss: 9.8884 - val_accuracy: 0.2054\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0903 - accuracy: 0.9698 - val_loss: 9.2635 - val_accuracy: 0.2078\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0673 - accuracy: 0.9773 - val_loss: 9.7435 - val_accuracy: 0.2090\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0847 - accuracy: 0.9724 - val_loss: 9.6567 - val_accuracy: 0.2072\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0782 - accuracy: 0.9736 - val_loss: 9.6289 - val_accuracy: 0.2094\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0646 - accuracy: 0.9779 - val_loss: 10.0799 - val_accuracy: 0.2074\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.1004 - accuracy: 0.9690 - val_loss: 9.1084 - val_accuracy: 0.2124\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0563 - accuracy: 0.9810 - val_loss: 9.8645 - val_accuracy: 0.2122\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0676 - accuracy: 0.9772 - val_loss: 9.7223 - val_accuracy: 0.2098\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0902 - accuracy: 0.9704 - val_loss: 9.3479 - val_accuracy: 0.2084\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0713 - accuracy: 0.9763 - val_loss: 9.7488 - val_accuracy: 0.2170\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0612 - accuracy: 0.9798 - val_loss: 10.1507 - val_accuracy: 0.2134\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0878 - accuracy: 0.9710 - val_loss: 9.9008 - val_accuracy: 0.2126\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 32s 257ms/step - loss: 0.0709 - accuracy: 0.9774 - val_loss: 9.7559 - val_accuracy: 0.1998\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0717 - accuracy: 0.9764 - val_loss: 9.5535 - val_accuracy: 0.2098\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 0.0527 - accuracy: 0.9824 - val_loss: 10.3614 - val_accuracy: 0.2074\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0610 - accuracy: 0.9805 - val_loss: 9.9889 - val_accuracy: 0.2088\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0803 - accuracy: 0.9733 - val_loss: 9.5544 - val_accuracy: 0.2140\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0679 - accuracy: 0.9776 - val_loss: 9.6052 - val_accuracy: 0.2102\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0594 - accuracy: 0.9809 - val_loss: 10.0103 - val_accuracy: 0.2092\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0717 - accuracy: 0.9767 - val_loss: 9.6077 - val_accuracy: 0.2130\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0614 - accuracy: 0.9796 - val_loss: 9.7283 - val_accuracy: 0.2106\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0688 - accuracy: 0.9780 - val_loss: 9.8561 - val_accuracy: 0.2074\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0584 - accuracy: 0.9809 - val_loss: 9.9686 - val_accuracy: 0.2070\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0729 - accuracy: 0.9757 - val_loss: 9.9371 - val_accuracy: 0.2136\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0649 - accuracy: 0.9794 - val_loss: 9.9322 - val_accuracy: 0.2172\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0549 - accuracy: 0.9827 - val_loss: 10.0596 - val_accuracy: 0.2162\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0650 - accuracy: 0.9796 - val_loss: 9.9199 - val_accuracy: 0.2082\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0765 - accuracy: 0.9750 - val_loss: 10.0136 - val_accuracy: 0.2114\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 32s 255ms/step - loss: 0.0657 - accuracy: 0.9789 - val_loss: 9.8002 - val_accuracy: 0.2052\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0617 - accuracy: 0.9797 - val_loss: 9.8229 - val_accuracy: 0.1980\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0553 - accuracy: 0.9815 - val_loss: 9.9074 - val_accuracy: 0.2102\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0565 - accuracy: 0.9816 - val_loss: 9.8759 - val_accuracy: 0.2084\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 9.5980 - val_accuracy: 0.2020\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0610 - accuracy: 0.9804 - val_loss: 9.8086 - val_accuracy: 0.2082\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0598 - accuracy: 0.9813 - val_loss: 9.8459 - val_accuracy: 0.2076\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0552 - accuracy: 0.9825 - val_loss: 9.9988 - val_accuracy: 0.2134\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0733 - accuracy: 0.9768 - val_loss: 9.3015 - val_accuracy: 0.2122\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0633 - accuracy: 0.9793 - val_loss: 9.5347 - val_accuracy: 0.2074\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0638 - accuracy: 0.9795 - val_loss: 9.8899 - val_accuracy: 0.2064\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0546 - accuracy: 0.9823 - val_loss: 9.8841 - val_accuracy: 0.2116\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0592 - accuracy: 0.9808 - val_loss: 9.7858 - val_accuracy: 0.2116\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0554 - accuracy: 0.9827 - val_loss: 9.9968 - val_accuracy: 0.2058\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 0.0530 - accuracy: 0.9831 - val_loss: 9.7308 - val_accuracy: 0.2052\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0465 - accuracy: 0.9848 - val_loss: 9.9639 - val_accuracy: 0.2058\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 32s 255ms/step - loss: 0.0617 - accuracy: 0.9808 - val_loss: 9.4353 - val_accuracy: 0.2026\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 0.0637 - accuracy: 0.9796 - val_loss: 9.5182 - val_accuracy: 0.2092\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0640 - accuracy: 0.9801 - val_loss: 9.6245 - val_accuracy: 0.2000\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0508 - accuracy: 0.9836 - val_loss: 10.0800 - val_accuracy: 0.2108\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0579 - accuracy: 0.9818 - val_loss: 9.8016 - val_accuracy: 0.2088\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0608 - accuracy: 0.9802 - val_loss: 9.5856 - val_accuracy: 0.2056\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0513 - accuracy: 0.9836 - val_loss: 9.6252 - val_accuracy: 0.2024\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0527 - accuracy: 0.9827 - val_loss: 10.0817 - val_accuracy: 0.2086\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0625 - accuracy: 0.9801 - val_loss: 9.7038 - val_accuracy: 0.2066\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0430 - accuracy: 0.9867 - val_loss: 9.9543 - val_accuracy: 0.2078\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 10.2077 - val_accuracy: 0.2112\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0573 - accuracy: 0.9818 - val_loss: 9.6149 - val_accuracy: 0.1974\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0442 - accuracy: 0.9857 - val_loss: 9.9936 - val_accuracy: 0.2028\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 10.4010 - val_accuracy: 0.2068\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0665 - accuracy: 0.9798 - val_loss: 9.7257 - val_accuracy: 0.1992\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0535 - accuracy: 0.9831 - val_loss: 9.5690 - val_accuracy: 0.2022\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0393 - accuracy: 0.9878 - val_loss: 10.3049 - val_accuracy: 0.2066\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0505 - accuracy: 0.9845 - val_loss: 10.2050 - val_accuracy: 0.2096\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0686 - accuracy: 0.9783 - val_loss: 9.2967 - val_accuracy: 0.2090\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 31s 249ms/step - loss: 0.0695 - accuracy: 0.9786 - val_loss: 9.5613 - val_accuracy: 0.2052\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0510 - accuracy: 0.9842 - val_loss: 9.4829 - val_accuracy: 0.2142\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 10.2341 - val_accuracy: 0.2186\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0563 - accuracy: 0.9827 - val_loss: 9.1530 - val_accuracy: 0.2116\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0461 - accuracy: 0.9849 - val_loss: 9.8609 - val_accuracy: 0.2052\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0396 - accuracy: 0.9874 - val_loss: 9.9520 - val_accuracy: 0.2038\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0495 - accuracy: 0.9842 - val_loss: 9.5988 - val_accuracy: 0.2002\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0582 - accuracy: 0.9825 - val_loss: 9.6855 - val_accuracy: 0.2058\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 31s 245ms/step - loss: 0.0557 - accuracy: 0.9827 - val_loss: 9.4984 - val_accuracy: 0.1942\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0455 - accuracy: 0.9865 - val_loss: 9.7820 - val_accuracy: 0.2136\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 31s 244ms/step - loss: 0.0636 - accuracy: 0.9806 - val_loss: 9.1790 - val_accuracy: 0.2070\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0614 - accuracy: 0.9814 - val_loss: 9.1093 - val_accuracy: 0.2072\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0436 - accuracy: 0.9868 - val_loss: 9.6735 - val_accuracy: 0.2026\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0453 - accuracy: 0.9855 - val_loss: 9.7437 - val_accuracy: 0.2076\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0412 - accuracy: 0.9866 - val_loss: 10.4113 - val_accuracy: 0.2182\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 32s 257ms/step - loss: 0.0482 - accuracy: 0.9854 - val_loss: 9.6653 - val_accuracy: 0.2100\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 32s 253ms/step - loss: 0.0526 - accuracy: 0.9838 - val_loss: 9.2510 - val_accuracy: 0.2020\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0459 - accuracy: 0.9860 - val_loss: 9.6997 - val_accuracy: 0.2114\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0581 - accuracy: 0.9825 - val_loss: 9.2689 - val_accuracy: 0.1966\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0418 - accuracy: 0.9869 - val_loss: 10.0073 - val_accuracy: 0.2096\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0406 - accuracy: 0.9877 - val_loss: 10.1426 - val_accuracy: 0.2176\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0571 - accuracy: 0.9819 - val_loss: 9.4234 - val_accuracy: 0.2028\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0494 - accuracy: 0.9853 - val_loss: 9.7324 - val_accuracy: 0.2022\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 10.2432 - val_accuracy: 0.2074\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0386 - accuracy: 0.9880 - val_loss: 10.2917 - val_accuracy: 0.2106\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0476 - accuracy: 0.9858 - val_loss: 9.9497 - val_accuracy: 0.2064\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0524 - accuracy: 0.9835 - val_loss: 9.5389 - val_accuracy: 0.2038\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 31s 246ms/step - loss: 0.0303 - accuracy: 0.9902 - val_loss: 10.3531 - val_accuracy: 0.2104\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 32s 258ms/step - loss: 0.0367 - accuracy: 0.9890 - val_loss: 9.7062 - val_accuracy: 0.2120\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 32s 253ms/step - loss: 0.0502 - accuracy: 0.9848 - val_loss: 9.5367 - val_accuracy: 0.2100\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.0453 - accuracy: 0.9861 - val_loss: 9.6911 - val_accuracy: 0.2078\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 30s 244ms/step - loss: 0.0411 - accuracy: 0.9873 - val_loss: 9.5697 - val_accuracy: 0.2114\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0434 - accuracy: 0.9874 - val_loss: 9.5702 - val_accuracy: 0.2126\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0583 - accuracy: 0.9823 - val_loss: 9.5282 - val_accuracy: 0.2090\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0491 - accuracy: 0.9855 - val_loss: 9.4647 - val_accuracy: 0.2108\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 9.2630 - val_accuracy: 0.2078\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 30s 239ms/step - loss: 0.0577 - accuracy: 0.9835 - val_loss: 8.8862 - val_accuracy: 0.2106\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0532 - accuracy: 0.9838 - val_loss: 9.1954 - val_accuracy: 0.2080\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 30s 237ms/step - loss: 0.0429 - accuracy: 0.9865 - val_loss: 9.3998 - val_accuracy: 0.2130\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 30s 239ms/step - loss: 0.0476 - accuracy: 0.9860 - val_loss: 9.4649 - val_accuracy: 0.2038\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0417 - accuracy: 0.9873 - val_loss: 9.5815 - val_accuracy: 0.2018\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0384 - accuracy: 0.9885 - val_loss: 9.7762 - val_accuracy: 0.2086\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 10.2129 - val_accuracy: 0.2116\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 30s 243ms/step - loss: 0.0403 - accuracy: 0.9876 - val_loss: 9.8022 - val_accuracy: 0.2096\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 31s 251ms/step - loss: 0.0429 - accuracy: 0.9872 - val_loss: 9.9822 - val_accuracy: 0.2168\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 30s 241ms/step - loss: 0.0450 - accuracy: 0.9865 - val_loss: 9.6340 - val_accuracy: 0.2190\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0615 - accuracy: 0.9819 - val_loss: 8.8869 - val_accuracy: 0.2088\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 30s 239ms/step - loss: 0.0400 - accuracy: 0.9876 - val_loss: 9.5078 - val_accuracy: 0.2100\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 30s 239ms/step - loss: 0.0289 - accuracy: 0.9912 - val_loss: 10.3793 - val_accuracy: 0.2118\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 30s 242ms/step - loss: 0.0287 - accuracy: 0.9915 - val_loss: 10.2293 - val_accuracy: 0.2040\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0400 - accuracy: 0.9879 - val_loss: 9.9131 - val_accuracy: 0.2076\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 30s 239ms/step - loss: 0.0440 - accuracy: 0.9876 - val_loss: 9.4583 - val_accuracy: 0.2114\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 0.0456 - accuracy: 0.9861 - val_loss: 9.3216 - val_accuracy: 0.2068\n",
            "157/157 [==============================] - 2s 12ms/step\n",
            "KR: 0 | GE for correct key (224): 235.0)\n",
            "KR: 1 | GE for correct key (224): 235.0)\n",
            "KR: 2 | GE for correct key (224): 235.0)\n",
            "KR: 3 | GE for correct key (224): 235.0)\n",
            "KR: 4 | GE for correct key (224): 235.0)\n",
            "KR: 5 | GE for correct key (224): 235.0)\n",
            "KR: 6 | GE for correct key (224): 235.0)\n",
            "KR: 7 | GE for correct key (224): 235.0)\n",
            "KR: 8 | GE for correct key (224): 235.0)\n",
            "KR: 9 | GE for correct key (224): 235.0)\n",
            "KR: 10 | GE for correct key (224): 235.0)\n",
            "KR: 11 | GE for correct key (224): 235.0)\n",
            "KR: 12 | GE for correct key (224): 235.0)\n",
            "KR: 13 | GE for correct key (224): 235.0)\n",
            "KR: 14 | GE for correct key (224): 235.0)\n",
            "KR: 15 | GE for correct key (224): 235.0)\n",
            "KR: 16 | GE for correct key (224): 235.0)\n",
            "KR: 17 | GE for correct key (224): 235.0)\n",
            "KR: 18 | GE for correct key (224): 235.0)\n",
            "KR: 19 | GE for correct key (224): 235.0)\n",
            "KR: 20 | GE for correct key (224): 235.0)\n",
            "KR: 21 | GE for correct key (224): 235.0)\n",
            "KR: 22 | GE for correct key (224): 235.0)\n",
            "KR: 23 | GE for correct key (224): 235.0)\n",
            "KR: 24 | GE for correct key (224): 235.0)\n",
            "KR: 25 | GE for correct key (224): 235.0)\n",
            "KR: 26 | GE for correct key (224): 235.0)\n",
            "KR: 27 | GE for correct key (224): 235.0)\n",
            "KR: 28 | GE for correct key (224): 235.0)\n",
            "KR: 29 | GE for correct key (224): 235.0)\n",
            "KR: 30 | GE for correct key (224): 235.0)\n",
            "KR: 31 | GE for correct key (224): 235.0)\n",
            "KR: 32 | GE for correct key (224): 235.0)\n",
            "KR: 33 | GE for correct key (224): 235.0)\n",
            "KR: 34 | GE for correct key (224): 235.0)\n",
            "KR: 35 | GE for correct key (224): 235.0)\n",
            "KR: 36 | GE for correct key (224): 235.0)\n",
            "KR: 37 | GE for correct key (224): 235.0)\n",
            "KR: 38 | GE for correct key (224): 235.0)\n",
            "KR: 39 | GE for correct key (224): 235.0)\n",
            "KR: 40 | GE for correct key (224): 235.0)\n",
            "KR: 41 | GE for correct key (224): 235.0)\n",
            "KR: 42 | GE for correct key (224): 235.0)\n",
            "KR: 43 | GE for correct key (224): 235.0)\n",
            "KR: 44 | GE for correct key (224): 235.0)\n",
            "KR: 45 | GE for correct key (224): 235.0)\n",
            "KR: 46 | GE for correct key (224): 235.0)\n",
            "KR: 47 | GE for correct key (224): 235.0)\n",
            "KR: 48 | GE for correct key (224): 235.0)\n",
            "KR: 49 | GE for correct key (224): 235.0)\n",
            "KR: 50 | GE for correct key (224): 235.0)\n",
            "KR: 51 | GE for correct key (224): 235.0)\n",
            "KR: 52 | GE for correct key (224): 235.0)\n",
            "KR: 53 | GE for correct key (224): 235.0)\n",
            "KR: 54 | GE for correct key (224): 235.0)\n",
            "KR: 55 | GE for correct key (224): 235.0)\n",
            "KR: 56 | GE for correct key (224): 235.0)\n",
            "KR: 57 | GE for correct key (224): 235.0)\n",
            "KR: 58 | GE for correct key (224): 235.0)\n",
            "KR: 59 | GE for correct key (224): 235.0)\n",
            "KR: 60 | GE for correct key (224): 235.0)\n",
            "KR: 61 | GE for correct key (224): 235.0)\n",
            "KR: 62 | GE for correct key (224): 235.0)\n",
            "KR: 63 | GE for correct key (224): 235.0)\n",
            "KR: 64 | GE for correct key (224): 235.0)\n",
            "KR: 65 | GE for correct key (224): 235.0)\n",
            "KR: 66 | GE for correct key (224): 235.0)\n",
            "KR: 67 | GE for correct key (224): 235.0)\n",
            "KR: 68 | GE for correct key (224): 235.0)\n",
            "KR: 69 | GE for correct key (224): 235.0)\n",
            "KR: 70 | GE for correct key (224): 235.0)\n",
            "KR: 71 | GE for correct key (224): 235.0)\n",
            "KR: 72 | GE for correct key (224): 235.0)\n",
            "KR: 73 | GE for correct key (224): 235.0)\n",
            "KR: 74 | GE for correct key (224): 235.0)\n",
            "KR: 75 | GE for correct key (224): 235.0)\n",
            "KR: 76 | GE for correct key (224): 235.0)\n",
            "KR: 77 | GE for correct key (224): 235.0)\n",
            "KR: 78 | GE for correct key (224): 235.0)\n",
            "KR: 79 | GE for correct key (224): 235.0)\n",
            "KR: 80 | GE for correct key (224): 235.0)\n",
            "KR: 81 | GE for correct key (224): 235.0)\n",
            "KR: 82 | GE for correct key (224): 235.0)\n",
            "KR: 83 | GE for correct key (224): 235.0)\n",
            "KR: 84 | GE for correct key (224): 235.0)\n",
            "KR: 85 | GE for correct key (224): 235.0)\n",
            "KR: 86 | GE for correct key (224): 235.0)\n",
            "KR: 87 | GE for correct key (224): 235.0)\n",
            "KR: 88 | GE for correct key (224): 235.0)\n",
            "KR: 89 | GE for correct key (224): 235.0)\n",
            "KR: 90 | GE for correct key (224): 235.0)\n",
            "KR: 91 | GE for correct key (224): 235.0)\n",
            "KR: 92 | GE for correct key (224): 235.0)\n",
            "KR: 93 | GE for correct key (224): 235.0)\n",
            "KR: 94 | GE for correct key (224): 235.0)\n",
            "KR: 95 | GE for correct key (224): 235.0)\n",
            "KR: 96 | GE for correct key (224): 235.0)\n",
            "KR: 97 | GE for correct key (224): 235.0)\n",
            "KR: 98 | GE for correct key (224): 235.0)\n",
            "KR: 99 | GE for correct key (224): 235.0)\n",
            "157/157 [==============================] - 2s 10ms/step\n",
            "KR: 0 | GE for correct key (224): 135.0)\n",
            "KR: 1 | GE for correct key (224): 135.0)\n",
            "KR: 2 | GE for correct key (224): 135.0)\n",
            "KR: 3 | GE for correct key (224): 135.0)\n",
            "KR: 4 | GE for correct key (224): 135.0)\n",
            "KR: 5 | GE for correct key (224): 135.0)\n",
            "KR: 6 | GE for correct key (224): 135.0)\n",
            "KR: 7 | GE for correct key (224): 135.0)\n",
            "KR: 8 | GE for correct key (224): 135.0)\n",
            "KR: 9 | GE for correct key (224): 135.0)\n",
            "KR: 10 | GE for correct key (224): 135.0)\n",
            "KR: 11 | GE for correct key (224): 135.0)\n",
            "KR: 12 | GE for correct key (224): 135.0)\n",
            "KR: 13 | GE for correct key (224): 135.0)\n",
            "KR: 14 | GE for correct key (224): 135.0)\n",
            "KR: 15 | GE for correct key (224): 135.0)\n",
            "KR: 16 | GE for correct key (224): 135.0)\n",
            "KR: 17 | GE for correct key (224): 135.0)\n",
            "KR: 18 | GE for correct key (224): 135.0)\n",
            "KR: 19 | GE for correct key (224): 135.0)\n",
            "KR: 20 | GE for correct key (224): 135.0)\n",
            "KR: 21 | GE for correct key (224): 135.0)\n",
            "KR: 22 | GE for correct key (224): 135.0)\n",
            "KR: 23 | GE for correct key (224): 135.0)\n",
            "KR: 24 | GE for correct key (224): 135.0)\n",
            "KR: 25 | GE for correct key (224): 135.0)\n",
            "KR: 26 | GE for correct key (224): 135.0)\n",
            "KR: 27 | GE for correct key (224): 135.0)\n",
            "KR: 28 | GE for correct key (224): 135.0)\n",
            "KR: 29 | GE for correct key (224): 135.0)\n",
            "KR: 30 | GE for correct key (224): 135.0)\n",
            "KR: 31 | GE for correct key (224): 135.0)\n",
            "KR: 32 | GE for correct key (224): 135.0)\n",
            "KR: 33 | GE for correct key (224): 135.0)\n",
            "KR: 34 | GE for correct key (224): 135.0)\n",
            "KR: 35 | GE for correct key (224): 135.0)\n",
            "KR: 36 | GE for correct key (224): 135.0)\n",
            "KR: 37 | GE for correct key (224): 135.0)\n",
            "KR: 38 | GE for correct key (224): 135.0)\n",
            "KR: 39 | GE for correct key (224): 135.0)\n",
            "KR: 40 | GE for correct key (224): 135.0)\n",
            "KR: 41 | GE for correct key (224): 135.0)\n",
            "KR: 42 | GE for correct key (224): 135.0)\n",
            "KR: 43 | GE for correct key (224): 135.0)\n",
            "KR: 44 | GE for correct key (224): 135.0)\n",
            "KR: 45 | GE for correct key (224): 135.0)\n",
            "KR: 46 | GE for correct key (224): 135.0)\n",
            "KR: 47 | GE for correct key (224): 135.0)\n",
            "KR: 48 | GE for correct key (224): 135.0)\n",
            "KR: 49 | GE for correct key (224): 135.0)\n",
            "KR: 50 | GE for correct key (224): 135.0)\n",
            "KR: 51 | GE for correct key (224): 135.0)\n",
            "KR: 52 | GE for correct key (224): 135.0)\n",
            "KR: 53 | GE for correct key (224): 135.0)\n",
            "KR: 54 | GE for correct key (224): 135.0)\n",
            "KR: 55 | GE for correct key (224): 135.0)\n",
            "KR: 56 | GE for correct key (224): 135.0)\n",
            "KR: 57 | GE for correct key (224): 135.0)\n",
            "KR: 58 | GE for correct key (224): 135.0)\n",
            "KR: 59 | GE for correct key (224): 135.0)\n",
            "KR: 60 | GE for correct key (224): 135.0)\n",
            "KR: 61 | GE for correct key (224): 135.0)\n",
            "KR: 62 | GE for correct key (224): 135.0)\n",
            "KR: 63 | GE for correct key (224): 135.0)\n",
            "KR: 64 | GE for correct key (224): 135.0)\n",
            "KR: 65 | GE for correct key (224): 135.0)\n",
            "KR: 66 | GE for correct key (224): 135.0)\n",
            "KR: 67 | GE for correct key (224): 135.0)\n",
            "KR: 68 | GE for correct key (224): 135.0)\n",
            "KR: 69 | GE for correct key (224): 135.0)\n",
            "KR: 70 | GE for correct key (224): 135.0)\n",
            "KR: 71 | GE for correct key (224): 135.0)\n",
            "KR: 72 | GE for correct key (224): 135.0)\n",
            "KR: 73 | GE for correct key (224): 135.0)\n",
            "KR: 74 | GE for correct key (224): 135.0)\n",
            "KR: 75 | GE for correct key (224): 135.0)\n",
            "KR: 76 | GE for correct key (224): 135.0)\n",
            "KR: 77 | GE for correct key (224): 135.0)\n",
            "KR: 78 | GE for correct key (224): 135.0)\n",
            "KR: 79 | GE for correct key (224): 135.0)\n",
            "KR: 80 | GE for correct key (224): 135.0)\n",
            "KR: 81 | GE for correct key (224): 135.0)\n",
            "KR: 82 | GE for correct key (224): 135.0)\n",
            "KR: 83 | GE for correct key (224): 135.0)\n",
            "KR: 84 | GE for correct key (224): 135.0)\n",
            "KR: 85 | GE for correct key (224): 135.0)\n",
            "KR: 86 | GE for correct key (224): 135.0)\n",
            "KR: 87 | GE for correct key (224): 135.0)\n",
            "KR: 88 | GE for correct key (224): 135.0)\n",
            "KR: 89 | GE for correct key (224): 135.0)\n",
            "KR: 90 | GE for correct key (224): 135.0)\n",
            "KR: 91 | GE for correct key (224): 135.0)\n",
            "KR: 92 | GE for correct key (224): 135.0)\n",
            "KR: 93 | GE for correct key (224): 135.0)\n",
            "KR: 94 | GE for correct key (224): 135.0)\n",
            "KR: 95 | GE for correct key (224): 135.0)\n",
            "KR: 96 | GE for correct key (224): 135.0)\n",
            "KR: 97 | GE for correct key (224): 135.0)\n",
            "KR: 98 | GE for correct key (224): 135.0)\n",
            "KR: 99 | GE for correct key (224): 135.0)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 138, 20)           260       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2760)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               1380500   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 500)               250500    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 500)               250500    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 500)               250500    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 9)                 4509      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,136,769\n",
            "Trainable params: 2,136,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/200\n",
            "125/125 [==============================] - 19s 147ms/step - loss: 1.7787 - accuracy: 0.2650 - val_loss: 1.7865 - val_accuracy: 0.2696\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 17s 139ms/step - loss: 1.7624 - accuracy: 0.2732 - val_loss: 1.7863 - val_accuracy: 0.2694\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 1.7467 - accuracy: 0.2738 - val_loss: 1.7569 - val_accuracy: 0.2762\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.7346 - accuracy: 0.2756 - val_loss: 1.7547 - val_accuracy: 0.2736\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.7253 - accuracy: 0.2761 - val_loss: 1.7545 - val_accuracy: 0.2770\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.7156 - accuracy: 0.2789 - val_loss: 1.7477 - val_accuracy: 0.2760\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 1.7046 - accuracy: 0.2786 - val_loss: 1.7591 - val_accuracy: 0.2730\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 17s 138ms/step - loss: 1.6866 - accuracy: 0.2824 - val_loss: 1.7698 - val_accuracy: 0.2612\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 17s 137ms/step - loss: 1.6591 - accuracy: 0.2894 - val_loss: 1.8085 - val_accuracy: 0.2562\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 1.6116 - accuracy: 0.3025 - val_loss: 1.8439 - val_accuracy: 0.2642\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.5424 - accuracy: 0.3288 - val_loss: 1.9846 - val_accuracy: 0.2406\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.4540 - accuracy: 0.3643 - val_loss: 2.1485 - val_accuracy: 0.2474\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 18s 147ms/step - loss: 1.3485 - accuracy: 0.4085 - val_loss: 2.3484 - val_accuracy: 0.2336\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 17s 137ms/step - loss: 1.2433 - accuracy: 0.4544 - val_loss: 2.6808 - val_accuracy: 0.2348\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.1370 - accuracy: 0.5043 - val_loss: 2.9236 - val_accuracy: 0.2290\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 1.0335 - accuracy: 0.5506 - val_loss: 3.3136 - val_accuracy: 0.2340\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.9401 - accuracy: 0.5963 - val_loss: 3.7575 - val_accuracy: 0.2284\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.8528 - accuracy: 0.6328 - val_loss: 4.1863 - val_accuracy: 0.2254\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.7774 - accuracy: 0.6695 - val_loss: 4.7106 - val_accuracy: 0.2276\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 18s 140ms/step - loss: 0.7118 - accuracy: 0.6991 - val_loss: 5.1274 - val_accuracy: 0.2368\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.6396 - accuracy: 0.7311 - val_loss: 5.7020 - val_accuracy: 0.2368\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 17s 133ms/step - loss: 0.5867 - accuracy: 0.7567 - val_loss: 6.2645 - val_accuracy: 0.2226\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.5457 - accuracy: 0.7739 - val_loss: 6.6185 - val_accuracy: 0.2240\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.4891 - accuracy: 0.7991 - val_loss: 7.1252 - val_accuracy: 0.2286\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.4479 - accuracy: 0.8175 - val_loss: 7.8693 - val_accuracy: 0.2160\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 17s 138ms/step - loss: 0.4214 - accuracy: 0.8313 - val_loss: 7.8766 - val_accuracy: 0.2186\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.3915 - accuracy: 0.8449 - val_loss: 8.3680 - val_accuracy: 0.2252\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.3602 - accuracy: 0.8582 - val_loss: 8.7112 - val_accuracy: 0.2300\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.3267 - accuracy: 0.8726 - val_loss: 9.3903 - val_accuracy: 0.2324\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.3038 - accuracy: 0.8820 - val_loss: 10.0173 - val_accuracy: 0.2254\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 0.2849 - accuracy: 0.8885 - val_loss: 10.2535 - val_accuracy: 0.2294\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 17s 137ms/step - loss: 0.2699 - accuracy: 0.8967 - val_loss: 10.0723 - val_accuracy: 0.2306\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.2328 - accuracy: 0.9118 - val_loss: 10.6442 - val_accuracy: 0.2230\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.2339 - accuracy: 0.9126 - val_loss: 10.2632 - val_accuracy: 0.2316\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.2059 - accuracy: 0.9265 - val_loss: 10.9731 - val_accuracy: 0.2186\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.1895 - accuracy: 0.9307 - val_loss: 11.7522 - val_accuracy: 0.2248\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 0.1819 - accuracy: 0.9342 - val_loss: 11.2933 - val_accuracy: 0.2210\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.1690 - accuracy: 0.9400 - val_loss: 11.5180 - val_accuracy: 0.2236\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.1585 - accuracy: 0.9437 - val_loss: 11.8239 - val_accuracy: 0.2246\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.1567 - accuracy: 0.9446 - val_loss: 11.8016 - val_accuracy: 0.2324\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 0.1402 - accuracy: 0.9502 - val_loss: 12.2066 - val_accuracy: 0.2230\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.1379 - accuracy: 0.9507 - val_loss: 12.7226 - val_accuracy: 0.2252\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.1266 - accuracy: 0.9553 - val_loss: 12.4728 - val_accuracy: 0.2222\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.1205 - accuracy: 0.9581 - val_loss: 12.7082 - val_accuracy: 0.2188\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.1083 - accuracy: 0.9624 - val_loss: 12.8222 - val_accuracy: 0.2160\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0985 - accuracy: 0.9659 - val_loss: 13.3271 - val_accuracy: 0.2190\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 18s 140ms/step - loss: 0.1094 - accuracy: 0.9632 - val_loss: 12.7315 - val_accuracy: 0.2136\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0916 - accuracy: 0.9691 - val_loss: 13.5608 - val_accuracy: 0.2190\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0961 - accuracy: 0.9672 - val_loss: 13.0541 - val_accuracy: 0.2208\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0852 - accuracy: 0.9716 - val_loss: 13.3983 - val_accuracy: 0.2194\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0854 - accuracy: 0.9715 - val_loss: 13.5434 - val_accuracy: 0.2134\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0815 - accuracy: 0.9725 - val_loss: 13.0778 - val_accuracy: 0.2168\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 17s 138ms/step - loss: 0.0796 - accuracy: 0.9732 - val_loss: 13.5938 - val_accuracy: 0.2152\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0924 - accuracy: 0.9699 - val_loss: 12.9605 - val_accuracy: 0.2220\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0816 - accuracy: 0.9728 - val_loss: 13.0206 - val_accuracy: 0.2276\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0592 - accuracy: 0.9814 - val_loss: 13.8230 - val_accuracy: 0.2196\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0634 - accuracy: 0.9788 - val_loss: 13.7050 - val_accuracy: 0.2248\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0957 - accuracy: 0.9675 - val_loss: 12.5283 - val_accuracy: 0.2174\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0790 - accuracy: 0.9736 - val_loss: 12.8847 - val_accuracy: 0.2262\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0570 - accuracy: 0.9816 - val_loss: 13.2488 - val_accuracy: 0.2212\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0624 - accuracy: 0.9785 - val_loss: 13.2432 - val_accuracy: 0.2120\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0613 - accuracy: 0.9803 - val_loss: 13.8104 - val_accuracy: 0.2170\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0583 - accuracy: 0.9807 - val_loss: 13.2501 - val_accuracy: 0.2212\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 17s 140ms/step - loss: 0.0829 - accuracy: 0.9722 - val_loss: 11.9622 - val_accuracy: 0.2194\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0672 - accuracy: 0.9778 - val_loss: 12.4842 - val_accuracy: 0.2128\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0551 - accuracy: 0.9821 - val_loss: 13.2000 - val_accuracy: 0.2166\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0606 - accuracy: 0.9803 - val_loss: 13.0944 - val_accuracy: 0.2190\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0546 - accuracy: 0.9821 - val_loss: 13.1017 - val_accuracy: 0.2208\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0508 - accuracy: 0.9841 - val_loss: 12.9897 - val_accuracy: 0.2218\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 18s 141ms/step - loss: 0.0611 - accuracy: 0.9796 - val_loss: 12.9592 - val_accuracy: 0.2198\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 18s 144ms/step - loss: 0.0659 - accuracy: 0.9781 - val_loss: 12.2117 - val_accuracy: 0.2210\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0618 - accuracy: 0.9799 - val_loss: 12.3990 - val_accuracy: 0.2194\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 17s 138ms/step - loss: 0.0468 - accuracy: 0.9849 - val_loss: 13.4636 - val_accuracy: 0.2200\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0574 - accuracy: 0.9816 - val_loss: 12.3698 - val_accuracy: 0.2210\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0486 - accuracy: 0.9843 - val_loss: 12.7310 - val_accuracy: 0.2154\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 18s 141ms/step - loss: 0.0500 - accuracy: 0.9844 - val_loss: 12.5806 - val_accuracy: 0.2208\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0673 - accuracy: 0.9780 - val_loss: 11.8296 - val_accuracy: 0.2212\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0578 - accuracy: 0.9810 - val_loss: 12.1475 - val_accuracy: 0.2166\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0468 - accuracy: 0.9851 - val_loss: 12.2949 - val_accuracy: 0.2170\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0482 - accuracy: 0.9847 - val_loss: 11.9305 - val_accuracy: 0.2258\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0481 - accuracy: 0.9842 - val_loss: 12.2081 - val_accuracy: 0.2076\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0482 - accuracy: 0.9845 - val_loss: 12.1464 - val_accuracy: 0.2158\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0514 - accuracy: 0.9820 - val_loss: 11.9970 - val_accuracy: 0.2166\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0470 - accuracy: 0.9852 - val_loss: 12.3712 - val_accuracy: 0.2206\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0527 - accuracy: 0.9827 - val_loss: 11.9995 - val_accuracy: 0.2196\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 11.3505 - val_accuracy: 0.2158\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0455 - accuracy: 0.9855 - val_loss: 11.9840 - val_accuracy: 0.2186\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0500 - accuracy: 0.9836 - val_loss: 11.7429 - val_accuracy: 0.2198\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 17s 139ms/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 12.2315 - val_accuracy: 0.2210\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0410 - accuracy: 0.9867 - val_loss: 12.2245 - val_accuracy: 0.2154\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0487 - accuracy: 0.9841 - val_loss: 12.1317 - val_accuracy: 0.2202\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0443 - accuracy: 0.9853 - val_loss: 11.8808 - val_accuracy: 0.2192\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 12.5469 - val_accuracy: 0.2250\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 12.1607 - val_accuracy: 0.2090\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0425 - accuracy: 0.9863 - val_loss: 11.6983 - val_accuracy: 0.2270\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0488 - accuracy: 0.9845 - val_loss: 11.1934 - val_accuracy: 0.2198\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0440 - accuracy: 0.9859 - val_loss: 11.6090 - val_accuracy: 0.2208\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0429 - accuracy: 0.9863 - val_loss: 11.5925 - val_accuracy: 0.2196\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0415 - accuracy: 0.9867 - val_loss: 11.8259 - val_accuracy: 0.2224\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 18s 144ms/step - loss: 0.0374 - accuracy: 0.9874 - val_loss: 11.5808 - val_accuracy: 0.2270\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0426 - accuracy: 0.9857 - val_loss: 11.4825 - val_accuracy: 0.2192\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0383 - accuracy: 0.9871 - val_loss: 11.5232 - val_accuracy: 0.2160\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0407 - accuracy: 0.9873 - val_loss: 11.5657 - val_accuracy: 0.2184\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0394 - accuracy: 0.9866 - val_loss: 11.5814 - val_accuracy: 0.2176\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0437 - accuracy: 0.9858 - val_loss: 11.3503 - val_accuracy: 0.2138\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.0369 - accuracy: 0.9887 - val_loss: 11.5076 - val_accuracy: 0.2178\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 17s 139ms/step - loss: 0.0428 - accuracy: 0.9859 - val_loss: 11.1113 - val_accuracy: 0.2094\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0451 - accuracy: 0.9847 - val_loss: 11.4707 - val_accuracy: 0.2174\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0366 - accuracy: 0.9881 - val_loss: 11.1521 - val_accuracy: 0.2198\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0317 - accuracy: 0.9897 - val_loss: 11.8792 - val_accuracy: 0.2168\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0309 - accuracy: 0.9901 - val_loss: 12.0268 - val_accuracy: 0.2158\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.0408 - accuracy: 0.9869 - val_loss: 11.1568 - val_accuracy: 0.2212\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 17s 138ms/step - loss: 0.0456 - accuracy: 0.9846 - val_loss: 11.1498 - val_accuracy: 0.2068\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0380 - accuracy: 0.9884 - val_loss: 11.2921 - val_accuracy: 0.2188\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 11.5166 - val_accuracy: 0.2082\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 10.9185 - val_accuracy: 0.2140\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0387 - accuracy: 0.9880 - val_loss: 11.2848 - val_accuracy: 0.2188\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 11.4733 - val_accuracy: 0.2178\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 18s 140ms/step - loss: 0.0279 - accuracy: 0.9908 - val_loss: 11.7505 - val_accuracy: 0.2164\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0391 - accuracy: 0.9870 - val_loss: 11.2960 - val_accuracy: 0.2166\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0474 - accuracy: 0.9843 - val_loss: 10.8906 - val_accuracy: 0.2182\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 17s 133ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 11.3660 - val_accuracy: 0.2148\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0386 - accuracy: 0.9874 - val_loss: 10.7123 - val_accuracy: 0.2224\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0312 - accuracy: 0.9902 - val_loss: 11.0302 - val_accuracy: 0.2164\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 11.5380 - val_accuracy: 0.2202\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 10.9747 - val_accuracy: 0.2138\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0383 - accuracy: 0.9875 - val_loss: 11.0385 - val_accuracy: 0.2124\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0279 - accuracy: 0.9914 - val_loss: 11.1949 - val_accuracy: 0.2146\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 11.2999 - val_accuracy: 0.2152\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 18s 141ms/step - loss: 0.0301 - accuracy: 0.9902 - val_loss: 11.4286 - val_accuracy: 0.2160\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0371 - accuracy: 0.9876 - val_loss: 10.8589 - val_accuracy: 0.2176\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0358 - accuracy: 0.9883 - val_loss: 10.5771 - val_accuracy: 0.2170\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0317 - accuracy: 0.9897 - val_loss: 11.0848 - val_accuracy: 0.2140\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0355 - accuracy: 0.9880 - val_loss: 10.8680 - val_accuracy: 0.2170\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0279 - accuracy: 0.9906 - val_loss: 10.9122 - val_accuracy: 0.2090\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 11.3512 - val_accuracy: 0.2116\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 18s 143ms/step - loss: 0.0359 - accuracy: 0.9883 - val_loss: 11.0766 - val_accuracy: 0.2142\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 17s 137ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 10.9807 - val_accuracy: 0.2142\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0253 - accuracy: 0.9917 - val_loss: 11.5823 - val_accuracy: 0.2190\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0263 - accuracy: 0.9913 - val_loss: 11.0150 - val_accuracy: 0.2232\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0429 - accuracy: 0.9856 - val_loss: 10.6853 - val_accuracy: 0.2158\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 11.2431 - val_accuracy: 0.2228\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0305 - accuracy: 0.9901 - val_loss: 10.7531 - val_accuracy: 0.2150\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0315 - accuracy: 0.9896 - val_loss: 10.8293 - val_accuracy: 0.2186\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0304 - accuracy: 0.9904 - val_loss: 10.4181 - val_accuracy: 0.2188\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0252 - accuracy: 0.9919 - val_loss: 10.9182 - val_accuracy: 0.2202\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 0.0309 - accuracy: 0.9904 - val_loss: 10.4629 - val_accuracy: 0.2080\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0246 - accuracy: 0.9920 - val_loss: 10.9911 - val_accuracy: 0.2258\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0276 - accuracy: 0.9909 - val_loss: 11.0238 - val_accuracy: 0.2166\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0275 - accuracy: 0.9913 - val_loss: 10.8572 - val_accuracy: 0.2216\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 18s 141ms/step - loss: 0.0252 - accuracy: 0.9919 - val_loss: 10.8790 - val_accuracy: 0.2190\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 18s 147ms/step - loss: 0.0298 - accuracy: 0.9900 - val_loss: 11.0726 - val_accuracy: 0.2168\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0354 - accuracy: 0.9889 - val_loss: 10.4305 - val_accuracy: 0.2252\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 10.9914 - val_accuracy: 0.2228\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0257 - accuracy: 0.9914 - val_loss: 10.7162 - val_accuracy: 0.2190\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0312 - accuracy: 0.9896 - val_loss: 10.5907 - val_accuracy: 0.2224\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 11.1348 - val_accuracy: 0.2220\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 18s 144ms/step - loss: 0.0193 - accuracy: 0.9934 - val_loss: 11.4153 - val_accuracy: 0.2282\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0297 - accuracy: 0.9902 - val_loss: 11.1092 - val_accuracy: 0.2174\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 11.1264 - val_accuracy: 0.2184\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0300 - accuracy: 0.9901 - val_loss: 10.5580 - val_accuracy: 0.2200\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0276 - accuracy: 0.9909 - val_loss: 10.6405 - val_accuracy: 0.2166\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 10.8152 - val_accuracy: 0.2232\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 17s 140ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 10.9626 - val_accuracy: 0.2262\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 17s 134ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 11.5068 - val_accuracy: 0.2254\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 10.5942 - val_accuracy: 0.2178\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 10.2869 - val_accuracy: 0.2234\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 11.0856 - val_accuracy: 0.2104\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.0231 - accuracy: 0.9924 - val_loss: 10.8079 - val_accuracy: 0.2208\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 11.1897 - val_accuracy: 0.2110\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 9.9557 - val_accuracy: 0.2266\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 10.5896 - val_accuracy: 0.2134\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0234 - accuracy: 0.9928 - val_loss: 10.9885 - val_accuracy: 0.2108\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 17s 137ms/step - loss: 0.0317 - accuracy: 0.9894 - val_loss: 10.1456 - val_accuracy: 0.2186\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 19s 148ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 10.6596 - val_accuracy: 0.2150\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 10.8529 - val_accuracy: 0.2158\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 10.6399 - val_accuracy: 0.2166\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0255 - accuracy: 0.9916 - val_loss: 10.7698 - val_accuracy: 0.2154\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 10.4290 - val_accuracy: 0.2180\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 18s 141ms/step - loss: 0.0202 - accuracy: 0.9935 - val_loss: 10.7181 - val_accuracy: 0.2050\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 18s 147ms/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 10.4160 - val_accuracy: 0.2220\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 10.0605 - val_accuracy: 0.2172\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0168 - accuracy: 0.9947 - val_loss: 10.9934 - val_accuracy: 0.2148\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 11.2798 - val_accuracy: 0.2188\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 10.8642 - val_accuracy: 0.2166\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 18s 144ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 10.6193 - val_accuracy: 0.2104\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 18s 142ms/step - loss: 0.0271 - accuracy: 0.9914 - val_loss: 10.4470 - val_accuracy: 0.2188\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 10.5019 - val_accuracy: 0.2140\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 10.8615 - val_accuracy: 0.2204\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0241 - accuracy: 0.9922 - val_loss: 10.3010 - val_accuracy: 0.2222\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0290 - accuracy: 0.9908 - val_loss: 10.4166 - val_accuracy: 0.2234\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 18s 146ms/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 10.1989 - val_accuracy: 0.2176\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 17s 138ms/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 10.1826 - val_accuracy: 0.2210\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0189 - accuracy: 0.9940 - val_loss: 10.3793 - val_accuracy: 0.2228\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 17s 135ms/step - loss: 0.0177 - accuracy: 0.9943 - val_loss: 10.6607 - val_accuracy: 0.2166\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 9.8557 - val_accuracy: 0.2264\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 0.0247 - accuracy: 0.9917 - val_loss: 10.1956 - val_accuracy: 0.2150\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 10.2047 - val_accuracy: 0.2162\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 18s 140ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 10.9869 - val_accuracy: 0.2152\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 17s 137ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 10.7255 - val_accuracy: 0.2082\n",
            "157/157 [==============================] - 1s 7ms/step\n",
            "KR: 0 | GE for correct key (224): 1.0)\n",
            "KR: 1 | GE for correct key (224): 1.0)\n",
            "KR: 2 | GE for correct key (224): 1.0)\n",
            "KR: 3 | GE for correct key (224): 1.0)\n",
            "KR: 4 | GE for correct key (224): 1.0)\n",
            "KR: 5 | GE for correct key (224): 1.0)\n",
            "KR: 6 | GE for correct key (224): 1.0)\n",
            "KR: 7 | GE for correct key (224): 1.0)\n",
            "KR: 8 | GE for correct key (224): 1.0)\n",
            "KR: 9 | GE for correct key (224): 1.0)\n",
            "KR: 10 | GE for correct key (224): 1.0)\n",
            "KR: 11 | GE for correct key (224): 1.0)\n",
            "KR: 12 | GE for correct key (224): 1.0)\n",
            "KR: 13 | GE for correct key (224): 1.0)\n",
            "KR: 14 | GE for correct key (224): 1.0)\n",
            "KR: 15 | GE for correct key (224): 1.0)\n",
            "KR: 16 | GE for correct key (224): 1.0)\n",
            "KR: 17 | GE for correct key (224): 1.0)\n",
            "KR: 18 | GE for correct key (224): 1.0)\n",
            "KR: 19 | GE for correct key (224): 1.0)\n",
            "KR: 20 | GE for correct key (224): 1.0)\n",
            "KR: 21 | GE for correct key (224): 1.0)\n",
            "KR: 22 | GE for correct key (224): 1.0)\n",
            "KR: 23 | GE for correct key (224): 1.0)\n",
            "KR: 24 | GE for correct key (224): 1.0)\n",
            "KR: 25 | GE for correct key (224): 1.0)\n",
            "KR: 26 | GE for correct key (224): 1.0)\n",
            "KR: 27 | GE for correct key (224): 1.0)\n",
            "KR: 28 | GE for correct key (224): 1.0)\n",
            "KR: 29 | GE for correct key (224): 1.0)\n",
            "KR: 30 | GE for correct key (224): 1.0)\n",
            "KR: 31 | GE for correct key (224): 1.0)\n",
            "KR: 32 | GE for correct key (224): 1.0)\n",
            "KR: 33 | GE for correct key (224): 1.0)\n",
            "KR: 34 | GE for correct key (224): 1.0)\n",
            "KR: 35 | GE for correct key (224): 1.0)\n",
            "KR: 36 | GE for correct key (224): 1.0)\n",
            "KR: 37 | GE for correct key (224): 1.0)\n",
            "KR: 38 | GE for correct key (224): 1.0)\n",
            "KR: 39 | GE for correct key (224): 1.0)\n",
            "KR: 40 | GE for correct key (224): 1.0)\n",
            "KR: 41 | GE for correct key (224): 1.0)\n",
            "KR: 42 | GE for correct key (224): 1.0)\n",
            "KR: 43 | GE for correct key (224): 1.0)\n",
            "KR: 44 | GE for correct key (224): 1.0)\n",
            "KR: 45 | GE for correct key (224): 1.0)\n",
            "KR: 46 | GE for correct key (224): 1.0)\n",
            "KR: 47 | GE for correct key (224): 1.0)\n",
            "KR: 48 | GE for correct key (224): 1.0)\n",
            "KR: 49 | GE for correct key (224): 1.0)\n",
            "KR: 50 | GE for correct key (224): 1.0)\n",
            "KR: 51 | GE for correct key (224): 1.0)\n",
            "KR: 52 | GE for correct key (224): 1.0)\n",
            "KR: 53 | GE for correct key (224): 1.0)\n",
            "KR: 54 | GE for correct key (224): 1.0)\n",
            "KR: 55 | GE for correct key (224): 1.0)\n",
            "KR: 56 | GE for correct key (224): 1.0)\n",
            "KR: 57 | GE for correct key (224): 1.0)\n",
            "KR: 58 | GE for correct key (224): 1.0)\n",
            "KR: 59 | GE for correct key (224): 1.0)\n",
            "KR: 60 | GE for correct key (224): 1.0)\n",
            "KR: 61 | GE for correct key (224): 1.0)\n",
            "KR: 62 | GE for correct key (224): 1.0)\n",
            "KR: 63 | GE for correct key (224): 1.0)\n",
            "KR: 64 | GE for correct key (224): 1.0)\n",
            "KR: 65 | GE for correct key (224): 1.0)\n",
            "KR: 66 | GE for correct key (224): 1.0)\n",
            "KR: 67 | GE for correct key (224): 1.0)\n",
            "KR: 68 | GE for correct key (224): 1.0)\n",
            "KR: 69 | GE for correct key (224): 1.0)\n",
            "KR: 70 | GE for correct key (224): 1.0)\n",
            "KR: 71 | GE for correct key (224): 1.0)\n",
            "KR: 72 | GE for correct key (224): 1.0)\n",
            "KR: 73 | GE for correct key (224): 1.0)\n",
            "KR: 74 | GE for correct key (224): 1.0)\n",
            "KR: 75 | GE for correct key (224): 1.0)\n",
            "KR: 76 | GE for correct key (224): 1.0)\n",
            "KR: 77 | GE for correct key (224): 1.0)\n",
            "KR: 78 | GE for correct key (224): 1.0)\n",
            "KR: 79 | GE for correct key (224): 1.0)\n",
            "KR: 80 | GE for correct key (224): 1.0)\n",
            "KR: 81 | GE for correct key (224): 1.0)\n",
            "KR: 82 | GE for correct key (224): 1.0)\n",
            "KR: 83 | GE for correct key (224): 1.0)\n",
            "KR: 84 | GE for correct key (224): 1.0)\n",
            "KR: 85 | GE for correct key (224): 1.0)\n",
            "KR: 86 | GE for correct key (224): 1.0)\n",
            "KR: 87 | GE for correct key (224): 1.0)\n",
            "KR: 88 | GE for correct key (224): 1.0)\n",
            "KR: 89 | GE for correct key (224): 1.0)\n",
            "KR: 90 | GE for correct key (224): 1.0)\n",
            "KR: 91 | GE for correct key (224): 1.0)\n",
            "KR: 92 | GE for correct key (224): 1.0)\n",
            "KR: 93 | GE for correct key (224): 1.0)\n",
            "KR: 94 | GE for correct key (224): 1.0)\n",
            "KR: 95 | GE for correct key (224): 1.0)\n",
            "KR: 96 | GE for correct key (224): 1.0)\n",
            "KR: 97 | GE for correct key (224): 1.0)\n",
            "KR: 98 | GE for correct key (224): 1.0)\n",
            "KR: 99 | GE for correct key (224): 1.0)\n",
            "157/157 [==============================] - 1s 6ms/step\n",
            "KR: 0 | GE for correct key (224): 1.0)\n",
            "KR: 1 | GE for correct key (224): 1.0)\n",
            "KR: 2 | GE for correct key (224): 1.0)\n",
            "KR: 3 | GE for correct key (224): 1.0)\n",
            "KR: 4 | GE for correct key (224): 1.0)\n",
            "KR: 5 | GE for correct key (224): 1.0)\n",
            "KR: 6 | GE for correct key (224): 1.0)\n",
            "KR: 7 | GE for correct key (224): 1.0)\n",
            "KR: 8 | GE for correct key (224): 1.0)\n",
            "KR: 9 | GE for correct key (224): 1.0)\n",
            "KR: 10 | GE for correct key (224): 1.0)\n",
            "KR: 11 | GE for correct key (224): 1.0)\n",
            "KR: 12 | GE for correct key (224): 1.0)\n",
            "KR: 13 | GE for correct key (224): 1.0)\n",
            "KR: 14 | GE for correct key (224): 1.0)\n",
            "KR: 15 | GE for correct key (224): 1.0)\n",
            "KR: 16 | GE for correct key (224): 1.0)\n",
            "KR: 17 | GE for correct key (224): 1.0)\n",
            "KR: 18 | GE for correct key (224): 1.0)\n",
            "KR: 19 | GE for correct key (224): 1.0)\n",
            "KR: 20 | GE for correct key (224): 1.0)\n",
            "KR: 21 | GE for correct key (224): 1.0)\n",
            "KR: 22 | GE for correct key (224): 1.0)\n",
            "KR: 23 | GE for correct key (224): 1.0)\n",
            "KR: 24 | GE for correct key (224): 1.0)\n",
            "KR: 25 | GE for correct key (224): 1.0)\n",
            "KR: 26 | GE for correct key (224): 1.0)\n",
            "KR: 27 | GE for correct key (224): 1.0)\n",
            "KR: 28 | GE for correct key (224): 1.0)\n",
            "KR: 29 | GE for correct key (224): 1.0)\n",
            "KR: 30 | GE for correct key (224): 1.0)\n",
            "KR: 31 | GE for correct key (224): 1.0)\n",
            "KR: 32 | GE for correct key (224): 1.0)\n",
            "KR: 33 | GE for correct key (224): 1.0)\n",
            "KR: 34 | GE for correct key (224): 1.0)\n",
            "KR: 35 | GE for correct key (224): 1.0)\n",
            "KR: 36 | GE for correct key (224): 1.0)\n",
            "KR: 37 | GE for correct key (224): 1.0)\n",
            "KR: 38 | GE for correct key (224): 1.0)\n",
            "KR: 39 | GE for correct key (224): 1.0)\n",
            "KR: 40 | GE for correct key (224): 1.0)\n",
            "KR: 41 | GE for correct key (224): 1.0)\n",
            "KR: 42 | GE for correct key (224): 1.0)\n",
            "KR: 43 | GE for correct key (224): 1.0)\n",
            "KR: 44 | GE for correct key (224): 1.0)\n",
            "KR: 45 | GE for correct key (224): 1.0)\n",
            "KR: 46 | GE for correct key (224): 1.0)\n",
            "KR: 47 | GE for correct key (224): 1.0)\n",
            "KR: 48 | GE for correct key (224): 1.0)\n",
            "KR: 49 | GE for correct key (224): 1.0)\n",
            "KR: 50 | GE for correct key (224): 1.0)\n",
            "KR: 51 | GE for correct key (224): 1.0)\n",
            "KR: 52 | GE for correct key (224): 1.0)\n",
            "KR: 53 | GE for correct key (224): 1.0)\n",
            "KR: 54 | GE for correct key (224): 1.0)\n",
            "KR: 55 | GE for correct key (224): 1.0)\n",
            "KR: 56 | GE for correct key (224): 1.0)\n",
            "KR: 57 | GE for correct key (224): 1.0)\n",
            "KR: 58 | GE for correct key (224): 1.0)\n",
            "KR: 59 | GE for correct key (224): 1.0)\n",
            "KR: 60 | GE for correct key (224): 1.0)\n",
            "KR: 61 | GE for correct key (224): 1.0)\n",
            "KR: 62 | GE for correct key (224): 1.0)\n",
            "KR: 63 | GE for correct key (224): 1.0)\n",
            "KR: 64 | GE for correct key (224): 1.0)\n",
            "KR: 65 | GE for correct key (224): 1.0)\n",
            "KR: 66 | GE for correct key (224): 1.0)\n",
            "KR: 67 | GE for correct key (224): 1.0)\n",
            "KR: 68 | GE for correct key (224): 1.0)\n",
            "KR: 69 | GE for correct key (224): 1.0)\n",
            "KR: 70 | GE for correct key (224): 1.0)\n",
            "KR: 71 | GE for correct key (224): 1.0)\n",
            "KR: 72 | GE for correct key (224): 1.0)\n",
            "KR: 73 | GE for correct key (224): 1.0)\n",
            "KR: 74 | GE for correct key (224): 1.0)\n",
            "KR: 75 | GE for correct key (224): 1.0)\n",
            "KR: 76 | GE for correct key (224): 1.0)\n",
            "KR: 77 | GE for correct key (224): 1.0)\n",
            "KR: 78 | GE for correct key (224): 1.0)\n",
            "KR: 79 | GE for correct key (224): 1.0)\n",
            "KR: 80 | GE for correct key (224): 1.0)\n",
            "KR: 81 | GE for correct key (224): 1.0)\n",
            "KR: 82 | GE for correct key (224): 1.0)\n",
            "KR: 83 | GE for correct key (224): 1.0)\n",
            "KR: 84 | GE for correct key (224): 1.0)\n",
            "KR: 85 | GE for correct key (224): 1.0)\n",
            "KR: 86 | GE for correct key (224): 1.0)\n",
            "KR: 87 | GE for correct key (224): 1.0)\n",
            "KR: 88 | GE for correct key (224): 1.0)\n",
            "KR: 89 | GE for correct key (224): 1.0)\n",
            "KR: 90 | GE for correct key (224): 1.0)\n",
            "KR: 91 | GE for correct key (224): 1.0)\n",
            "KR: 92 | GE for correct key (224): 1.0)\n",
            "KR: 93 | GE for correct key (224): 1.0)\n",
            "KR: 94 | GE for correct key (224): 1.0)\n",
            "KR: 95 | GE for correct key (224): 1.0)\n",
            "KR: 96 | GE for correct key (224): 1.0)\n",
            "KR: 97 | GE for correct key (224): 1.0)\n",
            "KR: 98 | GE for correct key (224): 1.0)\n",
            "KR: 99 | GE for correct key (224): 1.0)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 138, 12)           156       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1656)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 600)               994200    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 9)                 5409      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,442,165\n",
            "Trainable params: 2,442,165\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/200\n",
            "125/125 [==============================] - 20s 152ms/step - loss: 1.7969 - accuracy: 0.2601 - val_loss: 1.8000 - val_accuracy: 0.2186\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 21s 167ms/step - loss: 1.7691 - accuracy: 0.2644 - val_loss: 1.7947 - val_accuracy: 0.2696\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 1.7631 - accuracy: 0.2718 - val_loss: 1.7877 - val_accuracy: 0.2616\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.7595 - accuracy: 0.2728 - val_loss: 1.7911 - val_accuracy: 0.2686\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 21s 165ms/step - loss: 1.7565 - accuracy: 0.2729 - val_loss: 1.7894 - val_accuracy: 0.2584\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.7514 - accuracy: 0.2752 - val_loss: 1.8001 - val_accuracy: 0.2700\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.7470 - accuracy: 0.2795 - val_loss: 1.7985 - val_accuracy: 0.2518\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 20s 161ms/step - loss: 1.7435 - accuracy: 0.2801 - val_loss: 1.8037 - val_accuracy: 0.2608\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 20s 157ms/step - loss: 1.7362 - accuracy: 0.2830 - val_loss: 1.8134 - val_accuracy: 0.2604\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 1.7311 - accuracy: 0.2876 - val_loss: 1.8194 - val_accuracy: 0.2342\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 20s 158ms/step - loss: 1.7264 - accuracy: 0.2890 - val_loss: 1.8257 - val_accuracy: 0.2418\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 20s 157ms/step - loss: 1.7211 - accuracy: 0.2890 - val_loss: 1.8231 - val_accuracy: 0.2462\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 1.7148 - accuracy: 0.2935 - val_loss: 1.8337 - val_accuracy: 0.2256\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 20s 159ms/step - loss: 1.7122 - accuracy: 0.2956 - val_loss: 1.8238 - val_accuracy: 0.2442\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 20s 159ms/step - loss: 1.7058 - accuracy: 0.2961 - val_loss: 1.8432 - val_accuracy: 0.2362\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.6999 - accuracy: 0.3003 - val_loss: 1.8546 - val_accuracy: 0.2392\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 19s 155ms/step - loss: 1.6965 - accuracy: 0.3035 - val_loss: 1.8442 - val_accuracy: 0.2366\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 20s 161ms/step - loss: 1.6898 - accuracy: 0.3060 - val_loss: 1.8691 - val_accuracy: 0.2388\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.6861 - accuracy: 0.3058 - val_loss: 1.8711 - val_accuracy: 0.2442\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 1.6827 - accuracy: 0.3051 - val_loss: 1.8649 - val_accuracy: 0.2492\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 21s 165ms/step - loss: 1.6772 - accuracy: 0.3078 - val_loss: 1.8751 - val_accuracy: 0.2266\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.6708 - accuracy: 0.3118 - val_loss: 1.8761 - val_accuracy: 0.2446\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 1.6643 - accuracy: 0.3141 - val_loss: 1.8963 - val_accuracy: 0.2284\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 21s 167ms/step - loss: 1.6565 - accuracy: 0.3190 - val_loss: 1.8838 - val_accuracy: 0.2418\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.6523 - accuracy: 0.3217 - val_loss: 1.8810 - val_accuracy: 0.2320\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 1.6430 - accuracy: 0.3246 - val_loss: 1.9144 - val_accuracy: 0.2288\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 20s 164ms/step - loss: 1.6308 - accuracy: 0.3279 - val_loss: 1.9392 - val_accuracy: 0.2208\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 1.6183 - accuracy: 0.3372 - val_loss: 1.9479 - val_accuracy: 0.2318\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 1.6044 - accuracy: 0.3405 - val_loss: 1.9838 - val_accuracy: 0.2316\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 20s 160ms/step - loss: 1.5865 - accuracy: 0.3498 - val_loss: 1.9914 - val_accuracy: 0.2208\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 19s 155ms/step - loss: 1.5674 - accuracy: 0.3584 - val_loss: 1.9880 - val_accuracy: 0.2292\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 20s 162ms/step - loss: 1.5448 - accuracy: 0.3683 - val_loss: 2.0671 - val_accuracy: 0.2330\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 21s 165ms/step - loss: 1.5224 - accuracy: 0.3796 - val_loss: 2.1315 - val_accuracy: 0.2104\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 1.4965 - accuracy: 0.3911 - val_loss: 2.1348 - val_accuracy: 0.2140\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.4628 - accuracy: 0.4048 - val_loss: 2.1703 - val_accuracy: 0.2218\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 20s 160ms/step - loss: 1.4270 - accuracy: 0.4219 - val_loss: 2.1687 - val_accuracy: 0.2152\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 19s 155ms/step - loss: 1.3928 - accuracy: 0.4384 - val_loss: 2.3640 - val_accuracy: 0.2082\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 1.3521 - accuracy: 0.4565 - val_loss: 2.3711 - val_accuracy: 0.2206\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 20s 158ms/step - loss: 1.3185 - accuracy: 0.4687 - val_loss: 2.4816 - val_accuracy: 0.2078\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 1.2687 - accuracy: 0.4891 - val_loss: 2.5902 - val_accuracy: 0.2216\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 1.2185 - accuracy: 0.5140 - val_loss: 2.5887 - val_accuracy: 0.2180\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 21s 167ms/step - loss: 1.1718 - accuracy: 0.5305 - val_loss: 2.8347 - val_accuracy: 0.2102\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.1170 - accuracy: 0.5548 - val_loss: 2.9090 - val_accuracy: 0.2134\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 1.0594 - accuracy: 0.5781 - val_loss: 3.1311 - val_accuracy: 0.2096\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 21s 165ms/step - loss: 1.0070 - accuracy: 0.6019 - val_loss: 3.2803 - val_accuracy: 0.2090\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.9406 - accuracy: 0.6262 - val_loss: 3.3951 - val_accuracy: 0.2030\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.8707 - accuracy: 0.6567 - val_loss: 3.6328 - val_accuracy: 0.2006\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 20s 164ms/step - loss: 0.8024 - accuracy: 0.6850 - val_loss: 3.9974 - val_accuracy: 0.2028\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 19s 154ms/step - loss: 0.7329 - accuracy: 0.7144 - val_loss: 4.4777 - val_accuracy: 0.2000\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.6587 - accuracy: 0.7449 - val_loss: 4.6827 - val_accuracy: 0.2026\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 20s 159ms/step - loss: 0.5819 - accuracy: 0.7768 - val_loss: 5.2746 - val_accuracy: 0.1922\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 20s 157ms/step - loss: 0.5096 - accuracy: 0.8065 - val_loss: 5.8427 - val_accuracy: 0.1974\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.4473 - accuracy: 0.8318 - val_loss: 6.2489 - val_accuracy: 0.1986\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 20s 158ms/step - loss: 0.3828 - accuracy: 0.8575 - val_loss: 6.4788 - val_accuracy: 0.1930\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 20s 159ms/step - loss: 0.3326 - accuracy: 0.8772 - val_loss: 7.1854 - val_accuracy: 0.1942\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 0.2834 - accuracy: 0.8980 - val_loss: 7.3772 - val_accuracy: 0.1920\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 19s 156ms/step - loss: 0.2492 - accuracy: 0.9109 - val_loss: 7.9241 - val_accuracy: 0.1950\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 20s 161ms/step - loss: 0.2240 - accuracy: 0.9199 - val_loss: 8.3143 - val_accuracy: 0.2006\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 19s 154ms/step - loss: 0.1826 - accuracy: 0.9358 - val_loss: 9.1962 - val_accuracy: 0.1948\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 19s 156ms/step - loss: 0.1754 - accuracy: 0.9377 - val_loss: 9.3019 - val_accuracy: 0.2032\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 20s 161ms/step - loss: 0.1656 - accuracy: 0.9423 - val_loss: 9.5879 - val_accuracy: 0.1894\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 0.1462 - accuracy: 0.9493 - val_loss: 9.7216 - val_accuracy: 0.1932\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.1442 - accuracy: 0.9510 - val_loss: 9.8129 - val_accuracy: 0.1994\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 21s 167ms/step - loss: 0.1386 - accuracy: 0.9523 - val_loss: 10.4908 - val_accuracy: 0.1994\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 0.1104 - accuracy: 0.9626 - val_loss: 11.1148 - val_accuracy: 0.1914\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 0.1050 - accuracy: 0.9642 - val_loss: 10.7655 - val_accuracy: 0.1968\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 21s 168ms/step - loss: 0.1400 - accuracy: 0.9527 - val_loss: 10.8524 - val_accuracy: 0.1884\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.1356 - accuracy: 0.9542 - val_loss: 10.4890 - val_accuracy: 0.1996\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.1178 - accuracy: 0.9606 - val_loss: 10.7708 - val_accuracy: 0.1940\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 21s 168ms/step - loss: 0.0907 - accuracy: 0.9689 - val_loss: 11.1758 - val_accuracy: 0.1942\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.0904 - accuracy: 0.9700 - val_loss: 11.4892 - val_accuracy: 0.2004\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.1018 - accuracy: 0.9671 - val_loss: 11.2082 - val_accuracy: 0.2088\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 20s 164ms/step - loss: 0.1141 - accuracy: 0.9620 - val_loss: 11.0809 - val_accuracy: 0.1988\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 19s 153ms/step - loss: 0.0862 - accuracy: 0.9712 - val_loss: 11.5341 - val_accuracy: 0.1962\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.0909 - accuracy: 0.9701 - val_loss: 11.6306 - val_accuracy: 0.1920\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 20s 158ms/step - loss: 0.0972 - accuracy: 0.9680 - val_loss: 11.4992 - val_accuracy: 0.1872\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 20s 159ms/step - loss: 0.1132 - accuracy: 0.9625 - val_loss: 11.1942 - val_accuracy: 0.1836\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.1003 - accuracy: 0.9658 - val_loss: 11.5066 - val_accuracy: 0.1926\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 19s 156ms/step - loss: 0.0842 - accuracy: 0.9716 - val_loss: 11.6458 - val_accuracy: 0.1892\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 20s 162ms/step - loss: 0.0730 - accuracy: 0.9764 - val_loss: 12.0678 - val_accuracy: 0.1860\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.0925 - accuracy: 0.9704 - val_loss: 11.4821 - val_accuracy: 0.1960\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.0702 - accuracy: 0.9770 - val_loss: 12.0407 - val_accuracy: 0.1874\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 20s 164ms/step - loss: 0.0677 - accuracy: 0.9777 - val_loss: 11.6924 - val_accuracy: 0.1988\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.0838 - accuracy: 0.9717 - val_loss: 12.5212 - val_accuracy: 0.1974\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 19s 156ms/step - loss: 0.0823 - accuracy: 0.9725 - val_loss: 12.8547 - val_accuracy: 0.1894\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 20s 160ms/step - loss: 0.1087 - accuracy: 0.9659 - val_loss: 11.9459 - val_accuracy: 0.1930\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 19s 153ms/step - loss: 0.1025 - accuracy: 0.9661 - val_loss: 11.7770 - val_accuracy: 0.1970\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 21s 167ms/step - loss: 0.0729 - accuracy: 0.9764 - val_loss: 12.4252 - val_accuracy: 0.1850\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 0.0694 - accuracy: 0.9766 - val_loss: 12.4194 - val_accuracy: 0.1984\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 19s 153ms/step - loss: 0.0797 - accuracy: 0.9742 - val_loss: 12.6144 - val_accuracy: 0.1918\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 21s 168ms/step - loss: 0.0822 - accuracy: 0.9743 - val_loss: 11.9818 - val_accuracy: 0.2000\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 19s 153ms/step - loss: 0.0710 - accuracy: 0.9769 - val_loss: 12.3804 - val_accuracy: 0.2010\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 20s 162ms/step - loss: 0.0801 - accuracy: 0.9733 - val_loss: 12.5392 - val_accuracy: 0.1868\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 20s 156ms/step - loss: 0.0812 - accuracy: 0.9736 - val_loss: 12.5649 - val_accuracy: 0.1892\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 0.0899 - accuracy: 0.9705 - val_loss: 12.9803 - val_accuracy: 0.2014\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 21s 165ms/step - loss: 0.0874 - accuracy: 0.9720 - val_loss: 12.3409 - val_accuracy: 0.1910\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 19s 154ms/step - loss: 0.0738 - accuracy: 0.9757 - val_loss: 12.3505 - val_accuracy: 0.1948\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 20s 162ms/step - loss: 0.0612 - accuracy: 0.9802 - val_loss: 12.5232 - val_accuracy: 0.1992\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 20s 159ms/step - loss: 0.0484 - accuracy: 0.9845 - val_loss: 12.5075 - val_accuracy: 0.1980\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 19s 154ms/step - loss: 0.0380 - accuracy: 0.9873 - val_loss: 12.8954 - val_accuracy: 0.1964\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 21s 168ms/step - loss: 0.0615 - accuracy: 0.9805 - val_loss: 12.7149 - val_accuracy: 0.1982\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 19s 154ms/step - loss: 0.0967 - accuracy: 0.9699 - val_loss: 12.4097 - val_accuracy: 0.2000\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 20s 163ms/step - loss: 0.1025 - accuracy: 0.9680 - val_loss: 12.1220 - val_accuracy: 0.1942\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 20s 156ms/step - loss: 0.0822 - accuracy: 0.9734 - val_loss: 12.6852 - val_accuracy: 0.1916\n",
            "Epoch 105/200\n",
            " 47/125 [==========>...................] - ETA: 10s - loss: 0.0608 - accuracy: 0.9811Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/268/EnsembleSCA/run_ensemble.py\", line 15, in <module>\n",
            "    ensemble_aes.run_ensemble(\n",
            "  File \"/content/drive/MyDrive/268/EnsembleSCA/commons/ensemble_aes.py\", line 264, in run_ensemble\n",
            "    ge_validation, ge_attack, sr_validation, sr_attack, kp_krs = self.run_cnn(X_profiling, Y_profiling,\n",
            "  File \"/content/drive/MyDrive/268/EnsembleSCA/commons/ensemble_aes.py\", line 128, in run_cnn\n",
            "    model.fit(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
            "    return concrete_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/268/EnsembleSCA/run_ensemble.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGP3islINi4X"
      },
      "outputs": [],
      "source": [
        "%pycat /content/EnsembleSCA/run_ensemble.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "SPSddZW2TZa0",
        "outputId": "f85ea8aa-3ecd-487b-a361-4c1490b883cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/EnsembleSCA/run_ensemble.py\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-64c55a0f89f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/EnsembleSCA/run_ensemble.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nfrom commons.ensemble_aes import EnsembleAES\\nimport matplotlib.pyplot as plt\\n\\nimport tensorflow as tf\\n\\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\\'GPU\\')))\\n\\nensemble_aes = EnsembleAES()\\nensemble_aes.set_dataset(\"ascad_fixed_key\")  # \"ascad_fixed_key\", \"ascad_random_key\" or \"ches_ctf\"\\nensemble_aes.set_leakage_model(\"HW\")\\nensemble_aes.set_target_byte(2)\\nensemble_aes.set_mini_batch(400)\\nensemble_aes.set_epochs(10)\\nensemble_aes.run_ensemble(\\n    number_of_models=5,\\n    number_of_best_models=3\\n)\\n\\n# plotting GE and SR\\nplt.subplot(1, 2, 1)\\nplt.plot(ensemble_aes.get_ge_best_model_validation(), label=\"GE best validation\")\\nplt.plot(ensemble_aes.get_ge_best_model_attack(), label=\"GE best attack\")\\nplt.plot(ensemble_aes.get_ge_ensemble(), label=\"GE Ensemble All Models\")\\nplt.plot(ensemble_aes.get_ge_ensemble_best_models(), label=\"GE Ensemble Best Models\")\\nplt.xlabel(\"Traces\")\\nplt.ylabel(\"Guessing Entropy\")\\nplt.legend()\\nplt.subplot(1, 2, 2)\\nplt.plot(ensemble_aes.get_sr_best_model_validation(), label=\"SR best validation\")\\nplt.plot(ensemble_aes.get_sr_best_model_attack(), label=\"SR best attack\")\\nplt.plot(ensemble_aes.get_sr_ensemble(), label=\"SR Ensemble All Models\")\\nplt.plot(ensemble_aes.get_sr_ensemble_best...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-98>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/EnsembleSCA/run_ensemble.py'"
          ]
        }
      ],
      "source": [
        "%%writefile /content/EnsembleSCA/run_ensemble.py\n",
        "\n",
        "from commons.ensemble_aes import EnsembleAES\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "ensemble_aes = EnsembleAES()\n",
        "ensemble_aes.set_dataset(\"ascad_fixed_key\")  # \"ascad_fixed_key\", \"ascad_random_key\" or \"ches_ctf\"\n",
        "ensemble_aes.set_leakage_model(\"HW\")\n",
        "ensemble_aes.set_target_byte(2)\n",
        "ensemble_aes.set_mini_batch(400)\n",
        "ensemble_aes.set_epochs(10)\n",
        "ensemble_aes.run_ensemble(\n",
        "    number_of_models=5,\n",
        "    number_of_best_models=3\n",
        ")\n",
        "\n",
        "# plotting GE and SR\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ensemble_aes.get_ge_best_model_validation(), label=\"GE best validation\")\n",
        "plt.plot(ensemble_aes.get_ge_best_model_attack(), label=\"GE best attack\")\n",
        "plt.plot(ensemble_aes.get_ge_ensemble(), label=\"GE Ensemble All Models\")\n",
        "plt.plot(ensemble_aes.get_ge_ensemble_best_models(), label=\"GE Ensemble Best Models\")\n",
        "plt.xlabel(\"Traces\")\n",
        "plt.ylabel(\"Guessing Entropy\")\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ensemble_aes.get_sr_best_model_validation(), label=\"SR best validation\")\n",
        "plt.plot(ensemble_aes.get_sr_best_model_attack(), label=\"SR best attack\")\n",
        "plt.plot(ensemble_aes.get_sr_ensemble(), label=\"SR Ensemble All Models\")\n",
        "plt.plot(ensemble_aes.get_sr_ensemble_best_models(), label=\"SR Ensemble Best Models\")\n",
        "plt.xlabel(\"Traces\")\n",
        "plt.ylabel(\"Success Rate\")\n",
        "plt.legend()\n",
        "plt.savefig(\"yyz_cnn_plot\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndIoL1laHjrk",
        "outputId": "f7cbc1a9-c5cf-4b60-a275-29d61e1731d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 13M\n",
            "-rw------- 1 root root 6.7M Mar 14 04:54 'Copy of mlp_2_epoch20_batch100_50k.h5'\n",
            "-rw------- 1 root root 2.8M Mar 14 04:54 'Copy of mlp_epoch100_batch50_50k.h5'\n",
            "-rw------- 1 root root 2.8M Mar 14 04:54 'Copy of mlp_epoch50_batch50.h5'\n",
            "drwx------ 5 root root 4.0K Mar 18 03:20  EnsembleSCA\n",
            "-rw------- 1 root root 1.4K Mar 19 20:13  run_ensemble.py\n"
          ]
        }
      ],
      "source": [
        "!ls -lah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW0qbyq_IyBQ"
      },
      "outputs": [],
      "source": [
        "!ls commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWk-kivvI57u"
      },
      "outputs": [],
      "source": [
        "%cd commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "cudhIDXsI-ic",
        "outputId": "56aaec24-040b-4484-9b5f-0df063459ab6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1zU9R/A8dexNwgoQ1FQRNFwZyqlWRqWqaVpKjlylTkzR5a7Mq0claVlKlqapqn5y0zNmRscZLkVN4iDIRvuvr8/Tr5wcgwVDpX38/G4h3ff7+c+3/cdcrzvMzWKoigIIYQQQogyw6y0AxBCCCGEEKYlCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBkjCaAQQgghRBljUdoBPMp0Oh1Xr17F0dERjUZT2uEIUeYoisLt27fx9vbGzOzR+T4rnx1ClK5H9bOjOEkC+ACuXr2Kj49PaYchRJl36dIlKlWqVNphFJl8dgjxcHjUPjuKkySAD8DR0RHQ/wdycnIq5WiEKHsSExPx8fFRfxcfFfLZIUTpelQ/O4qTJIAPILvrxsnJST7EhShFj1o3qnx2CPFweNQ+O4pT2ez4FkIIIYQowyQBFEIIIYQoYyQBFEIIIYQoY2QMoBGKopCVlYVWqy2wXEZGBlWqVCEjI4O0tDQTRSdE2WJubo6FhUWZHqsjhBDFTRLAu2RkZBAdHU1KSkqhZXU6HfPmzePatWtcv37dBNEJUTbZ2dnh5eWFlZVVaYcihBCPBUkAc9HpdERFRWFubo63tzdWVlYFtjpotVpSU1Px9fXF3NzchJEKUTYoikJGRgbXr18nKiqK6tWrl9lFW4UQojhJAphLRkYGOp0OHx8f7OzsCi2f3UVsY2MjCaAQJcTW1hZLS0suXLhARkYGNjY2pR2SEEI88uSrtBHSwiDEw0V+J4UQonjJp6oQQgghRBkjCaAoEl9fX2bPnl3aYRSL3r1788orr6iPn332WYYPH17gc4rr9T9O7+PjYOfOnbRr1w5vb280Gg1r164t9Dnbt2+nQYMGWFtb4+/vT1hYWInHKYQQxU0SwMdITEwMw4YNw9/fHxsbGzw8PAgODmbu3LkGs5p9fX3RaDR5btOmTSu12MPCwnBxcSmVa69evZqPPvqoWOvM7/WEh4czYMCAYr2WuH/JycnUrVuXb775pkjlo6KiaNu2LS1btuTIkSMMHz6cfv36sXHjxhKOVAghipdMAnlMnDt3juDgYFxcXJg6dSpBQUFYW1tz9OhRvv/+eypWrEj79u3V8lOmTKF///4GdZTVTbFdXV1Ndq3y5cub7FqicC+++CIvvvhikcvPmzcPPz8/ZsyYAUBgYCC7du1i1qxZhISElFSYQghR7CQBNIHMa9dQMjKwcHfHzNa2RK7xzjvvYGFhQUREBPb29urxqlWr0qFDBxRFMSjv6OiIp6fnPV3j9u3bdOvWjXXr1uHi4sIHH3zAoEGD1PPx8fGMHDmS3377jfT0dBo1asSsWbOoW7cuAJGRkQwfPpyIiAg0Gg3Vq1fnu+++IykpiTfffBPI2Zh74sSJTJo0yeD6p06dokaNGhw/fpyaNWuqx2fNmsWcOXM4e/YsWq2WAQMGsHXrVmJiYqhcuTLvvPMOw4YNy/d1Pfvss9SrV0/tmo2NjaVv37789ddfeHp68vHHH+d5zsyZM1m0aBHnzp3D1dWVdu3a8dlnn+Hg4MD27dvzfT2+vr4MHz5c7XK+ePEiQ4YMYcuWLZiZmdGmTRu+/vprPDw8AJg0aRJr167lvffeY/z48cTFxfHiiy8yf/78Mpuwl6a9e/fSqlUrg2MhISGFDiFIT08nPT1dfZyYmFgS4YmHRUYKbPsEkmJLO5KyqVwVeG5caUfx0JMEsBCKopCaaXxHEK1WS1qWQkpGFubmitEyAOlxiejSUrGyc8Tc3LLI17a1NC/S7gc3b95k06ZNTJ061SD5y604dlH4/PPP+eCDD5g8eTIbN25k2LBhBAQE0Lp1awA6d+6Mra0tGzZswNnZme+++47nn3+eU6dO4erqSmhoKPXr12fu3LmYm5tz5MgRLC0tadasGbNnz2bChAmcPHkSAAcHhzzXDwgIoFGjRixdutSgy3bp0qV0794d0K/lWKlSJVauXImbmxt79uxhwIABeHl50aVLlyK9zt69e3P16lW2bduGpaUlQ4cOJTbW8IPczMyMr776Cj8/P86dO8c777zD6NGj+fbbb4v8enQ6HR06dMDBwYEdO3aQlZXFoEGDeP3119m+fbta7uzZs6xdu5bff/+duLg4unTpwrRp0/jkk0+K9HpE8YmJiVGT82weHh4kJiaSmpqKbT5f8D799FMmT55sihDFw+DE77B3TmlHUXZ515cEsAgkASxEaqaWWhMKGd+z9q8i1nbpnq59bEoIdlaF/4jOnDmDoijUqFHD4Li7u7u6Rd2gQYOYPn26em7MmDGMG2f4C7JhwwaeeeaZfK8THBzM+++/D+iTsd27dzNr1ixat27Nrl27OHDgALGxsVhbWwPwxRdfsHbtWlatWsWAAQO4ePEio0aNUlvvqlevrtbt7OyMRqMptFUyNDSUOXPmqAngqVOnOHjwID/99BMAlpaWBn9o/fz82Lt3L7/88kuREsBTp06xYcMGDhw4wJNPPgnAggULCAwMNCiXu8XH19eXjz/+mLfffptvv/0WKyurIr2eLVu2cPToUaKiovDx8QFgyZIl1K5dm/DwcPX6Op2OsLAwtcWvR48ebNmyRRLAR8jYsWMZMWKE+jgxMVH9mYvH0K0o/b8+TaBW+4LLiuJnX6G0I3gkSAL4GDtw4AA6nY7Q0FCD7ieAUaNG0bt3b4NjFStWLLC+pk2b5nmc3W0aGRlJUlISbm5uBmVSU1M5e/YsACNGjKBfv378+OOPtGrVis6dO1OtWrV7ek1du3Zl5MiR7Nu3jyZNmrB06VIaNGhg0CX8zTffsHDhQi5evEhqaioZGRnUq1evSPUfP34cCwsLGjZsqB6rWbNmngkdf/31F59++iknTpwgMTGRrKws0tLSSElJKdIi4tnX8vHxMUgEatWqhYuLC8ePH1cTQF9fX4PuXi8vrzwtksI0PD09uXbtmsGxa9eu4eTklG/rH4C1tbX6xUg8BrRZEHsMzMyhnC9oMyHmH8jK0J+/HK7/178VNB2UbzVClCZJAAtha2nOsSnGB3drtVoiI/+hbt06Be4Ekh51AV1qMlYVK2Hu7HRP1y4Kf39/NBqN2t2YrWrVqvp6jPxhcnd3x9/fv8ixFCYpKQkvLy+Drsts2cnTpEmT6N69O+vXr2fDhg1MnDiR5cuX8+qrrxb5Op6enjz33HMsW7aMJk2asGzZMgYOHKieX758OSNHjmTGjBk0bdoUR0dHPv/8c/bv3/+gL1F1/vx5Xn75ZQYOHMgnn3yCq6sru3btom/fvmRkZBQ5ASwqS0vDYQMajQadTles1xBF07RpU/744w+DY5s3b87z5Ug85rZ9Artm6u9XagyJV/S3u7lIK694eEkCWAiNRpNvN6xWq8HGQn++oATQ3MoMXaYZlpZmWBShS/deubm50bp1a+bMmcOQIUPyHQf4oPbt25fncXbXaIMGDYiJicHCwgJfX9986wgICCAgIIB3332Xbt26sWjRIl599VWsrKzUrfUKExoayujRo+nWrRvnzp2ja9eu6rndu3fTrFkz3nnnHfVYdgtkUdSsWZOsrCwOHjyotsCdPHmS+Ph4tczBgwfR6XTMmDFD3aHil19+MainKK8nMDCQS5cucenSJbUV8NixY8THx1OrVq0ixyzuX1JSEmfOnFEfR0VFceTIEVxdXalcuTJjx47lypUrLFmyBIC3336bOXPmMHr0aPr06cPWrVv55ZdfWL9+fWm9BFEaspM/gMsHDM956Se94eAJAW1MF5MQ90jWATSFYpiAUZhvv/2WrKwsGjVqxIoVKzh+/DgnT57kp59+4sSJE3kS1Nu3bxMTE2NwK2xm4u7du/nss884deoU33zzDStXrlRn17Zq1YqmTZvyyiuvsGnTJs6fP8+ePXv48MMPiYiIIDU1lcGDB7N9+3YuXLjA7t27CQ8PVxNIX19fkpKS2LJlCzdu3DBYt/BuHTt25Pbt2wwcOJCWLVvi7e2tnqtevToRERFs3LiRU6dOMX78eMLDw4v8PtaoUYM2bdrw1ltvsX//fg4ePEi/fv0MWlH9/f3JzMzk66+/5ty5c/z444/MmzfPoJ6ivJ5WrVoRFBREaGgohw4d4sCBA/Ts2ZMWLVrQqFGjIscs7l9ERAT169enfv36gH6YQv369ZkwYQIA0dHRXLx4US3v5+fH+vXr2bx5M3Xr1mXGjBn88MMPsgSMyPHWTv0t9BewdSntaITInyJUqampyrFjx5TU1NQilc/KylLCw8OVrKysAsulnT+vpBw9qmTeulUcYebr6tWryuDBgxU/Pz/F0tJScXBwUBo3bqx8/vnnSnJyslquSpUqCpDn9tZbb+Vbd5UqVZTJkycrnTt3Vuzs7BRPT0/lyy+/NCiTmJioDBkyRPH29lYsLS0VHx8fJTQ0VLl48aKSnp6udO3aVfHx8VGsrKwUb29vZfDgwQbv9dtvv624ubkpgDJx4sQCX2uXLl0UQFm4cKHB8bS0NKV3796Ks7Oz4uLiogwcOFB5//33lbp166plevXqpXTo0EF93KJFC2XYsGHq4+joaKVt27aKtbW1UrlyZWXJkiVKlSpVlFmzZqllZs6cqXh5eSm2trZKSEiIsmTJEgVQ4uLiCnw9d9dz4cIFpX379oq9vb3i6OiodO7cWYmJiVHPT5w40SB2RVGUWbNmKVWqVCnw/Xnc5Pe7mZCQoABKQkJCKUV2fx7VuMucxBhF+aapokzzVZSJTvrbdL+c+8Zu4pEgv4OKolEUJf/1S8qYtLQ0oqKi8PPzw8bGptDyWq2Ww4cPU79+/QK7gDMuXEB7+zaW3t5YmHDRYSEeF/n9biYmJuLs7ExCQgJOTkUfX1vaHtW4y5wjy2DtwMLLZWs1CZ5+t8TCEcVHfgelC9g0TNAFLIQQopjF31m6y6954WVdKkvyJx4pMgnEBNK1GVgAWdpMecOFEOJhdmI9nN8Flrbwt37LP6o8DVE7C36ebbmSj02IYiT5iAlkKvrET6vLKu1QhBBC5CczDZZ3z3u8fEDhzw0o+p7SQjwMJAE0CX0XsAy3FEKIh1jC5bzHWn8ENdrCgO0Q8y9UbADHf4eKDSH+AlR9Fs5sgQY9TR2tEA9EEkBT0GQngLJ4rxBCPLQSLuY9FjxU/693ff0NwKO2YRm3e9vRSIiHgUwCMQGNRloAhRDioRd/137tsqeseIxJC6AJSAIohBCPgITsBFADrlXh2bGlGo4QJUkSQJO4swyMdAELIcTDK7sFsNVEWdJFPPYkATQBaQEUQoiHiDYLVr2pH9Pn5g/bp4EuExKu6M87+5RufEKYgIwBNIXsBJBHNwH09fVl9uzZpR3GQ6d379688sorpR2GEOJeRO2A4+tgy2TYPw9i/4MbpyAzGTRmOZM9hHiMPZIJ4M6dO2nXrh3e3t5oNBrWrl2rnsvMzGTMmDEEBQVhb2+Pt7c3PXv25OrVqwZ13Lp1i9DQUJycnHBxcaFv374kJyeXSLymagGMiYlh2LBh+Pv7Y2Njg4eHB8HBwcydO5eUlBS1nK+vLxqNJs9t2rRpJRpfQcLCwnBxcSmWuowlq8VZvxDiEZd7OM7VI/p/X54Fvf+AoYdlVq8oEx7JLuDk5GTq1q1Lnz596Nixo8G5lJQUDh06xPjx46lbty5xcXEMGzaM9u3bExERoZYLDQ0lOjqazZs3k5mZyZtvvsnEiRMZNGhQscer0dzJs0swATx37hzBwcG4uLgwdepUgoKCsLa25ujRo3z//fdUrFiR9u3bq+WnTJlC//79DepwdHQssfiEEOKhkZWWcz/zzhf/gBfByat04hGiNCiPOEBZs2ZNgWUOHDigAMqFCxcURVGUY8eOKYASHh6ultmwYYPi6+urHD16VElNTS3StbOyspTw8HAlKyurwHKJVy4oKUePKjejThSp3vsREhKiVKpUSUlKSjJ6XqfTqferVKmizJo1657qr1KlijJlyhSla9euip2dneLt7a3MmTPHoExcXJzSt29fxd3dXXF0dFRatmypHDlyRD1/5MgR5dlnn1UcHBwUR0dHpUGDBkp4eLiybds2BTC4TZw40WgcZ86cUdq3b69UqFBBsbe3Vxo1aqRs3rxZPd+iRYs8dRVU/5IlS5SGDRsqDg4OioeHh9KtWzfl2rVrBtf8999/lbZt2yqOjo6Kg4OD8vTTTytnzpxRFEVRevXqpXTo0EEte+DAAcXd3V2ZNm3aPb2/omCpqanKsWPH8vxuJiQkKICSkJBQSpHdn0c17sfGoZ8UZaJTzu1jL0XRaks7KmFC8juoKI9kF/C9SkhIQKPRqF2Ae/fuxcXFhUaNGqllWrVqhZmZGenp6YZPVhTISM73ZpaVWuB5MpLRZKVCVipkphRa1uBWxBbDmzdvsmnTJgYNGoS9vb3RMtnd0A/i888/p27duhw+fJj333+fYcOGsXnzZvV8586diY2NZcOGDRw8eJAGDRrw/PPPc+vWLUDf6lqpUiXCw8M5ePAg77//PpaWljRr1ozZs2fj5OREdHQ00dHRjBw50mgMSUlJvPTSS2zZsoXDhw/Tpk0b2rVrx8WL+gVcV69eTaVKlZgyZYpaV0H1Z2Zm8tFHHxEZGcnatWs5f/48vXv3Vq935coVmjdvjrW1NVu3buXgwYP06dOHrKy82/pt3bqV1q1b88knnzBmzJgHfr+FECUkLSHnfoNe0PF7MCsTfw6FUD2SXcD3Ii0tjTFjxtCtWzecnJwA/Vi5ChUMF/i0sLDA2dkZrVZrWEFmCkz1Nlq3OdAA4M+CY3C486/tvQb/wVWwMp7Q5XbmzBkURaFGjRoGx93d3UlL03d1DBo0iOnTp6vnxowZw7hx4wzKb9iwgWeeeSbf6wQHB/P+++8DEBAQwO7du5k1axatW7dm165dHDhwgNjYWKytrQH44osvWLt2LatWrWLAgAFcvHiRUaNGUbNmTQCqV6+u1u3s7IxGo8HT07PA11q3bl3q1q2rPv7oo49Ys2YN69atY/Dgwbi6umJubo6jo6NBXfnV36dPH/V+1apV+eqrr3jyySdJSkrCwcGBb775BmdnZ5YvX46lpaX62u+2Zs0aevbsyQ8//MDrr79e4GsQQpSy7ASwUV94eWbpxiJEKXmsE8DMzEy6dOmCoijMnTu3tMMxuQMHDqDT6QgNDc3Tsjlq1CiDli6AihUrFlhf06ZN8zzOnmwRGRlJUlISbm5uBmVSU1M5e/YsACNGjKBfv378+OOPtGrVis6dO1Ot2r0Ntk5KSmLSpEmsX7+e6OhosrKySE1NVVsA79XBgweZNGkSkZGRxMXFodPpB4dfvHiRWrVqceTIEZ555hk1+TNm//79/P7776xatUpmBAvxKMhOAG2cSjcOIUrRY5sAZid/Fy5cYOvWrWrrH4CnpyexsbEG5bOyskhISMDc3NywIks7fUscoNPpDGbyarVa/v33X+rWrZv3ebkkx0ZjdiOOFFtz3Pxq5FsuD0u7IhXz9/dHo9Fw8uRJg+NVq1YFwNY2b9uju7s7/v7+RY+lEElJSXh5ebF9+/Y857K73idNmkT37t1Zv349GzZsYOLEiSxfvpxXX321yNcZOXIkmzdv5osvvsDf3x9bW1tee+01MjIy7jnm5ORkQkJCCAkJYenSpZQvX56LFy8SEhKi1mfsvbtbtWrVcHNzY+HChbRt27bAZFEI8RC4Ha3/V7Z6E2XYYznoITv5O336NH/99VeeVqmmTZsSHx/PwYMH1WNbt25Fp9Op3ZcqjUbfDWtlj5mNI+a2TgY3nYWtej6/m8bKHixswcKm0LIGtyKO23Nzc6N169bMmTOnxJayAdi3b1+ex4GBgQA0aNCAmJgYLCws8Pf3N7i5u7urzwkICODdd99l06ZNdOzYkUWLFgFgZWWVt/vdiN27d9O7d29effVVgoKC8PT05Pz58wZljNVl7NiJEye4efMm06ZN45lnnqFmzZp5vhjUqVOHv//+m8zMzHxjcnd3Z+vWrZw5c4YuXboUWFYI8RDI3vLNpXLpxiFEKXokE8CkpCSOHDnCkSNHAIiKiuLIkSNcvHiRzMxMXnvtNSIiIli6dClarZaYmBhiYmLUVp3AwEDatGlD//79OXDgALt372bw4MG0bdu2wJa8+6VOwCjBZQC//fZbsrKyaNSoEStWrOD48eOcPHmSn376iRMnTuR5Xbdv31bfl+xbYmJigdfYvXs3n332GadOneKbb75h5cqVDBs2DNBPomnatCmvvPIKmzZt4vz58+zZs4cPP/yQiIgIUlNTGTx4MNu3b+fChQvs3r2b8PBwNYH09fUlKSmJLVu2cOPGDYN1C3OrXr06q1ev5siRI0RGRtK9e3e12zabr68vO3fu5MqVK9y4cSPf+itXroyVlRVff/01586dY926dXz00UcGdQ0ePJjExES6du1KREQEp0+f5scff8zT2lqhQgW2bt3KiRMn6Natm9FJIkKIUrbtU/h9BFy58+XfRXb8EGVYKc9Cvi/GlvUAlF69eilRUVFGz3FnOZBsN2/eVLp166Y4ODgoTk5OyptvvqncuHHD6FIT+SnqMjAp12OUlKNHlRun/32Ql12oq1evKoMHD1b8/PwUS0tLxcHBQWncuLHy+eefK8nJyWq5KlWqGH1/3nrrrXzrrlKlijJ58mSlc+fOip2dneLp6al8+eWXBmUSExOVIUOGKN7e3oqlpaXi4+OjhIaGKhcvXlTS09OVrl27Kj4+PoqVlZXi7e2tDB482OC9fvvttxU3N7cCl4GJiopSWrZsqdja2io+Pj7KnDlzlBYtWijDhg1Ty+zdu1epU6eOYm1treT+L26s/mXLlim+vr6KtbW10rRpU2XdunUKoBw+fFh9XmRkpPLCCy8odnZ2iqOjo/LMM88oZ8+eVRQl7zIwV69eVQICApQuXboU+v9CFJ0sAyMeWNJ1w6VfJjopSlpiaUclSon8DiqKRlFkg9psaWlpREVF4efnh42NTaHltVothw8fpn79+gW2HKbdvI4SfY1Uaw2u1WsXZ8hClAn5/W4mJibi7OxMQkKCwTjfh92jGvcj7VYUfFUv53GNttBtWamFI0qX/A4+ol3Aj5qcLmDJtYUQolSk3zZ8XK1l6cQhxENCEkAT0NxZYFRDye8HLIQQwoj0u8Y4O8v4P1G2SQJoAtl7AWsUUEpyJogQQgjjcu/+AWDnZrycEGWEJIAmoLYAKvoWwAxtBplaWSpECCFMQqeD1HjDYzbOpRKKEA+Lx3Yh6IdJdgsggFbRcjb+LDpFh7utOx72HqUYmRBCPOayMuCL6pAWb3hcEkBRxkkLoCnk6gLO0GagU/Tr1t1IvcHtjNsFPVMIIcSDiDmaN/kD2QZOlHmSAJqAxkw/C1ijQJbOcIHglEzjCx4LIYQoBsnXjR+3KHypLyEeZ9IFbAp3loHRAJk6w7F/N1Jv4GrjiqW57B8rhBDF5tASSL4BZ7caP1/ErTaFeFxJAmgKmpwWwNiU2DynY1NiqehY0dRRCSHE40lRYN2Q/M+Xr2m6WIR4SEkXsCnkSgBzDuV8+7y7VVAUn+3bt6PRaIiPj8+3TFhYGC4uLiUey/nz59FoNOoe1kWJzRQmTZpEvXr1ilz+7tchxENHm5H3WPPR0HkxvPoddP/F9DEJ8ZCRBNAENLm6gLN523ur963MrYrlOjExMQwbNgx/f39sbGzw8PAgODiYuXPnkpKSM9bQ19cXjUaT5zZt2rR863722WeNPuftt98ultgfF5cvX8bKyoonnniiWOrL/lktX748z7natWuj0WgICwsrlmsJ8chLvgmnN8Otc4bH7SvAcx9C7VegblcoV6VUwhPiYSJdwKZw11gTRytHXGxcSMhIICkjCQ0PPhbl3LlzBAcH4+LiwtSpUwkKCsLa2pqjR4/y/fffU7FiRdq3b6+WnzJlCv379zeMy9GxwGv079+fKVOmGByzs7N74NgfJ2FhYXTp0oWdO3eyf/9+nnrqqQeu08fHh0WLFtG1a1f12L59+4iJicHe3v6B6xfisfFzV7h8IO9xl8qmj0WIh5y0AJqCWc7brFHAXGMOgJ2FPnnSoXvgS7zzzjtYWFgQERFBly5dCAwMpGrVqnTo0IH169fTrl07g/KOjo54enoa3ApLJuzs7PI8J3sT7exuwdWrV9OyZUvs7OyoW7cue/fuVZ9/4cIF2rVrR7ly5bC3t6d27dr88ccf6vl///2XF198EQcHBzw8POjRowc3btxQzz/77LMMGTKE4cOHU65cOTw8PJg/fz7Jycm8+eabODo64u/vz4YNG/LEvnv3burUqYONjQ1NmjTh33//LfC1/vbbbzRo0AAbGxuqVq3K5MmTycrKKvA5iqKwaNEievToQffu3VmwYEGB5YsqNDSUHTt2cOnSJfXYwoULCQ0NxcLC8DvcxYsX6dChAw4ODjg5OdGlSxeuXbtmUGbatGl4eHjg6OhI3759SUtLy3PNH374gcDAQGxsbKhZsybffvttvvHFxcURGhpK+fLlsbW1pXr16ixatOgBX7UQ90hRjCd/vs9A81Gmj0eIh5wkgIVQFIWUzBSjt9SsVNJ16aRmpeZbJiUzhZSsVFK1aaRq00jPTCNdm05KZgrp2nTSstJIzTT+/KLuG3zz5k02bdrEoEGD8k3iNCaa8fbhhx8ycuRIjhw5QkBAAN26dVMTp0GDBpGens7OnTs5evQo06dPx8HBAYD4+Hiee+456tevT0REBH/++SfXrl2jS5cuBvUvXrwYd3d3Dhw4wJAhQxg4cCCdO3emWbNmHDp0iBdeeIEePXoYdHkDjBo1ihkzZhAeHk758uVp164dmZnGx17+/fff9OzZk2HDhnHs2DG+++47wsLC+OSTTwp87du2bSMlJYVWrVrxxhtvsHz5cpKTk+/3rVR5eHgQEhLC4sWLAUhJSWHFihX06dPHoJxOp6NDhw7cunWLHTt2sHnzZs6dO8frr7+ulvnll1+YNGkSU6dOJSIiAi8vrzzJ3dKlS5kwYQKffPIJx48fZ+rUqYwfP169/t3Gjx/PsWPH2LBhA8ePH2fu3Lm4u7s/8OsW4p6k3DJ+vPfvUKONaWMR4hEgXcCFSM1K5allhXTjHS+Za+/vvh87y8K7WM+cOYOiKNSoUcPguLu7u9q6M2jQIKZPn66eGzNmDOPGjTMov2HDBp555pl8r/Ptt9/yww8/GBz77rvvCA0NVR+PHDmStm3bAjB58mRq167NmTNnqFmzJhcvXqRTp04EBQUBULVqVfV5c+bMoX79+kydOlU9tnDhQnx8fDh16hQBAQEA1K1bV4177NixTJs2DXd3d7U7e8KECcydO5d//vmHJk2aqHVNnDiR1q1bA/okslKlSqxZsyZPgpkd9/vvv0+vXr3UOD/66CNGjx7NxIkT831/FixYQNeuXTE3N+eJJ56gatWqrFy5kt69e+f7nKLq06cP7733Hh9++CGrVq2iWrVqeSZubNmyhaNHjxIVFYWPj36j+yVLllC7dm3Cw8N58sknmT17Nn379qVv374AfPzxx/z1118GrYATJ05kxowZdOzYEQA/Pz81Ec5+T3K7ePEi9evXp1GjRoB+3KIQJpN4FRa3A886pR2JEI8USQAfYwcOHECn0xEaGkp6errBuVGjRuVJTCpWLHgpmtDQUD788EODYx4ehlvZ1amT8yHs5eUFQGxsLDVr1mTo0KEMHDiQTZs20apVKzp16qSWj4yMZNu2bWqLYG5nz55VE8Dc9Zubm+Pm5qYmlLnjiY01XG6nadOm6n1XV1dq1KjB8ePGM/fIyEh2795t0OKn1WpJS0sjJSXF6LjH+Ph4Vq9eza5du9Rjb7zxBgsWLCiWBLBt27a89dZb7Ny5k4ULF+Zp/QM4fvw4Pj4+avIHUKtWLVxcXDh+/DhPPvkkx48fzzNxp2nTpmzbtg2A5ORkzp49S9++fQ3GiGZlZeHsbHzrrIEDB9KpUye1BfaVV16hWbNmD/yahSiSnV/AzTP6mxCiyCQBLISthS37u+83ek6n0xEZGUndunUxMyugNz3+EqnRt9EocNlNg6uzB642rtzOuM3l25cBsLe0p7KT4UBlWwvbIsXo7++PRqPh5MmTBsezW9hsbfPW4+7ujr+/f5Hqz+bs7Fzocywtcxa0zu521un0Yxz79etHSEgI69evZ9OmTXz66afMmDGDIUOGkJSURLt27QxaKbNlJ5J31599jYKueT+SkpKYPHmy2gKWm42N8d0Dli1bRlpamsGkD0VR0Ol0Bi2Y98vCwoIePXowceJE9u/fz5o1ax6ovvwkJSUBMH/+/DwTWMzNzY0+58UXX+TChQv88ccfbN68meeff55BgwbxxRdflEiMQhgwts2bEKJQMgawEBqNBjtLO6M3WwtbrM2ssbWwzbeMnaUddoCNhQ225jbYWtjgYOmAnaUd1ubW2FjYYGNhg7WFdZ7nFXXcnpubG61bt2bOnDnFMuasJPn4+PD222+zevVq3nvvPebPnw9AgwYN+O+///D19cXf39/gVhwzXfft26fej4uL49SpUwQGBhot26BBA06ePJknDn9//3wT/QULFvDee+9x5MgR9RYZGckzzzzDwoULHzh+0HcD79ixgw4dOlCuXLk85wMDA7l06ZLBZJFjx44RHx9PrVq11DL79xt+ocn93nh4eODt7c25c+fyvHY/P798Yytfvjy9evXip59+Yvbs2Xz//fcP+nKFKJoijpUWQhiSFkCTyEnkNApYW1jr7+dK8O7eI/heffvttwQHB9OoUSMmTZpEnTp1MDMzIzw8nBMnTtCwYUOD8rdv3yYmJsbgmJ2dnTqr15iUlJQ8z7G2tjaajBgzfPhwXnzxRQICAoiLi2Pbtm1qEjZo0CDmz59Pt27dGD16NK6urpw5c4bly5fzww8/5Nv6VFRTpkzBzc0NDw8PPvzwQ9zd3XnllVeMlp0wYQIvv/wylStX5rXXXsPMzIzIyEj+/fdfPv744zzljxw5wqFDh1i6dCk1axruMNCtWzemTJli9Hn3KjAwkBs3buS79E6rVq0ICgoiNDSU2bNnk5WVxTvvvEOLFi3U8XnDhg2jd+/eNGrUiODgYJYuXcp///1nMB5z8uTJDB06FGdnZ9q0aUN6ejoRERHExcUxYsSIPNedMGECDRs2pHbt2qSnp/P777/nm1wLUezy2+sXwMHTdHEI8YiRFkBT0IDuTq6nUcDGXN+N6GTlpE7yeNAEsFq1ahw+fJhWrVoxduxY6tatS6NGjfj6668ZOXIkH330kUH5CRMm4OXlZXAbPXp0gdeYP39+nud069atyDFqtVoGDRpEYGAgbdq0ISAgQJ2B6u3tze7du9FqtbzwwgsEBQUxfPhwXFxcCu5eL6Jp06YxbNgwGjZsSExMDP/73/+wsjK+AHdISAi///47mzZt4sknn6RJkybMmjWLKlWMLx67YMECatWqlSf5A3j11VeJjY01WO7mQbi5uRnt0gf9F4rffvuNcuXK0bx5c1q1akXVqlVZsWKFWub1119n/PjxjB49moYNG3LhwgUGDhxoUE+/fv344YcfWLRoEUFBQbRo0YKwsLB8WwCtrKwYO3YsderUoXnz5pibmxtduFqIEhF/0fDxu/9B25nw0hfQP599gIUQaJSirjVSBqSlpREVFYWfn1++Y71y02q1HD58mPr16xfcQnXjNLdvpGORBdfczPD1qpVTh07LiVsnAAh0C8RMIzm5EHfL73czMTERZ2dnEhISCmy9ftg8qnE/VDLTIPEKfN3A8PikhNKJRzxS5HdQuoBNRIOFogAavOy8DM6YaczQaDQoikKWLqvYtoUTQojH2o+vwsU9pR2FEI8saW4yBY0GsztdwBYa87tOabAw0+fhmTrjCxMLIYS4y93Jn+8z8GbeXYCEEMZJC6CpGelxtzCzIFObSXxaPPaWsrerEEIUSHvXmOnXFsETeZdtEkLkT1oATUGj0c/+ABQj69OlZqYCEJ8eb8qohBDi0fTPCsPHLsYnaAkh8icJoElochaCMdIC6GLtYspghBDi0Xbhru5fFx/j5YQQ+ZIE0CQ0OUsBGkkAy9uVN204QgjxKMu8a8F7e/kMFeJeSQJoCrl39DCSAOZe+kVW5RFCiEJkpBg+LuKuSUKIHJIAmpiiKOhSUsm8Go2SpR/InDsB1Cn3v4etEEKUCZkphZcRQhRIEkBT0OTqAtYppJ87S9atm6SdOIE2MTH3CEEUpAVQCCEKdGfiHJWbwaDw0o1FiEeUJIAmkXsSiGELX9atW2g0GnVfYOkCLl7bt29Ho9EQHx+fb5mwsDBcXFxMFtPjQqPRsHbt2iKX7927d777LwtxT7JbAFuMhvIBpRuLEI8oSQBNIdcyMHePAVQy9Is/Z6eID9IFHBMTw7Bhw/D398fGxgYPDw+Cg4OZO3cuKSk5XSa+vr5q0pn7Nm3atHzrfvbZZ40+5+23377veB8nkyZNMnhfnJ2deeaZZ9ixY0exXaOoiWpYWBgajYbAwMA851auXIlGo8HX17fY4hLC5LITQCtZN1WI+yULQZtE/pNAslv+zDRm6BTdfXcBnzt3juDgYFxcXJg6dSpBQUFYW1tz9OhRvv/+eypWrEj79u3V8lOmTKF///4GdTg6OhZ4jf79+zNlyhSDY3Z2dvcV7+Oodu3a/PXXXwDcunWLL774gpdffpnLly/j7Oxs0ljs7e2JjY1l7969NG3aVD2+YMECKleubNJYhCh22ZNALG1LNw4hHmHSAmgKGtQcUNFqDU4pd1a0z04E77cF8J133sHCwoKIiAi6dOlCYGAgVatWpUOHDqxfv5527doZlHd0dMTT09PgZm9f8LdpOzu7PM/J3kT7/PnzaDQaVq9eTcuWLbGzs6Nu3brs3btXff6FCxdo164d5cqVw97entq1a/PHH3+o5//9919efPFFHBwc8PDwoEePHty4cUM9/+yzzzJkyBCGDx9OuXLl8PDwYP78+SQnJ/Pmm2/i6OiIv78/Gzbk3Q5q9+7d1KlTBxsbG5o0acK///5b4Gv97bffaNCgATY2NlStWpXJkyeTlZVV4HMsLCzU96VWrVpMmTKFpKQkTp06pZaJj4+nX79+lC9fHicnJ5577jkiIyPV85GRkbRs2RJHR0ecnJxo2LAhERERbN++nTfffJOEhAS1lXHSpEkFxtK9e3cWLlyoHrt8+TLbt2+ne/fuecrPnTuXatWqYWVlRY0aNfjxxx8Nzp8+fZrmzZtjY2NDrVq12Lx5c546Ll26RJcuXXBxccHV1ZUOHTpw/vz5fGNctWoVQUFB2Nra4ubmRqtWrUhOTs63vBCq7DGAlvIFVIj7JQlgIfSzdlPyvZGWVuB5XUoKutQ0lLQ0dGlpaG/fRnfnvi4tDW1yMtrkZEhNh9Q0tLmeV9TxgDdv3mTTpk0MGjQo3yROY6JlEj788ENGjhzJkSNHCAgIoFu3bmriNGjQINLT09m5cydHjx5l+vTpODg4APrE6LnnnqN+/fpERETw559/cu3aNbp06WJQ/+LFi3F3d+fAgQMMGTKEgQMH0rlzZ5o1a8ahQ4d44YUX6NGjh0GXN8CoUaOYMWMG4eHhlC9fnnbt2pGZaXzv5b///puePXsybNgwjh07xnfffUdYWBiffPJJkd+H9PR0Fi1ahIuLCzVq1FCPd+7cmdjYWDZs2MDBgwdp0KABzz//PLdu3QIgNDSUSpUqER4ezsGDB3n//fextLSkWbNmzJ49GycnJ6Kjo4mOjmbkyJEFxtCnTx9++eUX9b0ICwujTZs2eHh4GJRbs2YNw4YN47333uPff//lrbfe4s0332Tbtm0A6HQ6OnbsiJWVFfv372fevHmMGTPGoI7MzExCQkJwdHTk77//Zvfu3Tg4ONCmTRsyMjLyxBYdHU23bt3o06cPx48fZ/v27XTs2LHUxsB+8803+Pr6YmNjw1NPPcWBAwcKLD979mxq1KiBra0tPj4+vPvuu6SlpZko2jLs3A5Y2gUybusfSwIoxP1ThCo1NVU5duyYkpqaqh7TJicrx2rULJWbNjm5SHHv27dPAZTVq1cbHHdzc1Ps7e0Ve3t7ZfTo0erxKlWqKFZWVuq57NvOnTvzvUaLFi0US0vLPM/56aefFEVRlKioKAVQfvjhB/U5//33nwIox48fVxRFUYKCgpRJkyYZrf+jjz5SXnjhBYNjly5dUgDl5MmTagxPP/20ej4rK0uxt7dXevTooR6Ljo5WAGXv3r2KoijKtm3bFEBZvny5WubmzZuKra2tsmLFCkVRFGXRokWKs7Ozev75559Xpk6dahDLjz/+qHh5eeX7/kycOFExMzNT3xeNRqM4OTkpGzZsUMv8/fffipOTk5KWlmbw3GrVqinfffedoiiK4ujoqISFhRm9xt1x5id3uXr16imLFy9WdDqdUq1aNeW3335TZs2apVSpUkUt36xZM6V///4GdXTu3Fl56aWXFEVRlI0bNyoWFhbKlStX1PMbNmxQAGXNmjWKoujfnxo1aig6nU4tk56ertja2iobN25UFEVRevXqpXTo0EFRFEU5ePCgAijnz58v9PUoivHfTUVRlISEBAVQEhISilSPMcuXL1esrKyUhQsXKv/995/Sv39/xcXFRbl27ZrR8kuXLlWsra2VpUuXKlFRUcrGjRsVLy8v5d133y3yNYsj7jJpopPhLTW+tCMSjyj5HVQUGQP4GDtw4AA6nY7Q0FDS09MNzo0aNYrevXsbHKtYsWKB9YWGhvLhhx8aHLu7NalOnTrqfS8vLwBiY2OpWbMmQ4cOZeDAgWzatIlWrVrRqVMntXxkZCTbtm1TWwRzO3v2LAEBAXnqNzc3x83NjaCgoDzxxMbGGtSRexycq6srNWrU4Pjx40ZfZ2RkJLt37zZo8dNqtaSlpZGSkpLvuMcaNWqwbt06AG7fvs2KFSvo3Lkz27Zto1GjRkRGRpKUlISbm5vB81JTUzl79iwAI0aMoF+/fvz444+0atWKzp07U61aNaPXK4o+ffqwaNEiKleuTHJyMi+99BJz5swxKHP8+HEGDBhgcCw4OJgvv/xSPe/j44O3t7d6Pvf7Cfr37MyZM3nGkaalpamvLbe6devy/PPPExQUREhICC+88AKvvfYa5cqVu+/Xer9mzpxJ//79efPNNwGYN28e69evZ+HChbz//vt5yu/Zs4fg4GC1K93X15du3bqxf/9+k8YtNGBV8LhlIUT+JAEshMbWlhqHDho9p9VqiYyMpG7dupibm+dbx+0bV7BNvkVmck4Zc0dHlMxMdGlpWHp6Em2ZQlLGbWwsbfFz8lWvXRT+/v5oNBpOnjxpcLxq1aoA2Bqpx93dHX9//yLVn83Z2bnQ51haWqr31XGNOv24xn79+hESEsL69evZtGkTn376KTNmzGDIkCEkJSXRrl07pk+fnqfO7ETy7vqzr1HQNe9HUlISkydPpmPHjnnO2djY5Ps8Kysrg/enfv36rF27ltmzZ/PTTz+RlJSEl5cX27dvz/Pc7Nm9kyZNonv37qxfv54NGzYwceJEli9fzquvvnpfryU0NJTRo0czadIkevTogYVFyfzKJyUl0bBhQ5YuXZrnXPnyebfpMjc3Z/PmzezZs4dNmzbx9ddf8+GHH7J//378/PxKJEZjMjIyOHjwIGPHjlWPmZmZ0apVK4Pxq7k1a9aMn376iQMHDtC4cWPOnTvHH3/8QY8ePUwVtgCwcQIzGcUkxP2SBLAQGo0GTT4tPopWCzY2mNnZYVZAAmjpYI+ZNgUzbU4ZMwcHuDMhRBsfj5uvN0nmmWBhjdk9zqx1c3OjdevWzJkzhyFDhhQ6maM0+fj48Pbbb/P2228zduxY5s+fz5AhQ2jQoAG//vorvr6+JZKk7Nu3T539GhcXx6lTp4wukwLQoEEDTp48ec8JsjHm5uakpqaq9cbExGBhYVHgMiwBAQEEBATw7rvv0q1bNxYtWsSrr76KlZUV2rsmERXG1dWV9u3b88svvzBv3jyjZQIDA9m9eze9evVSj+3evZtatWqp5y9dukR0dLSajO/bt8+gjgYNGrBixQoqVKigTgwqjEajITg4mODgYCZMmECVKlVYs2YNI0aMuKfX+CBu3LiBVqvN05Lt4eHBiRMnjD6ne/fu3Lhxg6effhpFUcjKyuLtt9/mgw8+yPc66enpBq3wiYmJxfMCyjJr086sF+JxI1+fTEKDRnPX8i/mFmisrNTHZsn6AeT3Owv422+/JSsri0aNGrFixQqOHz/OyZMn+emnnzhx4kSeFsrbt28TExNjcCvsj1JKSkqe58TFxRU5xuHDh7Nx40aioqI4dOgQ27ZtU5OwQYMGcevWLbp160Z4eDhnz55l48aNvPnmm/ec9BgzZcoUtmzZwr///kvv3r1xd3fPd1HiCRMmsGTJEiZPnsx///3H8ePHWb58OePGjSvwGllZWer7cvr0aT7++GOOHTtGhw4dAGjVqhVNmzbllVdeYdOmTZw/f549e/bw4YcfEhERQWpqKoMHD2b79u1cuHCB3bt3Ex4err5Hvr6+JCUlsWXLFm7cuJFnokt+wsLCuHHjBjVr1jR6ftSoUYSFhTF37lxOnz7NzJkzWb16tTrJpFWrVgQEBNCrVy8iIyP5+++/8wwFCA0Nxd3dnQ4dOvD3338TFRXF9u3bGTp0KJcvX85zzf379zN16lQiIiK4ePEiq1ev5vr16/km5Q+T7du3M3XqVL799lsOHTrE6tWrWb9+PR999FG+z/n0009xdnZWbz4+PiaM+DFinvOZiU3RvmgIIfJR2oMQHyb5DTTPT1ZWlhIeHq5kZWUVWC75VrSSFXVYSTl6VL1lXr+uZCUk5ByLjVb+vf6vcuLmifuO/+rVq8rgwYMVPz8/xdLSUnFwcFAaN26sfP7550pyrgklVapUUYA8t7feeivfulu0aGH0OSEhIYqi5EwCOXz4sPqcuLg4BVC2bdumKIqiDB48WKlWrZpibW2tlC9fXunRo4dy48YNtfypU6eUV199VXFxcVFsbW2VmjVrKsOHD1cnFrRo0UIZNmyYQVxVqlRRZs2aZXCMXJMTsieB/O9//1Nq166tWFlZKY0bN1YiIyPV8sYmV/z5559Ks2bNFFtbW8XJyUlp3Lix8v333+f7/kycONHgfbGzs1OCgoKUuXPnGpRLTExUhgwZonh7eyuWlpaKj4+PEhoaqly8eFFJT09Xunbtqvj4+ChWVlaKt7e3MnjwYIP/j2+//bbi5uamAMrEiRONxlLYZJG7J4EoiqJ8++23StWqVRVLS0slICBAWbJkicH5kydPKk8//bRiZWWlBAQEKH/++afB+6wo+gk4PXv2VNzd3RVra2ulatWqSv/+/dVB1rkngRw7dkwJCQlRypcvr1hbWysBAQHK119/nW/MJTUJJD09XTE3Nzd4HYqiKD179lTat29v9DlPP/20MnLkSINjP/74o2Jra6totVqjz0lLS1MSEhLUW/YEp7I8AP2epCUqik6nKNN8cyaAzHmqtKMSjzCZBKIoGkWRvceypaWlERUVhZ+fX4FjvbJptVoOHz5M/fr1CxwDmBoXg1ViDBm3c7o2LStVwtzZmbT/jgEKGndXzlrEYaYxI9Dt4W8FEcKU8vvdTExMxNnZmYSEhCJ3Pd/tqaeeonHjxnz99deAfvxo5cqVGTx4sNFJIA0bNqRVq1YG41V//vln+vbty+3btwv8LCjOuMuMY+tgZW9Q7uoJqFAL3jE+TlOIwsjv4CPaBbxz507atWuHt7e30f1IFUVhwoQJeHl5YWtrS6tWrTh9+rRBmVu3bhEaGoqTkxMuLi707du35BahNbIGn8bCAo1Gg0V5d/1jrb7rV6foZD9gIUxoxIgRzJ8/n8WLF3P8+HEGDhyoLi4O0LNnT4NJIu3atWPu3LksX76cqKgoNm/ezPjx42nXrl2Rkj9xj05tzJv8ATR5x/SxCPEYeSQngSQnJ1O3bl369OljdKbmZ599xldffcXixYvx8/Nj/PjxhISEcOzYMbX1IDQ0lOjoaDZv3kxmZiZvvvkmEydOZNCgQcUfsEaTJwfU3JnokP0vWVqw1t9VUNS9gYUQJev111/n+vXrTJgwgZiYGOrVq8eff/6pTgy5ePEiZrlmm44bNw6NRsO4ceO4cuWKurD4vSwULu6BseRv5GlwqGD6WIR4jDzyXcAajYY1a9aoA/oVRcHb25v33ntPHcSekJCAh4cHYWFhdO3alePHj1OrVi3Cw8Np1KgRAH/++ScDBw7kf//7H/7+/sXbBZxwHeuEK6Qn5uTbNoGBaMzN0d6+TcaFC2isrTlbTr9jQg3XGliYPZK5uRAloiS7gEvDoxq3yaUlwDQje1ePvQzWsgaguH/yO/iIdgEXJCoqipiYGFq1aqUec3Z25qmnnlLX9dq7dy8uLi5q8gf6mY5mZmZ5FkwuDhqNhtwNehoLSzR3EkaNtb7ZT0lPp/ydSbj3OxNYCCEeKzs+M35ctoAT4oE9ds1MMTExQN4dKjw8PNRzMTExVKhg2H1gYWGBs7NzgUuO6HSG4/OKvjyJBo2ZAhYayFIws8/58NLkWsTYMUXhupMGrbEuDyGEKGtij+XcrxcKlZtA5aZgJmMthXhQj10C+CCyE7z8dpGIiYnh6tWr917xnTGAipMZNi5V1FY//am8Y/2ydFn3fg0hHmMPsrOLeIRdPZJzv34PqNI036JCiHvz2CWAnp6eAFy7ds1gC7Fr165Rr149tczde8VmZWVx4sQJFEXh6tWrlC9fHisrK4MEzcXFBWfnnNXntVotp06dIi0trcAxgBkZmZClkIFWP5g8M1N/uyM91x83XaaGlNQULHWWxqoSokxRFIWMjAyuX7+OmZkZVrkWTxePucRoSL2V89iy8HHZQoiie+wSQD8/Pzw9PdmyZYua8CUmJrJ//34GDhwI6Deyj4+P5+DBgzRs2BCArVu3kpGRQcWKFdHpdEVq6dPpdNy4cYPz588bzBK8W1Z6ChapN8jEEsvkvImiLiMDbUICADfSINU6FUfZ5FwIlZ2dHZUrVy7w90w8ZhJzfQY3HgBe9UotFCEeR49kApiUlMSZM2fUx1FRURw5cgRXV1cqV67M8OHD+fjjj6levbq6DIy3t7c6UzgwMJA2bdrQv39/5s2bR2ZmJoMHD6Zr1674+Pio+3sWNsYvKSmJtm3bEhERgYODQ77lov/dgdfu9ziPN76D1+U5r2Rlca5dewC+6mnOc7XbMaDGgPt4Z4R4/Jibm2NxZ91MUYZk3tnq0L0GvPR56cYixGPokUwAIyIiaNmypfo4e/P4Xr16ERYWxujRo0lOTmbAgAHEx8fz9NNP8+effxosH7F06VIGDx7M888/j5mZGZ06deKrr74C9OPyLC0tsbQsuBs2IyODCxcuYGVlVeCyMVYWGmySLmGJLt9yZjduQGYmcUnm3Mq6VaRlaIQQ4rGVnQBa2pZuHEI8ph7JBPDZZ58tcLcMjUbDlClTmDJlSr5lXF1dWbZsWUmEl4e5uf5tNi9geRcza2t0mZlYZUFK9gefEEKUVdmfg1b2pRuHEI8pGVBjAmYW+oHr5uTfpay50+JnlQUbzm/gvxv/mSQ2IYR4KGWm6v+VFkAhSoQkgCagtgAWkACa3VkaxurO5OC+m/qWeFxCCPFQSo2H9fqdnCQBFKJkSAJoAuYWOQlgfl3X2S2A1ln688mZyaYJTgghHjZbJkP2Z6CldAELURIkATQBc/PsLmAdWTrjCeDdLYBCCFFmXdiTc19aAIUoEZIAmoC5pb4F0AIt2nwSQI2d/kPOOlcCKJNBhBBlU64lf2TfXyFKhCSAJmBukdMCuPVErNEyFu7lASiXlHPsWso1ImIi2H1ld4nHKIQQDyVz2RVJiJIgCaAJWFjoP8As0DJs+WGjZSw9PABwvZ3TQhibEsubG9/k7b/eJj4tvsTjFEKIh4LuTleImQXU6166sQjxmJIE0ATMcs0CtshnKyuLOwmg2+2cY2fic3Y7iU013nIohBCPnYw7w1/6bYHyNUo3FiEeU5IAmoDGPLsFUIefu/EZbRYVKgDgr3VTj528dVK9fzP1ZglGKIQQDxF1FxAZ/ydESZEE0BTM9C2AZhqFmh7GE0BzJ0cAqpiXJ8Q3BIA1Z9ao52+k3ijhIIUQ4iEh28AJUeIkATQFM3P1bmZmuvEiDvoEUHc7icT0xDznLyReKJnYhBDiYaLNAm2G/r5sAydEiZEE0BTMcrZczsw0vtCfuaMDALrbt7l0+1Ke8xuiNpRMbEII8TDJSs25Ly2AQpQYSQBNoQgJoJmDPgHUJifTu1avPOcv3r5Il/91IVMrK0ULIR5j2RNA0ICFTamGIsTjTBJAUzBIADOMF3F0zC5AJ9/2DK0/NE+Z47eOG8wMFkKIx07uCSAaTcFlhRD3zaLwIuKBaXLybG1WPi2Adnc+7BQFJT6BPrXfJEObjqW5FcdvHuevi38BcCXpCrYWtlR2qoyZRvJ3IcRjRiaACGESkgCagkaDTmOBmZJFVj5dwBozMyw8PMiKiSFh3f9I3PgnLyrg1KYNViGDiE6O5r+b//F5+OdcTb5K3yf6MrzhcNO+DiGEKGkZsgSMEKYgTUimcmcmcGY+LYAADs2bA5ByMIL0Y8dJP36c67NmEdO9F409GwNwNfkqAAv+XVDCAQshRClIitH/61C+dOMQ4jEnCaCJKHfGAWqzsvItYx0QAEDaf8cMjmtv3cJNK4OhhRBlQPydVRCcfUo3DiEec5IAmsqdBDArn0kgkLMYtPZm3l0/vP+JyXMsOim6mIITQoiHRPxF/b8ulUs3DiEec5IAmojGTL8dXEZmJlqdYrSMOhPYCOeLcXmOvbHhjeIJTgghHhYJd1oAJQEUokRJAmgiGnN9C6AFWpIzjHcDmzs55Tlm6e0NgE1ccp5zsSmxKIrxZFIIIR5J2S2A0gUsRImSBNBENHe6gM3RkpRmPAHU2OQd51d+xAgALP/ai2NK3mTvVtqtYoxSCCFKyem/IGIh3DqnfywtgEKUKEkATeXOLGBLtNzOJwG09PIyeGxVtSqWHhXUx+PXWvK0ZzMcLB3UY5eTLpdAsEIIYUIpt2BpJ/j9XchI0h9zkRZAIUqSrANoKrlbANONLwVj4eqK76+rQKslefdunNq1x8LdDTN7e3TJyfheSGOm9xBimzrSf1N/opOjOXnrJLXdamNhJj9KIcQjKinW8LGNC1jnPyZaCPHgpAXQVO4kaBYaXb4tgAC2tWtjW6cO7gMHYlWpImY2NtQ4GIHdU08BkH7qFFWcqvCUl/7xR/s+YsjWISUfvxBClJTUu4aySPevECVOEkBTydUCWFACmJ/sNQIzovTjYyo5VFLP7bqyqxgCFEKIUrLoRcPHkgAKUeKk39BU7owBtEBHWqb2np9u4eYKQFZ8PADVlPL4XFew0EKcAyRnJmNvaV9s4QohRKmxl11AhChpJmsB7NWrFzt37jTV5R4+uVoA7ycBNHd2BkCXkABApWGzmfGDlumLtHzzrZYD0QeKL1YhhDCVrPS8x2xdTB6GEGWNyRLAhIQEWrVqRfXq1Zk6dSpXrlwx1aUfDmY56wCmZeru/el31gjUJiSSFReHcu26es5SC/P/mlY8cQohhCmlJeY9Zp13TVQhRPEyWQK4du1arly5wsCBA1mxYgW+vr68+OKLrFq1isxM47NiHytqC+D9dQGbO+lbALWJiaQePpLnfP+FZSyhFkI8HtIS8h6ztDN9HEKUMSadBFK+fHlGjBhBZGQk+/fvx9/fnx49euDt7c27777L6dOnTRmOaaljALWkZd1PF/CdFsDEBNL++y/Pec8b916nEEKUOmMJoEZj+jiEKGNKZRZwdHQ0mzdvZvPmzZibm/PSSy9x9OhRatWqxaxZs0ojpJJn0AJ4713A2WMAtXHxRhPAmy4yoVsI8YiIjoSrR/T30+8kgBVq55yXLS6FKHEmmwWcmZnJunXrWLRoEZs2baJOnToMHz6c7t2743RnfNuaNWvo06cP7777rqnCMh11HcD7mwRi4eEBgJKaStL27XnOa7TygSmEeARkZcB3zfX3x16G1Dj9fRvnnDJO3qaPS4gyxmQJoJeXFzqdjm7dunHgwAHq1auXp0zLli1xcXExVUimZW6p/wcdqfeRAJrZ2GDu5ob25k2j5+3SFNKy0rCxyLufsBBCPDSyt3oDiLsACXfGLzt5wetL4XI41Hy5dGITogwxWQI4a9YsOnfujI1N/gmKi4sLUVFRpgrJtHKNAUy6jy5gAEtvbzUBdHzhBVKPHiUrOhoAm0yIS7qBl0ulgqoQQojSlZmScz/xKsRf1N93qQyBL+tvQogSZ7IEsEePHur9S5cuAeDjU4Y2+zZYBub+Jmw4tX2JtKNHwcyMCiPexczRESUrizMtngUgdHEbylerzeI2i6UlUAjxcMpMzbm/rHPOfecy9PdAiIeAyWYOZGVlMX78eJydnfH19cXX1xdnZ2fGjRtXppaBud9ZwABuvXtT7a/N+P68DCtfXyzc3LD08OBSRWsAgo8pHLt5jAm7J/DfDcOJIjMPzmTQlkFk6YxvQ7fm9Bq6/K8LV5Ou3ldsQghRJLlbAHOzdzdtHEKUcSZLAIcMGcL333/PZ599xuHDhzl8+DCfffYZCxYsYOjQoaYKo/Q84F7A2awqVcK2bl2DY5Ytnwag4k39RJAN5zfQdX1XAFIyUzgSe4RF/y5i5+WdRFyLMFrvhD0TOH7rODMPzrzv2IQQolAZ+SSAFramjUOIMs5kXcDLli1j+fLlvPhizqbfderUwcfHh27dujF37lxThVI6cu0FHJeSUaxV168bQvRPW3BPNJwJrCgKL65+kVtpt9Rjt1Jv3f10A9FJ0cUamxBCGMivBdDCyrRxCFHGmawF0NraGl9f3zzH/fz8sLIqA7/4uVoAbyUVbwJoVbEiAF63LQ2OJ2YkGiR/AOHXwvM8PyXXB3KGrnhjE0IIA/kmgDJuWQhTMlkCOHjwYD766CPS03M2/k5PT+eTTz5h8ODBpgqj9KjrAOpIzrj/iSDGWPn5AeAal4lTck4r4BcRX+Qpu/7ceq4lX+O1da/x5aEvAbicdFk9fz3lep7nCCFEsck9CSQ3C2vTxiFEGWeyLuDDhw+zZcsWKlWqRN07Y9giIyPJyMjg+eefp2PHjmrZ1atXmyos07mTAFpq9EvAxKdk4ulsXixVW5Qrh3XNmqSfOEGTEwqbGuq3UVp7Zq1a5vdXf+flNS+TmpVKq1WtADgZd5JhDYZx+XZOAngz7SYJ6Qk4WzsjxMMsIyPjod0+UqvVFji5LSMjgypVqpCRkUFaWpoJI3sIZGSBg5EZv4oVlLX3QpQYS0tLzM2L52/s48pkCaCLiwudOnUyOFaSy8BotVomTZrETz/9RExMDN7e3vTu3Ztx48ahubPPpKIoTJw4kfnz5xMfH09wcDBz586levXqxR/QnQTQ3gLIhFvJGXg6F1+Xh/MrHYiddoI3LldmU8PLec5Xcapi9HlBi4PyHDsQc4B65evRalUrLDQWrHt1HRUdKhZbrEI8iJSUFIYMGcLixYsNjg8ZMoSKFSvy/vvvl1Jk+s+UmJgY4uPjCyyn0+mYN28e165d4/r1MtbqbuEPwTPyHo/XQeJjug6sKBUuLi54enqqf/OFIZMlgIsWLTLVpQCYPn06c+fOZfHixdSuXZuIiAjefPNNnJ2d1VnHn332GV999RWLFy/Gz8+P8ePHExISwrFjxwpcsPq+3JkEYm+pQKo+ASxO9k2a6P+9EkegayDHbx1Xz33Z8st7qmvE9hE09WqKTtGRoWSw4uQKRjQcUazxCnG/xo4dS2RkJNu3b6dNmzYkJycD0KpVKyZNmlSqCWB28lehQgXs7Ozy/cOj1WpJTU3F19e37LVSJMdCspE/PW5VwLwMjAcXJU5RFFJSUoiNjQX0O5GJvEyWAGa7fv06J0+eBKBGjRqUL1++RK6zZ88eOnToQNu2bQHw9fXl559/5sCBA4D+P8js2bMZN24cHTp0AGDJkiV4eHiwdu1aunbtWrwB3WkBtLvzjt8q5pnAVpUrA6BNSODl1Ve42kghwUFDQ4+GPFf5OQCGNxjO7EOz862jXdV2/O/c/wDYG71XPR6fFl+ssQrxINauXcuKFSto0qSJQYJVu3Ztzp49W2pxabVaNflzc3MrtCyAjY1N2UsA083AwkhibGMH5ib/kyQeU7a2+mWFYmNjqVChQtn7PSsCk00CSU5Opk+fPnh5edG8eXOaN2+Ot7c3ffv2JSUln1lhD6BZs2Zs2bKFU6dOAfrxhrt27VKXoYmKiiImJoZWrVqpz3F2duapp55i7969RutMT08nMTHR4FZk2V3AdybqnrueVEDhe2dmZ4eVfzUAntx7iw9+0f+BWRSS0/LaN6gvf732Fy/5vUSfJ/rkqaONXxujdR+IOYCiKEbPCWFq169fp0KFCnmOJycnl2pXT/aYPzs7u1KL4ZGgy2cCnHTTiWKW/btYJjabuA8mSwBHjBjBjh07+N///kd8fDzx8fH89ttv7Nixg/fee6/Yr/f+++/TtWtXatasiaWlJfXr12f48OGEhoYC+q4aAA8PD4PneXh4qOfu9umnn+Ls7Kze7mkM450EUJul/484+6/TxZ5UeX30kXrf7xpUjlXy/EH0sPdgevPpPF/5+TzPr+5SnfWvrs9z/ErSFYOJIkKUpkaNGrF+fd7/pz/88ANNmzYthYgMyXijQij5JYAm+3Mkygj5XSyYydrbf/31V1atWsWzzz6rHnvppZewtbWlS5cuxb4Q9C+//MLSpUtZtmwZtWvX5siRIwwfPhxvb2969ep1X3WOHTuWESNyxsIlJiYWPQm8kwCWtzOHm/pDyRlaHKyL70dgV78+Pj/8wKV+/QD4YoGW1Db/Yhv0RJ6ynvae6v2pT0/F1cYVLwcvgzUB3W3dcbF24Uz8GV5a8xL/9PxHfqFEqZs6dSovvvgix44dIytLv6vOK6+8woEDB9ixY0cpRycKld0C6FIFUCD+ov6xfLYIYVIm+8qVkpKSp7UNoEKFCiXSBTxq1Ci1FTAoKIgePXrw7rvv8umnnwLg6alPgK5du2bwvGvXrqnn7mZtbY2Tk5PBrcjuJIA1KuRsd1TcC0IDODwdjGKTs57W+c6dSf3vvzzl3Gxyxig9V/k5gisGA2BnmdN9VcO1Bs28m6mPj906VuzxCnGvnn76aY4cOUJWVha1atUCoHz58uzdu5eGDRveV53ffPMNvr6+2NjY8NRTT6ljhfMTHx/PoEGD8PLywtramoCAAEk+jdBoNKxduzbngKJAxp3hL2bmj1Sr37PPPsvw4cPVx76+vsyePbvA5+R5/fepuOoRIjeT/fY1bdqUiRMnGqx5lZqayuTJk0uk2yYlJQUzM8OXZ25ujk6nX4fPz88PT09PtmzZop5PTExk//79JdONdCcBtNLoqOiiTwLfWLCfTK2u2C/l8NRTBo8T163LU8bczJxNnTax/tX12FvaG63nhSovMKDOAPVx6PpQmRAiHgrVqlVj/vz5bNu2DYD58+cTFJR3SaOiWLFiBSNGjGDixIkcOnSIunXrEhISos4gvFtGRgatW7fm/PnzrFq1ipMnTzJ//nyjX3AfBdevX2fgwIFUrlwZa2trPD09CQkJYffu3WoZX19fNBoNGo0GOzs7goKC+OGHH+79YhnJOffNrR54949JkyZRr169B6rjfoWHhzNgwIDCC96D/F5PdHS0wTaqQhQHk3UBz549mzZt2uRZCNrGxoaNGzcW+/XatWvHJ598QuXKlalduzaHDx9m5syZ9Omjn/yg0WgYPnw4H3/8MdWrV1eXgfH29uaVV14p9niyE0B0WZSzt+RKfCoXb6Ww7UQsL9Q23uJ4v7ynTiX6gw9JutMikXHhotFyXg7Gp8YveXEJx28e51X/V9FoNHzf+nsGbB6AVtHy3T/fMabxmGKNV4h7YW5uTnR0dJ6JIDdv3qRChQrqDNuimjlzJv379+fNN98EYN68eaxfv56FCxcaXVJm4cKF3Lp1iz179mBpqZ/V5evrS1paGlFRj946dp06dSIjI4PFixdTtWpVrl27xpYtW7h586ZBuSlTptC/f39SUlJYuXIl/fv3p2LFiveWmOQe/2d5pzekXFUwtzRe/iFWUitYGJNfr5QQD8JkLYBBQUGcPn2aTz/9lHr16lGvXj2mTZvG6dOnqV27drFf7+uvv+a1117jnXfeITAwkJEjR/LWW2/xUa6JEqNHj2bIkCEMGDCAJ598kqSkJP7888/iXwMQ1HUA0WWRkpHzIagrgcm1Fm5u+Hw3j0rffgNA1o0b9/T8+hXq0z2wuzre7ymvnBbFP8//WXyBCnEf8ps8lZ6efs/7imdkZHDw4EGD1QDMzMxo1apVvqsBrFu3jqZNmzJo0CA8PDx44oknmDp16j0nng+D+Ph4/v77b6ZPn07Lli2pUqUKjRs3ZuzYsbRv396grKOjI56enlStWpUxY8bg6urK5s2bC71GduuVra0tVQPrsur3vyBXr8OlG4l0eaM3Li4uuLq60qFDB86fP6+e3759O40bN8be3h4XFxeCg4O5cOECYWFhTJ48mcjISLV1MiwsLM/1N23ahI2NTZ7FuYcNG8Zzz+mXyLp58ybdunWjYsWKagvnzz//XODrursL+PTp0zRv3hwbGxtq1apl9L0ZM2YMAQEB2NnZUbVqVcaPH6/OUC3o9dzdBXz06FGee+45bG1tcXNzY8CAASQl5aws0bt3b1555RW++OILvLy8cHNzY9CgQTIbVhgwSQtgZmYmNWvW5Pfff6d///6muCSOjo7Mnj27wDEaGo2GKVOmMGXKlJIPSG0B1PJW86qM+fUoQLHuCXw3iztrkWXd9U3+XplpzJjbai4D/xrIjdQbRF6PpG75usURohBF9tVXXwH639sffvgBBwcHdUjJN998w4EDB6hZs+Y91Xnjxg20Wq3R1QBOnDhh9Dnnzp1j69athIaG8scff3DmzBneeecdrKys1HVHsymKQmqmFp1OZ5C4arU60rIUUjKyMDcv/m+BtpbmRZqw5eDggIODA2vXrqVJkyZYWxe+H69Op2PNmjXExcUVKeEeP34806ZN48svv+THhd/T9Z2xHH3iCQLLB5CZmUlISAhNmzbl77//xsLCgo8//pg2bdrwzz//YGZmxiuvvEL//v35+eefycjI4MCBA2g0Gl5//XX+/fdf/vzzT/766y9Av5TX3Z5//nlcXFz49ddf6du3L6Bfh3HFihV88sknAKSlpdGwYUPGjBmDk5MT69evp0ePHlSrVo3GjRsX6T3p2LEjHh4e7N+/n4SEBIPxgtkcHR0JCwvD29ubo0eP0r9/fxwdHRk9enSRX09ycrL6noWHhxMbG0u/fv0YPHiwQQK8bds2vLy82LZtG2fOnOH111+nXr16JvsbLB5+JkkALS0ty95+l3fL1QX8+pOVmbHpFLG300lMK7lvZOZu7gBob9xAUfIuCXMvcid88/+Zz5zn5zxwfELci1mzZgH6pGrevHkGY3oXLlxI1apVmTdvXonHodPpqFChAt9//z3m5uY0bNiQK1eu8PPPP+dJAFMztdSaUMAQl7V/lUiMx6aEYGdV+Me7hYUFYWFh9O/fn3nz5tGgQQNatGhB165dqVOnjkHZMWPGMG7cONLT08nKysLV1ZV+d1YcKEjnzp3Vch9N+IDNmzby9Q8/8W2zF1mxYgU6nY4ffvhB/XxatGgRLi4ubN++nUaNGpGQkMDLL79MtWr6dU4DAwPVuh0cHLCwsCiwi9Tc3JyuXbuybNkyNQHcsmUL8fHx6vakFStWZOTIkepzhgwZwsaNG/nll1+KlAD+9ddfnDhxgo0bN+Lt7Q3kzFbPbdy4cep9X19fRo4cyfLlyxk9ejS2trZFej3Lli0jLS2NJUuWYG+vb0mdM2cO7dq1Y/r06eqXmXLlyjFnzhzMzc2pWbMmbdu2ZcuWLZIACpXJuoAHDRrE9OnT1WUbypxcCSBAyxr68UuJqSWXAFq461sAlcxMMnJ1qdwPRytHJjWdBMCOyzs4dO3QA0YnxL2JiooiKiqKFi1aEBkZSVRUFEeP6lvSDx48yMaNG3nqrglQhXF3d8fc3PyeVgPw8vIiICDAYGeBwMBArl+//kgumN6pUyeuXr3KunXraNOmDdu3b6dBgwZ5ulNHjRrFkSNH2Lp1K0899RSzZs3C39+/0PoNJtUpOpo2rMPxU/odWyIjIzlz5gyOjo5qa6SrqytpaWmcPXsWV1dXevfuTUhICO3atePLL78kOjr6nl9jaGgo27dv5+rVqwAsXbqUtm3b4uLiAuhbBD/66COCgoJwdXXFwcGBjRs3cvGi8fHTdzt+/Dg+Pj5q8pfndd+xYsUKgoOD8fT0xMHBgXHjxhX5GrmvVbduXTX5AwgODkan06m7bIF+Z5zc/0e9vLzyndgkyiaTTQIJDw9ny5YtbNq0iaCgIIP/vACrV682VSilI9cYQAAnW/1b/8fRGAY/V71kLmljg22DBqQeOkT8ylV4jB71QPV1CujEtkvb2HF5B73+7MWqdquo4VqjmKIVomiyZ/4WBysrKxo2bMiWLVvUyV86nY4tW7YwePBgo88JDg5m2bJl6HQ6daWBU6dOUaFChTyt7LaW5hybEpKnDq1WS2TkP9StW6dEtqiytby3Om1sbGjdujWtW7dm/Pjx9OvXj4kTJ9K7d2+1jLu7O/7+/vj7+7Ny5UqCgoJo1KiRuhRP0dxJkO+8TUlJSTRs2JClS5fmKZk9yWLRokUMHTqUP//8kxUrVjBu3Dg2b95Mkzv7nxfFk08+SbVq1Vi+fDkDBw5kzZo1Bgnu559/zpdffsns2bPVv0/Dhw8nI6P4lurau3cvoaGhTJ48mZCQEJydnVm+fDkzZswotmvklj1BKZtGo1FbzIUAEyaALi4uanN7mZRrDCCA7Z3umWPRiaRlarG5xw/soirXrSuphw6RUsi6ZkXV54k+7Lisn1286cImSQBFqbh8+TLr1q3j9OnTAHzwwQfqeLSZM2feU10jRoygV69eNGrUiMaNGzN79mySk5PVWcE9e/akYsWK6hqiAwcOZM6cOQwbNowhQ4Zw+vRppk6dytixY/PUrdFojHbFarUabCz05x7GPUpr1apV4LpzPj4+vP7664wdO5bffvutwLr27dtHz5499Q8UhX2HjlL/zkoQDRo0YMWKFVSoUKHAdVXr169P/fr1GTt2LE2bNmXZsmU0adIEKyurIk++CQ0NZenSpVSqVAkzMzOD7vrdu3fToUMH3njjDUD/JeDUqVNFTm4DAwO5dOkS0dHReHl5qa87tz179lClShU+/PBD9diFCxcMyhTl9QQGBhIWFkZycrLakLJ7927MzMyoUUM+j0XRmSwBXLRoUeGFHmd3dQG3DvTgqy36P143ktKpVK5k9g+1rV8fgLSTJ1G0WjQP+MemgUcDhtQfwteHv+Zi4r11XQhRHLZs2UL79u2pWrWqOlHjp59+AvQJxb16/fXXuX79OhMmTCAmJoZ69erx559/qmOpLl68aLCmqI+PDxs3buTdd9+lTp06VKxYkWHDhtG/f/977s4rbTdv3qRz58706dOHOnXq4OjoSEREBJ999hkdOnQo8LnDhg3jiSeeICIigkaNGuVbbuXKlTRq1Iinn36apQu/48CR/1gwR5+kh4aG8vnnn9OhQwemTJlCpUqVuHDhAqtXr2b06NFkZmby/fff0759e7y9vTl58iSnT59WE0pfX1+ioqI4cuQIlSpVwtHRMd+JLKGhoUyaNIlPPvmE1157zaBc9erVWbVqFXv27KFcuXLMnDmTa9euFTkBbNWqFQEBAfTq1YvPP/+cxMREg0Qv+xoXL15k+fLlPPnkk6xfv541a9YYlCnK6wkNDWXixIn06tWLSZMmcf36dYYMGUKPHj0e2bUoRekw2RjA5557Ls80fNAvvpw9Ff+xdlcCGFTJGS9n/XIzN0tgR5Bslt7eaKysIDOTzCtXiqVOfxf9uJ+Ltx+tP3bi8TB27FhGjhzJ0aNH1SWbjh07RosWLejcufN91Tl48GAuXLhAeno6+/fvNxhLuH379jzj4Zo2bcq+ffvUsWoffPDBQ9mSVxgHBwd1PF/z5s154oknGD9+PP3792fOnIInetWqVYsXXniBCRMmFFhu8uTJLF++nDp16rBk+Up+/mYqtQIDALCzs2Pnzp1UrlyZjh07EhgYSN++fUlLS8PJyQk7OztOnDhBp06dCAgIYMCAAQwaNIi33noL0I9fbNOmDS1btqR8+fIFLt3i7+9P48aN+eeff9Q94bONGzeOBg0aEBISwrPPPounp+c9rQdrZmbGmjVrSE1NpXHjxvTr10+dYZytffv2vPvuuwwePJh69eqxZ88exo8fb1CmKK/Hzs6OjRs3cuvWLZ588klee+01nn/++UJ/XkLcTaOYaNSymZkZMTExeRZvjY2NpWLFio/k+kSJiYk4OzuTkJBQ+LZwp/+CpZ3Asw68/TcAbb/6m/+uJvJ1t/q0q+td8PMfwLl27Uk/fRqf7+bh0KLFA9d3Ju4Mr657FQdLB/Z02yP7AwuTcnR05MiRI1SrVg0XFxcSEhJISEggKioqzxpyppS9ELSfn1+ha4lqtVoOHz5M/fr1H8nE8b4lXoWka2DvDs5F3EddiPtU0O/kPf39fkyVeBfwP//8o94/duwYMTEx6mOtVsuff/5JxYoVSzqM0nfXJBAANwd90/6Qnw/j7WJDwyquJXJpKz8/0k+fJj0qqlgSQB8nHzRoSMpMIi49DlebkolbCGPs7e3Vwfmenp4kJCSo527c46LnwtSy2xsenT2AhXhclXgCWK9ePXVVc2Ndvba2tnz99dclHUbpy97qKPYYaLPA3AJ3h5xFVCf89h/rhz5TIpe28vMDeOClYLJZm1vjae9JdHI0FxIvSAIoTKpJkybs2rWLwMBAWrduzcmTJ/n888/5448/7mlmqDAhnRYUHWR3OEmvgRClrsQTwKioKBRFoWrVqhw4cMBg/0QrKysqVKhQNrpAzHK91UeWQsNeVHDMaZJ2sC65H4W1v34B1aSdO1EyMvRjAh9QjXI1iE6O5kjsEepXqP/A9QlRVDNnzlS3vfrggw+YM2cOa9asoUaNGvc8A1iYgE4L1/7T7wNsmT3ZTRJAIUpbiSeAVapUAZD1h3IngBf3QsNevNGkMvN26BdEzdSW3Pvj0PI5zMuVI+tqNCkREdg3a/bAdTbybMT2y9s5HHuYN3mzGKIUomiqVq2q3s9eBmPPnj1ldhzPQ0+boU/+ADJT9P9KC6AQpc5ky8CAfrPsbdu2ERsbmychLGwm2SPPLFcrp40LAJXK2TGne30GLztMSkbJ7Qls7mCPQ/PmJPz2Gwm/rSuWBLC6i37x6ku3Lz1wXUIUh9WrVzNp0iSDccfiIaAY+XIrCaAQpc5kCeD8+fMZOHAg7u7ueHp6Gswc1Wg0j38CmLvLwyZng+/spWBSM0suAQRw7tTxTgL4G2nH/qPKsmWYOzred30+jvoZfJduX0Kn6DDTyKBuUfK+++47Nm/ejJWVFcOGDVP3hX366ac5e/ZszoLD4uFx29jWbfJ5IURpM1kC+PHHH/PJJ58wZswYU13y4ZJ+O+e+dU7iZWup/xEkp5dsAmiXa6HW9NNnuDZ9Ol4ffXTfS7h4OXhhobEgXZvO9ZTreNjLAqSiZE2bNo0JEyZQp04dTpw4wW+//cZ7770HQMeOHRk2bBjlypUr5ShFHrk/+7JZPPg4ZCHEgzHZ17C4uLj7XqT1seCSe82rnKUX7a31XcOpGVmUJI2Z4Y86YdWvnAisRdatW/dVn4WZBV4O+i2PZEFoYQqLFi1i/vz5REREsGHDBlJTUzlwZ4vDESNGSPL3MMpvmVlrGa8pRGkzWQLYuXNnNm3aZKrLPXzK+YLrncHr2pydP2yt9AlgSqaWkl6T27pWYJ5jKfv2kXn16n3Vl90NfCCmePYZFqIgFy9eVJeSeuaZZ7C0tDS6/64oZdpMSL4OKbeMt/6ZW8kYQCEeAibrAvb392f8+PHs27ePoKAgLC0tDc4PHTrUVKGUHr8WcOucfh3AO+zvbBSvKJCWqVMTwpLgOX48if/7H24DBnA2pA1KejpXRrwHGg3lerxBasRBKs35Gkvvou1K4uvky56re5gXOY9g72DqVahXYrELkZ6ebrCav5WVlbT6PYwSrkBaHACaig1Ys2AGr7RpmXPerAws+1WIZ599lnr16jF79uzSDkWUYSZLAL///nscHBzYsWMHO3bsMDin0WjKRgJofmfcS+4WQMucD8PEtMwSTQDt6tfHrr5+zT7njq8S//Ny/QlFIW7JjwDEzp5Nxc8+K1J97aq1Y9mJZQCsPbOW2m61ydRlYqeu9SVE8Ro/fjx2dvr/XxkZGXz++eeAfj1AqzvrW8pagPfm+vXrTJgwgfXr13Pt2jXKlStH3bp1mTBhAsHBwQD4+vpy4cIFQL94f7Vq1Rg2bBj9+vXLW2GGkVa/3DTF9xk3adIk1q5dy5EjRx64Lo1Gw5o1awz2AC7O+oV42JgsAYyKijLVpR5e2buB6HL2PTYz01DDw5GT124TcT6OtnW8TBKK5V17MmfLvHylyHU84f4EbwS+wU/Hf+LX07/y6+lfAdjSeQsV7IzXL8T9at68OSdPnlQfN2vWTE1K/vnnH8zNzWVf6vvQqVMnMjIyWLx4MVWrVuXatWts2bKFmzdvGpSbMmUK/fv3JyUlhZUrV9K/f38qVqzIiy++eG8XlBZAIR4KMhfflLITQG2mweGnquq3UjtyKc5koVjms/9y6qFDZF4pehLYvWb3PMd2X9l933EJkZ/t27ezbds2g9vvv/8OwO+//862bdvYunVrKUf5aImPj+fvv/9m+vTptGzZkipVqtC4cWPGjh1L+/btDco6Ojri6elJ1apVGTNmDK6urmzevLnQa0Rfu8GLbwzGtlpTqjZtx6p1Gw3OX7p0iS5duuDi4oKrqysdOnTgfK5tK7dv307jxo2xt7fHxcWF4OBgLly4QFhYGJMnTyYyMlLdbjQsLMxoDOHh4bRu3Rp3d3ecnZ1p0aIFhw4dUs/7+voC8Oqrr6LRaPD19S2w/pkzZxIUFIS9vT0+Pj6888476u402Xbv3s2zzz6LnZ0d5cqVIyQkhLg445/x69evx9nZmaVLlxb6fgpRXEo8AaxVqxa3cs00feeddww2bI+NjVW7dB57ZsYTQD93/W4Gl26lmiwUm1q18j13/auvAMi8coXb27YVWI+3Q97xgruu7Hqw4IR4XCgKZCQbvZllpeZ77oFvRZxQ5uDggIODA2vXriU9Pb1Iz9HpdPz666/ExcWp3e4FGf/5XDq99DyRm5YT+upLdO03hOPHjwOQmZlJSEgIjo6O/P333+zevRsHBwfatGlDRkYGWVlZvPLKK7Ro0YJ//vmHvXv3MmDAADQaDa+//jrvvfcetWvXJjo6mujoaF5//XWjMdy+fZtevXqxa9cu9u3bR/Xq1XnppZe4fVvfXR0eHg7oZ5pHR0cTHh5eYP1mZmZ89dVX/PfffyxevJitW7cyevRo9XpHjhzh+eefp1atWuzdu5ddu3bRrl07tNq8y30tW7aMbt26sXTpUkJDQ4v0MxCiOJR4F/CJEyfIysqZ9PDTTz8xcuRI3N3dAVAUhbS0tJIO4+FgZAwgQGVXfQJ8KS7FZKFY+flhWaUyuttJlOvWDY21NeknT5K4fj0ZF/W7e0R17IQ2IYFK336L43MtjdZjbmZO80rN2Xl5J/Ur1Odw7GF2XN5Bpi4Ty+yEV4iyKjMFpub9kmQONAD4s4Su+8FVsLIvtJiFhQVhYWH079+fefPm0aBBA1q0aEHXrl2pU6eOQdkxY8Ywbtw40tPTycrKwtXV1fgYwLt0frkV/XqFQvkafPTMK2ze9w9ff/013377LStWrECn0/HDDz+o3feLFi3CxcWF7du306hRIxISEnj55ZepVk2/p3n24t+gT2AtLCzw9PQsMIbs2ePZvv/+e1xcXNixYwcvv/yyuke9i4uLQV351T98+HD1vq+vLx9//DFvv/023377LQCfffYZjRo1Uh8D1K5dO09c33zzDR9++CH/+9//aNGiRYGvQYjiZvIuYGNLnZSZcTvmd/JtneGafxXL2QJwNd50LYAac3P8Vq6k2h/rKT9kMO4D+uPWtw8Aqf/+S8xHH6NNSAAg6a5JO3f7vPnnTGg6ga+f+xozjRnp2nTi0+JL+iUIIYpBp06duHr1KuvWraNNmzZs376dBg0a5OlOHTVqFEeOHGHr1q089dRTzJo1C39//0Lrb9qwDuTaKahp06ZqC2BkZCRnzpzB0dFRbY10dXUlLS2Ns2fP4urqSu/evQkJCaFdu3Z8+eWXREcb21mkYNeuXaN///5Ur14dZ2dnnJycSEpK4uLF+1vD9K+//uL555+nYsWKODo60qNHD27evElKiv5LfHYLYEFWrVrFu+++y+bNmyX5E6XCpHsBl3n5tAA62+pbypLSs1AUxWQJsbmT4WKsVlWrorGzQ0lJIS7XWJTC1gm0s7Sjc4B+kW8Xaxdupd3iVtotytuVL/6ghXiUWNrpW+PuotVqiYyMpG7dupibl8CkiHuciW9jY0Pr1q1p3bo148ePp1+/fkycOJHevXurZdzd3fH398ff35+VK1cSFBREo0aNqFXAcBJVPltFJiUl0bBhQ6Nj37Jb5RYtWsTQoUP5888/WbFiBePGjWPz5s00adKkyK+vV69e3Lx5ky+//JIqVapgbW1N06ZNycjIKPzJdzl//jwvv/wyAwcO5JNPPsHV1ZVdu3bRt29fMjIysLOzw9bWttB66tevz6FDh1i4cCGNGjUqOw0h4qFR4i2A2YNn7z5WJqkJoOEYQAdrfR6eqVVIzzKycbqJmNnY4PRS3hl9yX//zfWvvkKXnFxoHeWs9euyxaWbbkKLEA8tjUbfFWvkprOwzffcA98e8DO2Vq1aJBfw++7j48Prr79epIW49x06mvPZB+zbt0/txm3QoAGnT5+mQoUKanKZfXN2ztkzvX79+owdO5Y9e/bwxBNPsGyZfvkpKysro+Pq7rZ7926GDh3KSy+9RO3atbG2tjYYiw5gaWmZpy5j9R88eBCdTseMGTNo0qQJAQEBXL3rS3KdOnXYsmVLgTFVq1aNbdu28dtvvzFkyJBCX4MQxa3EE0BFUXj++edp0KABDRo0IDU1lXbt2qmPW7duXdIhPDzM7jS43pUA2ltZqJ/Xt9NKdku4wtjeNe7H3FU/Q/nGt3O5MnoM2vh40gtY0sfVVl/+Vur9bTEnRGH+/PNPdu0ynGj09NNP071793xnWQrjbt68yXPPPcdPP/3EP//8Q1RUFCtXruSzzz6jQ4cOBT532LBh/O9//yMiIqLAcit/38zCZas4deoUEydO5MCBAwwePBiA0NBQ3N3d6dChA3///TdRUVFs376doUOHcvnyZaKiohg7dix79+7lwoULbNq0idOnT6sJpK+vL1FRURw5coQbN27kO5GlevXq/Pjjjxw/fpz9+/cTGhqap5XO19eXLVu2EBMTo/4/Mla/v78/mZmZfP3115w7d44ff/yRefPmGdQ1duxYwsPDeeedd/jnn384ceIEc+fOzZN0BgQEsG3bNn799VeDcYVCmEKJJ4ATJ06kU6dOdOjQgQ4dOjB+/Hg6d+6sPu7UqRMTJkwo6TAeDtnfgu8aH2dmpsHhzo4g/15NMHFQhhyCg9Hc+WB0fLEN5YcNU88lbdnC+a7dONf2ZZL+Nj7T19VGnwDeTLtp9LwQD2rUqFEkJiYC8N9//wHQunVroqKiGDFiRGmG9shxcHBQx/M1b96cJ554gvHjx9O/f3/mzJlT4HNr1arFCy+8UOjn9+T33mb56nXUqVOHJUuW8PPPP6vdxnZ2duzcuZPKlSvTsWNHAgMD6du3L2lpaTg5OWFnZ8eJEyfo1KkTAQEBDBgwgEGDBvHWW28B+vGLbdq0oWXLlpQvX56ff/7ZaAwLFiwgLi6OBg0a0KNHD4YOHUqFu9ZCnTFjBps3b8bHx4f6dxbMN1Z/3bp1mTlzJtOnT+eJJ55g6dKlfPrppwZ1BQQEsGnTJiIjI2ncuDFNmzblt99+w8Ii76irGjVqsHXrVn7++Wfee++9At9LIYqTRinpDWgfY4mJiTg7O5OQkICTUxE2Nz+8FH57R39/zAWwdVFPNf10C9EJ+tnQEeNa4e5gXQIRF40uNRWNtTUaMzO0SUlcCH2D9FwL8GYL2LcXcxcXg2MzI2ay6L9FhAaG8n7j900UsShLHBwc+Pfff/H19WXs2LFMmzaNhIQEzpw5w0svvURMTEypxJWWlkZUVBR+fn4GW9YZo9VqOXz4MPXr1y+ZMYClKfofUO7qlnUPKNKsZCGKU0G/k/f89/sxJAtBm5K1Y879W+cMTml1OXn4mVjDBUVNzczWFo2Z/r+GuYMDVX9bS/n38rasXP/q6zzHKjlWAuDS7UslG6Qos6ysrNTZltu3b1ePu7q6qi2DojQZaVMwL3y9QCGEacksYFMKaJNzP93wD1Xs7ZyxK5na0psIkh/X7t25vWkzaUePqscyzucdC+jj6APA+YTzpgpNlDFPP/00I0aMIDg4mIMHD6rHT506RaVKlUoxMgEYX4TaTP7UCPGwkRZAU7KwgkqN9ffTDBPAd1sFqPcTUg0niTwMzOzt8Vv5C4EnjuN8Z7P05D17ybprUHNN15oAXLx9kX3R+0wdpigD5syZg4WFBatWrWLmzJnq8Q0bNtCmTZsCnilKnKKQpwXQxvmBZyULIYqfJICmZnNnrEH6bYPDQ5/3x85KPxYoPuXhSwBzc+3ZQ70f89HHBufK2ZTD2lw/frH/pv5cT7lu0tjE469y5cr8/vvvREZG0rNnT/X4rFmz+OrONoaitNyV/Nm5Qzm/0glFCFEgSQBNzTo7ATRsAdRoNLSu5QHA7L9Omzqqe2IdGIjrnV1DknfvRskyXLpmVKNR6v3nVj7Hb2d+M2l84vF26NAhjuYaigDQvXt3Pvjgg/ta2FcUo9R4w8dm5tL6J8RDymQDM/L7Zq7RaLCxscHf35/mzZs/fjPi7pY9ESQt72B1K3N9Pn4jKd2kO4LcK41GQ4URI4hf8Qu6pCROPdWEaps3YXFnzcB21drx8f6clsFZB2fRwb/gNcWEKKq33nqL999/n6CgIKLurElpa2vLypUrSUlJYfbs2aUbYFkWf8HwsZVD6cQhhCiUyRLAWbNmcf36dVJSUihX7s5uEXFx2NnZ4eDgQGxsLFWrVmXbtm34+PiYKizTszHeAgjwRpMqrDx4GYDUTC12Vg/vwGmNuTnWNWuQGnEQXXIyN+bNw9LLG7tGjbALeoKvn/ua3878xl8X/+Jm2k0S0hNwtnYuvGIhCnHq1Cnq1asHwNq1awH9Om9Hjx6la9eukgCWFt1dk9fKVc35vBNCPHRM1gU8depUnnzySU6fPs3Nmze5efMmp06d4qmnnuLLL7/k4sWLeHp68u6775oqpNLhdGeW4s0zeU7VqeSMuZm+1S8xtXR3BCkKm4CciStxS34kdvp0znfuTMyUKbSo1IJZLWdRxakKAJHXI0srTPGYURQF3Z1kI/cyMD4+Pnl2WhDFKDM1b5KX2117nGMrX/iEeJiZLAEcN24cs2bNolq1auoxf39/vvjiC8aOHUulSpX47LPP2L17t6lCKh3e+hXmuXokzymNRqPuC3w77eGeCALgPmiQ0eNxy34m/ZR+HGP9CvrXezj2sMniEo+3Ro0a8fHHH/Pjjz8afF5ERUXh4eFRipE9xtIS4foJuFnA+OS7E0AhxEPNZAlgdHQ0WVl5W7WysrLUlfu9vb25fft2njKPFdc7M+KSYoyul+Voo08AbyU//B+mFm5u+C43vvXS7S1/AZIAiuI3e/ZsDh06xODBgxk5cqR6fNWqVTRr1qwUI3uMpdzZ2jEzJf8yuoe/16KowsLCcLlrl6O7TZo0SR2KUJK2b9+ORqMhPj6+yLGZQu/evXnlzpJgRXH36xClz2QJYMuWLXnrrbc4fDgnETh8+DADBw7kueeeA+Do0aP4+T3mSwbkXhFfm7eV70p8KgCDlj0aCZNtvXp4fDAWyyqVqfTtN3hOmgRAwqpfubV4MfXM9F3AR68fJSUzhQxtBrfSbpVixOJRV6dOHY4ePUpCQgLvv5+z3eDnn3/O4sWLSzGyR9P169cZOHAglStXxtraGk9PT0JCQgxaV33rPYOmYgM0FRtgZ2dHUFAQP/zwg2FFd2//Bvj6+qLRaPLcpk2bVtIv65Gyd+9ezM3Nadu2bbHUl/0+79tnuBZreno6bm5uaDQag+ETomwy2SyDBQsW0KNHDxo2bIilpSWgb/17/vnnWbBgAaDf43PGjBmmCql0WOTa41ebrl8cOpfsRsEbSek8Klx79sT1znpsmdHR+n+vXuXap9OwKF+emv28OKGLZtLeSeyP3s+ttFs4WTmx8/WdmJs95rO+RYmIj49n1apVHDt2TD127NgxPDw8qFixYilG9ujp1KkTGRkZLF68mKpVq3Lt2jW2bNnCzZs3c5XSMGXkQPqHvkqKUzVWrlxJ//79qVi+HC82rQUulSHhstH6p0yZQv/+/Q2OOTo6Gi1bVi1YsIAhQ4awYMECrl69ire39wPX6ePjw6JFi2jSpIl6bM2aNTg4OHDrlnwJFyZsAfT09GTz5s0cO3aMlStXsnLlSo4dO8amTZvUcTstW7bkhRdeMFVIpaOQFsA3mlRW78c9At3Ad7Pw9DR4nHX9Or2uVUejU/D7+nfa/0+/MHRiRiInbp0ojRDFI+6ff/6hevXqTJ8+na+/ztmPevXq1YwdO7YUI3v0xMfH8/fffzN9+nRatmxJlSpVaNy4MWPHjqV9+/YGZR0d7PCs4E5VPz/GjBmDq6srm9evgaw0uHHKsGJHr5y7jo54enoa3Ozt7YGcbsEtW7bQqFEj7OzsaNasGSdPnlSfHxkZScuWLXF0dMTJyYmGDRsSERGhnt+1axfPPPMMtra2+Pj4MHToUJKTk9Xzvr6+fPzxx/Ts2RMHBweqVKnCunXruH79Oh06dMDBwYE6deoY1Jlt7dq1VK9eHRsbG0JCQrh0qeA9zn/44QcCAwOxsbGhZs2afPvtt4X+DJKSklixYgUDBw6kbdu2hIWFFfqcoujVqxfLly8nNTVVPbZw4UJ69eqVp+zRo0d57rnnsLW1xc3NjQEDBpCUlLMnvVarZcSIEbi4uODm5sbo0aNR7hrCpNPp+PTTT/Hz88PW1pa6deuyatWqfOO7cOEC7dq1o1y5ctjb21O7dm3++OOPYnjloqhMvhB0zZo1ad++Pe3bt6dGjRqmvnzpMzMHzZ1Wr6y8rXwT29VWZwKfv5mc5/zDTqPRUK57d4Nj1S9kEnAFnj2q8FKEgnOy/oPjUOyhAuvae3Uv4THheT5oRNk2YsQI3nzzTU6fPo2NjY16/KWXXmLnzp2lGFleiqKQkpmS55aalUq6Lp3UrFSj5x/0VtTfGQcHBxwcHFi7di3p6QX0OuRaklSnzeLXX38lLi4OKyvLvGWtncDRM+/xAnz44YfMmDGDiIgILCws6NOnj3ouNDSUSpUqER4ezsGDB3n//ffVXqSzZ8/Spk0bOnXqxD///MOKFSvYtWsXgwcPNqh/1qxZBAcHc/jwYdq2bUuPHj3o2bMnb7zxBocOHaJatWr07NnT4H1LSUnhk08+YcmSJezevZv4+Hi6du2a72tYunQpEyZM4JNPPuH48eNMnTqV8ePHFzos4ZdffqFmzZrUqFGDN954g4ULFxbLZ17Dhg3x9fXl119/BeDixYvs3LmTHj16GJRLTk4mJCSEcuXKER4ezsqVK/nrr78M3sMZM2YQFhbGwoUL2bVrF7du3WLNmjUG9Xz66acsWbKEefPm8d9///Huu+/yxhtvsGPHDqPxDRo0iPT0dHbu3MnRo0eZPn06Dg6ybqQpmawLWKvVEhYWxpYtW4iNjVWXcci2devWYr/mlStXGDNmDBs2bCAlJQV/f38WLVpEo0aNAP2H88SJE5k/fz7x8fEEBwczd+5cqlevXuyxGLCw1g+m1ub9wLU0N6OWlxNHryQ8EhNBjPF4fww2tWth5uDIlWHDUPYfpn6dnJ+37zWFyKoaziecz7eOk7dO8tbmt1BQGFh3IO/Ue8cEkYtHQXh4ON99912e4xUrVlQnlD0sUrNSeWrZU/kXOF4y193ffT92lnaFlrOwsCAsLIz+/fszb948GjRoQIsWLejatSt16tQxKDtm6leM++xb0jMyycrKwtXVlX7dXin0GmPGjGHcuHEGxzZs2MAzzzyjPv7kk09o0aIFAO+//z5t27YlLS0NGxsbLl68yKhRo6hZU7/PeO7P508//ZTQ0FCGDx+unvvqq69o0aIFc+fOVb8gvPTSS7z11lsATJgwgblz5/Lkk0/SuXNnNcamTZty7do1PO/0YmRmZjJnzhyeekr/81u8eDGBgYEcOHCAxo0b53mdEydOZMaMGXTs2BEAPz8/jh07xnfffWe01S3bggULeOONNwBo06YNCQkJ7Nixg2effbbQ97Ywffr0YeHChbzxxhuEhYXx0ksvUb58eYMyy5YtIy0tjSVLlqgts3PmzKFdu3ZMnz4dDw8PZs+ezdixY9XXNm/ePDZu3KjWkZ6eztSpU1n9vw0EN2uKg40lVatWZdeuXXz33Xfqzza3ixcv0qlTJ4KCggCoWrXqA79ecW9M1gI4bNgwhg0bhlar5YknnqBu3boGt+IWFxdHcHAwlpaWbNiwgWPHjjFjxgx1EWqAzz77jK+++op58+axf/9+7O3tCQkJIS0trdjjMZDdDWykCxignL3+/KOaAGqsrHDp1AnH1q0wd3NDSUqi456cb7Rtbusn+qw7u87oN91MXSbfHvkW5c6+oitOrkCnFLD+mChTrK2tSUzMu5D6qVOn8vxxE4Xr1KkTV69eZd26dbRp04bt27fToEEDw65IBUa93ZMjm35m628/89RTTzFr5kz8/SrnrdDasBVn1KhRHDlyxOCW/SU8W+5k08tL330cGxsL6Ft8+/XrR6tWrZg2bRpnz55Vy0ZGRhIWFqa2ZDo4OBASEoJOp1N3ibm7/uwhR9mJR+5j2dcEfXL85JNPqo9r1qyJi4sLx4/nzdqTk5M5e/Ysffv2NYjl448/Noj3bidPnuTAgQN069ZNvebrr7+ujot/UG+88QZ79+7l3LlzhIWFGbSsZjt+/Dh169ZVkz+A4OBgdDodJ0+eJCEhgejoaDURzo4z98/wzJkzpKSk8Gq7l6jgVk59/UuWLMn39Q8dOpSPP/6Y4OBgJk6cyD///FMsr1kUnclaAJcvX84vv/zCSy+9ZJLrTZ8+XR0Emy33DGNFUZg9ezbjxo2jQwf9NmVLlizBw8ODtWvXFtjU/8CyE0AjXcAALrb67o1Rq/7htYaVHtot4QqjMTPDsmJFtAaDyaHhH2d5xsyMv4PSOBx7mAYeDdRzOkXHihMr2Hopp0X4VtotzsafpXq5Em6ZFY+E9u3bM2XKFH755Rf12KVLlxgzZgydOnUqxcjysrWwZX/3/XmO63Q6IiMjqVu3LmZmxf893NbC9p7K29jY0Lp1a1q3bs348ePp168fEydOpHfv3moZd1cX/P0q42/txMqVKwkKCqLR2gXUCrir5cbeMAl3d3fH39+/wOtnd+kC6udddi/RpEmT6N69O+vXr2fDhg1MnDiR5cuX8+qrr5KUlMRbb73F0KFD89RZuXJOcmqs/oKuea+yx8vNnz/fIFECCtzedMGCBWRlZRlM+lAUBWtra+bMmYOz84Mtpu3m5sbLL79M3759SUtL48UXXyyRpdayX/+csBVU8PSiplfODjDW1tZGn9OvXz9CQkJYv349mzZt4tNPP2XGjBkMGTKk2OMTxpmsBdDKyqrQD4HitG7dOho1akTnzp2pUKEC9evXZ/78+er5qKgoYmJiaNWqlXrM2dmZp556ir179xqtMz09ncTERIPbfcmeCWykCxjg1LWcX9CUjLxLKzxKXHv1NHhsfucD7Z31OjxuKZy4dYLEjER+PfUre67sYfCWwUwPnw5A1xpdedJT/w386I2jpg1cPLRmzJhBUlISFSpUUAe4169fH0dHRz755JNSjs6QRqPBztIuz83WwhZrM2tsLWyNnn/Q24N+aaxVq5Z+IoU2E+5ufddm4OPjw+tdOjP2068Nz1k5gKb4/6wEBATw7rvvsmnTJjp27Kh+sW/QoAHHjh3D398/z83KyqqQWguWlZVlMDHk5MmTxMfHExgYmKesh4cH3t7enDt3Lk8c+S1tlpWVxZIlS5gxY4ZB62hkZCTe3t78/LPxNVZz0ykKWdqCk9Y+ffqwfft2evbsaTQZDQwMJDIy0mDizO7duzEzM6NGjRo4Ozvj5eXF/v05X2SysrI4ePCg+rhWrVpYW1sTffUSlf2q4utXFb+q1fD39y9wa1cfHx/efvttVq9ezXvvvWfwN1qUPJO1AL733nt8+eWXzJkzxyQtWufOnWPu3LmMGDGCDz74gPDwcIYOHYqVlRW9evVSxwrdvXOAh4dHvuOIPv30UyZPnvzgwZnf+eaZTxdw61oenIjRJ4HJ6VnYWz+8ewIXxrltWxxbt0ZJScHM2Rm0Wi7260/Kvn0885/Chhob+PTAp0af+6RldXzPxxJurfDP9X/oWL2jiaMXDyNnZ2c2b97M7t272bdvHyNHjmTlypVqS74oups3b9K5c2f69OlDnTp1cHR0JCIigs8++4wO7V6Ga/+ChQ2Qa6iGNgMUhWGDB/FEvQZERB6jUd1a+nO6vF9Yb9++necz1c7ODienwvcJTk1NZdSoUbz22mv4+flx+fJlwsPD1ZbeMWPG0KRJEwYPHky/fv2wt7fn2LFjbN68mTlz5tz3+wL6FsIhQ4bw1VdfYWFhweDBg2nSpInR8X8AkydPZujQoTg7O9OmTRvS09OJiIggLi6OESNG5Cn/+++/ExcXR9++ffO09HXq1IkFCxbw9ttvFxjj6WtJpGdpqenphJWF8cS7TZs2XL9+Pd/3OzQ0lIkTJ9KrVy8mTZrE9evXGTJkCD169FD/Pg4bNoxp06ZRvXp1atasycyZMw0WdHZ0dGTIsHf5YvKHKDqFS082ITkpkWun/8HJycnoGMjhw4fz4osvEhAQQFxcHNu2bTOaXIuSY7LMYteuXWzbto0NGzZQu3Ztg+Z30C/hUJx0Oh2NGjVi6tSpgL6F4N9//2XevHkFDsgtyNixYw1+kRMTEwv8dpMv8zstgPl0Ab/zrD9fb9XvFXw7PYsK936Fh4qZlRVkfxu3sMDh2Rak7NtHlesKq64fUcvZpyok2wAaDc09g6ky9Ct8bt6k4Wtm/G7xOx82+ZDF/y0myD2Ip7wKGFgvyoTg4GCCgoIYOXIkLVu2LO1wHkkODg768XyzZnH27FkyMzPx8fGhf//+fDC0H2Te0i/zkpuiA0VLrcCavNCiCRO+mMsfP95pCTSyG8iECROYMGGCwbG33nqLefPmFRqfubk5N2/epGfPnly7dg13d3c6duyofhGvU6cOO3bs4MMPP+SZZ55BURSqVavG66+/fn9vSC52dnaMGTOG7t27c+XKFZ555pkCx+b169cPOzs7Pv/8c0aNGoW9vT1BQUHqBJW7LViwgFatWhnt5u3UqROfffZZoePi0rP0CffttEzcHIx3tWo0Gtzd3fOtw87Ojo0bNzJs2DCefPJJ7Ozs6NSpEzNnzlTLvPfee0RHR9OrVy/MzMzo06cPr776KgkJCWqZcRMnY2bnzIJvZnH54nkcnZx5smFDPvzwA6PX1Wq1DBo0iMuXL+Pk5ESbNm2YNWtWga9XFC+NYqI1Nt58880Cz+ceq1ccqlSpQuvWrQ1Wq587dy4ff/wxV65c4dy5c1SrVo3Dhw8bbOfTokUL6tWrx5dfflnoNRITE3F2diYhIaFI32ZV3zWH6EgIXQXVWxstEjxtK1fiU1nzTjPqVnLBzOzRHAdoTPLevVx8sw+Z5vBuf3MyLOGNrTqa/6ewPUjDob5N+bLCYC7cWU5ma10N814y5+mKT7Pryi4ADvU4hKWZkSUoxGNv6NCh+Pv7M3ToUIPfwSVLlnDmzBlmz55dKnGlpaURFRWFn5+fwfI0xmi1Wg4fPkz9+vULHCNWqpJiIfGK/r61E6TnGvLiXgNQ9Ov/mVmCLrs3QwPe9UwcaNn1z+V4ACq62OabAJpKQmomF+5auizQywlLc5OvNqcq6Hfyvv9+P0ZM1gJY3AleYYKDgw0WEwX9LMEqVfRbk/n5+eHp6cmWLVvUBDAxMZH9+/czcODAkg2ukEkgAPbW+j8Kr367hwAPB9YPfaZUf5GKk+2dGXmWWpgzz7DL6NmjCs2/ukRCo7XqseYnzFn6rKImfwCHrx2msZfxrhjxePv1119Zt25dnuPNmjVj2v/ZO+/wKKq2D99bs+m9AYHQQ+9dEBCpKlIEEUWxC6jIawFRUCxgr4iKgvoJilgQQUB67733kEB6SC9b5/tjki3JpgDpOfd17ZWZM2dmzk52Z3/znKfMm1dpArDmYWcb0Bfwd86IA0NeomAHnz+Rs7MyuJaagwQEVJIINJoshcQfwIWETMJ8XfHQyQ/rkiRxNSUHg9lCfV83NEVMWwsqhhp79V944QX27NnDu+++y4ULF1i6dCnffvstkydPBmSz+NSpU3n77bdZuXIlx48fZ8KECdSpU+eGClzfFPlTwOai07y4amxWgXPxmfx2oPgM9NUJpbs73iNGFL39UjSpdhGear2Jtpcdf1iO2E0dC2oXycnJTqfNvLy8SEpKqoQR1VCcaTllns1An2ar/atU2ip/2FUAEZQvBSfvYlJziuhZ/sSkOT+30WzhUpJNGOpNFlKyDWTpTWTonfvACyqOcrUAduzYkY0bN+Lr60uHDh2KDf44dKj4qhA3SpcuXfjrr7+YMWMGc+bMoWHDhnz66aeMHz/e2ufll18mKyuLJ598ktTUVG677TbWrl1b4vTNLZNf/7cYAXj0aprD+sy/TjC+W4PyHFWFEvLGbMxpaWSWkADcc9AgMtato02kxM5Wtvalp5eSa8rlsTaP4a5xL/oAghpHkyZNWLt2baFqD2vWrBHJZMuUAgpQqQb/ppBYIA+eQgkeweDqY3u4FZQ7zvS5RZJQVkLaMKO5dJZfo13E8k1m3BGUIeUqAIcPH27NAVTuVjUn3HXXXdx1111FblcoFMyZM4c5c+ZU4KgoMQikNqB0cSHsq/lk7tiJytsLTd26nO/Zy6GPx4A78Bk9mox16+h6VcfX2J4Yk3OTWXh8IZfTLvNJP+E4XJuYNm0aU6ZMITEx0Zpz7Z133uHLL78U07/lidbT9vDqgBIUirxoYUFF4cx9P8dgxt1FTXqOEbVKgZv21n7iM3KNKBQKPPIyUWTrTWTqTXi4qHG7gewU17P0+LppMZjsBKAo8VnplKsAnD17ttPlWo82z2JlKLrW7x/P9GTUgl0ObSazBXUN8QPMx+M2m+ir+8nHXHvBFmXt2a8/bp06glKJx/UcDg3eynl1EmNX2SL8NkRt4P5V9yMhMa3TNBEdXAt49NFH0ev1vPPOO7z11lsALFu2jAULFjBhwoQS9haUmoI/0GqNbO1zCPoA4fdXOVicXPbEDD0alcJaR75NXe+bTrtmtli4nDd927qONwoFXE7KwixJqBQKWtbxKvWxr6bkoFIqMdhbAMXHptKpMDURHR3N1atXrev79u1j6tSpfPvttxU1hKqDi6f8V190RvZODXx5cWAz2taz+Tpl6gunWKhJeA0ZQpNNGwmdOxevoUPwGjYUpZsbLo3lab2sXbsIshQuFn4y+SQXY0/y+7nfK3rIgkrimWee4erVq1y4IKdLOnbsmBB/ZU6BX+j84DVlgahlRRWNYq7hOLMAWiTJYTrWfAsqy35fo9mC2SJhzjunWZJu+NhZehNGk20fYQGsfCpMAD7wwANs3rwZwFqBY9++fcycObPip2ArG6sALL6SyJT+TVk55TZrQEhGbs0WgACaOnXwGXEvdT/+GGWeL6ZrXs3J2BkzSOw9iC56uWzS0+3kJKldzln4v4/MNPm/Hfx88v+IyYzBUIx/paB6c/nyZc6fPw/gkN/s/PnzREZGVtKoaiAFv0P5risFK30UFISCCsGZfjKYLOjtplmNJVQJKQ77XQ1mx+MWPLaplA59uSZb1gchACufChOAJ06csGZQ/+2332jTpg27du1iyZIljkXHawMueTmHirEA2uOpk2fq03NrZ9SUz0jHCiAvfRzF8qHLmNRuEt/e+S33nJatgn12pOL/zLvc/8NAOv3ciec2Pce6yHWVMWRBOfLII4+wa9euQu179+51qF0ruAX0GZCT4tiWX8GooABUiXyclYEzA5zBbOFqSrbd+i1YAO0EWo7RzMXEzELnAkjK1Dv49hVFpt5ErtEmAIX+q3wqTAAajUZrQMiGDRu45557AIiIiCA2NraihlE1KMUUsD35AvCfo7XsOuXh2qYN4ct+dWirezwehUJBjzo9aJ3jZ20PT4AHtsg3o83Rm3lp60ucSDpRoeMVlC+HDx+mV69ehdq7d+/OkSNHKn5ANRFn/sn5QR4F/b7cq3utouqJVArfS/MthNraT/FmOpl9MuWJS2fbnGEv/kBYAKsCFSYAW7Vqxddff8327dtZv349gwcPBiAmJgZ/f/+KGkbVIF8AnvyzyHrA9uR/Tb7eepFrlZjrqTJxbdeOxhvWW9evTprEhTsGED15CsbLkQ59W12x3VgkJH45U3JRdUH1QaFQkJFR+OEpLS0Ns7lwLVrBTSAVEA5aD5vws7cA+jUCVfWtVV4UP/zwAz4+PsX2eeONNxyqSFU0pXHBu5VAC4vdzvnizVOnwddN9gXNtxDmWwIbBrjj4+osStzGtego2oX5cubk8VKPrW/fvkWW0xPcGhUmAN977z2++eYb+vbty7hx42jXrh0AK1euLLK4dq0ganeJXe7vYqs3fDaueL/Bmoy2Xj3qL15kXTdeu0bmxo3Wdbd/fgYgIAO2NvqCj/vKtSwPxB3gVPIpp07TgupHnz59mDt3roPYM5vNzJ07l9tuu60SR1Y9SUxM5JlnnqF+/fq4uLgQEhLCoBEPsHP/EWuf8I79UCgUKBQK3Oq1os0dY/hu6V+Fp4MLEB4ebt3P/jVv3rxyflfVg0ceecThuvj7+zN48OASawCDnKKlJCySVEiomi0WopKzSc9x3P+NN95AoVDQ/86BXL2eTbTdVPJ3X31GuzBfHrh3CKq8sqT5AjHfF1CjUnIjSSoyco1kG2q+X3tVpsIEYN++fUlKSiIpKYlFi2w/4k8++WSpioLXKMLsUpVYSrZYPHabLbntpcSiU8fUBlwiIpy2h7w1h/pNOuLStAkA8U88Q0SiC0gSMVkxjF01lu3XtlfkUAXlxHvvvcemTZto3rw5kyZNAqBTp05s27aNDz74oJJHV/0YNWoUhw8f5scff+TcuXOsXLmSvr26kZxil4xeoWLOnDnExsZyYtd6Hhw5lCdeeos169YXfeA88vezfz377LPl+I6qF4MHD7Zel40bN6JWq4vNX5tPtt75b0e+hQ6cJ1uOT9eTmmOwpoqxJzQ0lB3btnL64mWH9hXLlhBatx5KBda69GaLHPyRP1UsC8AbSzmTlCmC9SqTChOAOTk56PV6fH19Abhy5QqffvopZ8+eJSiolvmQBDQB90B52ZRbYneVUsFzdzQFKOSIW9tQ+/o6iECvoUNotHoVvvfdh0KhwPehh6zbMh98iiUfmBm7Vb5RLj+3vMLHKyh7WrZsybFjxxgzZgyJiYkAjBs3jjNnztC6detKHl31IjU1le3bt/Pee+/Rr18/GjRoQNeuXZnxwiTuGXi7XOEjoDkoFHh6ehISEkKjhg15ZfIj+Pl4s37j5hLPkb+f/cvdXc6FumXLFhQKBRs3bqRz5864ubnRs2dPhzruR48epV+/fnh6euLl5UWnTp04cOCAdfuOHTvo3bs3rq6uhIWF8dxzz5GVZRM34eHhvP3220yYMAEPDw8aNGjAypUrSUxMZPjw4Xh4eNC2bVuHY+azYsUKmjZtik6nY9CgQURHF1+S87vvvqNFixbodDoiIiL46quvSrw++VbXkJAQ2rdvz/Tp04mOjrZ+tkFOozZmzBh8fHzw8/Nj+PDhRF6RRVoDPzeunTrIYyPupEfzurRpVIeJIwcTczWKX5f8xJtvvsnRo0etVsaf/+/HIscSFBREjz79WLnc5jZz5MBeUq8n07v/QABUea4ARrOZN9+cw51dWtG5cTCdOnZgy0bHB4Ljhw8y4a5+dGkSwrih/ThzQrZs+rrJgUMGk4UTJ04wZMgQPDw8CA4O5qGHHiq2pONXX31l/Z8EBwczevToEq+xwDkVJgCHDx/OTz/9BMg3nW7duvHRRx9x7733smDBgooaRtUhoJn811g6n77GgfIN82JC7bYAAoR9vYCAyZNptncPdT/+GJfGja3bPPv3d+irMUP/oxJIEluitzBq5SjS9GkIqjd16tTh3XffZflyWdS/8sor+Pn5lbBXxSNJEpbsbKcvcnOL3Harr9K6O3h4eODh4cGKFSvQ6+0qE+XX+VVpQOvmsI9Fgj9WbyQlLR2ttmxKv82cOZOPPvqIAwcOoFarefTRR63bxo8fT7169di/fz8HDx5k+vTpaDSygLh48SKDBw9m1KhRHDt2jGXLlrFjx45CZQI/+eQTevXqxeHDhxk2bBgPPfQQEyZM4MEHH+TQoUM0btyYCRMmOFy37Oxs3nnnHX766Sd27txJamoq999/f5HvYcmSJcyaNYt33nmH06dP8+677/L666/z449FC66CZGZm8vPPP9OkSRP8/f0xmS0kpGYxcNAgPD092b59O1u2bUft4srj40ZhNBjQKGHcmFH07duXY8eOsXv3bh5+9HEUCgV33TuK//3vf7Rq1cpqZew7eLj1fM5y+d075kEHAbhi2RKGjrgPjVaLQqGwWvkWfvUln33yCdNem8M/m3czaNAgJtw/miuXL8rXLyuTZyfeT6tWLflj3VaenvYKH7/9OgAeeYGNydev079/fzp06MCBAwdYu3Yt8fHxjBkzxun1OXDgAM899xxz5szh7NmzrF27lj59+pT6+goKIFUQ/v7+0okTJyRJkqSFCxdKbdu2lcxms/Tbb79JERERFTWMMiUtLU0CpLS0tBvf+acRkjTbS5IOLylV9+NXU6UGr6ySOsz578bPVcsw6/VS2pq1Usoff0qnWrWWTjWPkB6e2VJq/UNr62tb9LbKHqbgJtm6dav19e+//0qA9O+//1rbboYvv/xSatCggeTi4iJ17dpV2rt3b6n2++WXXyRAGj58uJSTkyOdOnVKysnJsW43Z2VJp5pHVPjLnJVV6vf++++/S76+vpJOp5N69uwpzZgxQzq69R9JunZIkrKSJEmSpAYNGkharVZyd3eX1Gq1BEh+Pt7S+dMnij22/X72r23b5O/f5s2bJUDasGGDdZ/Vq1dLgPU6enp6Sj/88IPT4z/22GPSk08+6dC2fft2SalUWvdv0KCB9OCDD1q3x8bGSoD0+uuvW9t2794tAVJsbKwkSZK0ePFiCZD27Nlj7XP69GkJsH42Zs+eLbVr1866vXHjxtLSpUsdxvLWW29JPXr0KPL6PPzww5JKpbJeF0AKDQ2VDh48KEmSJEUmZUrvfPa11KhJU8liscjjiEmTDlyMl3Q6V2nBz39ISUlJEiBt2bLFetyE9BzpaHSKFJWc5TBOg8ksHY1Osb6uJNk+J7Nnz5batmsnHbiUIPkFBEqLlq+Sdp+9Krl7eErL/9sujX/saem23n2k1Cy9dDQ6RQoMDpWeffk16Wh0ihSZlClJkiR17NRZGjvhMelodIr0+rxPJB9fPyknJ0c6fjVVOhqdIs189yMJkPYdOCgdjU6RJr84Uxo4cKDDNYmOjpYA6ezZs5IkSdLtt98uPf/885IkSdIff/wheXl5Senp6UVeU3ucfSfzuaXf7xpChVkAs7Oz8fSUo1//++8/Ro4ciVKppHv37ly5cqWihlF10LjKf9dOh+zrJXZvHOiBQgHXswxczxJ+E8Wh1GrxGjwIn5Ej8Bk1CoAn1lpQ22Wh//Fk6Z/KBVWLvn37Wl/Dhg0D5Lrf/fr1o1+/fjd8vGXLljFt2jRmz57NoUOHaNeuHYMGDSIhIaHY/SIjI3nxxRfp3bv3Tb2PqsKoUaOIiYlh5cqVDB48mC1bttCx/738sGylQ5DHSy+9xJEjR9j0719069CaT974H02aNC3x+Pn72b865yV3z6dt27bW5dDQUADr9Z82bRqPP/44AwYMYN68eVy8eNHa9+jRo/zwww9WS6aHhweDBg3CYrFw+fJlp8cPDg4GoE2bNoXa7P/narWaLl26WNcjIiLw8fHh9OnThd5jVlYWFy9e5LHHHnMYy9tvv+0wXmf069fPel327dvHoEGDGDJkCFeuXCEj18S5Uye4cvkSnp6eeHh40LFJHXq3aYRen0tG4jX8/f155JFHGDRoEHfffTefffYZ8fFxQOFUK6YCiaFTcwr8lkig0WgYNmIMK35byvpVf9OwcROatbC5VqiUCjIz0kmMj6V95+6A7P8H0KNnTy5dOAdA1KXztG3bFp1OZx1Hu07y9cyfRj53+gSbN292uGYReS4+zq7bnXfeSYMGDWjUqBEPPfQQS5YsITs7u1A/QemosPj9Jk2asGLFCkaMGMG6det44YUXAPkL5+XlVVHDqDrk59TKTYPN78Cwj4rt7qpVUc/XlejrOfy0O5JeTQL4+8g1xnWtT6s63sXuW5sJfnUGmVu24BcfT98r7mxoLN8s9sbt5cF/H+STvp8Q6BZYyaMU3AgpKbYExenp6dSvX58//viDefPm8c4779zw8T7++GOeeOIJJk6cCMDXX3/N6tWrWbRoEdOnT3e6j9lsZvz48bz55pts376d1NRUp/0Urq40P3TQ6f5Hjx6lXbt2qFRlX0lD4ep6Q/11Oh133nknd955J6+//jqPjx/N7I++5pFnplr7BAQE0KRJE5qEBbP8m/dpM2AMnfvfQ8s27Yo9dv5+xZE/pQtY68ta8iIY3njjDR544AFWr17NmjVrmD17Nr/++isjRowgMzOTp556iueee67QMevXr1/s8Ys7542SmSn7Zi9cuJBu3RzrkZf0/3V3d3e4Pt999x3e3t4sXLiQ+595ieysLFq0ac+fv8nTsmfjbCmQereV3V8WL17Mc889x9q1a1m2bBkzX3uNr5f8Sa+ePa19JUlymnolX5xJdmLx3rHjefCeO4k8f4YnH3/M2q6wCwKxR6uWBaDSLkekh4u6yKCQ/G7ZWZncddddvP/++4X65D8I2OPp6cmhQ4fYsmUL//33H7NmzeKNN95g//79JabtERSmwiyAs2bN4sUXXyQ8PJyuXbvSo0cPQLYGdujQoaKGUXXQ6GzLsSWH/AM0CZQrXny64Tz3fb2bn/dEMezzHbdU7qemo3RxwWvoUACmmfuzfrTNSflo4lEWHK2F/qfVHG9vb4cXQP/+/Xnvvfd4+eWXb+hYBoOBgwcPMmDAAGubUqlkwIAB7N5ddIqmOXPmEBQUxGOPPVZkH5CFhdLNDXQ6JBcXhxc6HUo3t3J5KRQ3Fo3pgNlIyyYNyMrOdZ7mRakhrG4IY+8eyIyZr9/8eW6AZs2a8cILL1hnjxYvXgxAx44dOXXqlCxMC7y02uJz0pWEyWRyCAw5e/YsqamptGjRolDf4OBg6tSpw6VLlwqNo2HDhjd0XoVCgVKpJCcnBwXQok07oi5fJFPpToOGjahv98r//AN06NCBGTNmsGvXLlq2bMW/K34nI9dISq6FrFwDx6+lOU0dcyomnVMx6SRk6NHnlWpr0rwFjZtFcPbMKR544AGH/iqFAg9PLwKDQzlyYA8A2jwL4J7du2jUtDkAzZpHcOzYMXJzbYGOxw4dsL5HhUJBi9btOHnqFOHh4YWuW36gUEHUajUDBgzg/fff59ixY0RGRrJp06YbusYCmQoTgKNHjyYqKooDBw6wbp2tPNcdd9zBJ598UlHDqDrYVwHxDCnVLo3zBGBBlu6NKosR1Vjce8jTFDmbthKk9KaeRz3rtpPJJ8k2ZrPt6jZySxGRLai6BAcHO0SPloakpCTMZrN1CtD+WHFxcU732bFjB99//z0LFy4s9Xni4uI4fPiw9VWaPG8VQXJyMv379+fnn3/m2LFjXL58meW/LuX9BT8yfNDtoHZiSVS7gMaN559+jH9WrXIaPWtPRkYGcXFxDq/09NLlM83JyWHKlCls2bKFK1eusHPnTvbv328VYa+88gq7du1iypQpHDlyhPPnz/P3338XCgK5GTQaDc8++yx79+7l4MGDPPLII3Tv3r3IvLVvvvkmc+fO5fPPP+fcuXMcP36cxYsX8/HHHxd7Hr1eb70up0+f5tlnnyUzM5O7774bFDB0xH34+Pnz5ENj2bB5K1ejrrB/9w4+f/tVrl69yuXLl5kxYwa7d+/mypUr/Pfff1y8eIFGTeRAw9C6YVyLjuLMyeOcvRKDwT7YB9kC6Kwqx8Jlf3PozCX8fH3wcFGjVipQ2gWBPPL0syxe8BnrV/1F9OULTJ8+nSNHjvDMpClo1UoeeehBFAoFTzzxBNnxV9i+6T9++uZL6/GVwNiHHyfl+nXGjRvH/v37uXjxIuvWrWPixIlOk7qvWrWKzz//nCNHjnDlyhV++uknLBYLzZs3L/YaC5xToSncQ0JCyMzMZP369fTp0wdXV1e6dOlya0+r1RV7v7/c0kWlhnjbrIYDWgSz4XQ8AD/uiuThnuFlOboahXvPnmjq1sV47Rrnuvdg2Qfvs7Z1Km/teYtTyad4aM1DnEs5h5/Oj433bUStrHmVDWoS9uIpf+ptw4YNfP755+VemSEjI4OHHnqIhQsXEhAQUOr9QkJCHESm2WyuEiLQw8ODbt268cknn3Dx4kWMRiNh9eryxAMjeHXas86rfCgUENCMlgHNGDhwILNmzeLff/8t8hyzZs1i1qxZDm1PPfVUqfK/qlQqkpOTmTBhAvHx8QQEBDBy5EjefPNNQPbt27p1KzNnzqR3795IkkTjxo0ZO3bsjV0IJ7i5ufHKK6/wwAMPcO3aNXr37s33339fZP/HH38cNzc3PvjgA1566SXc3d1p06ZNiVUs1q5da53u9PT0JCIiguXLl9O3b19OxaTh6urG4t9X8+ncN3ho3FgyMzMICanDoIED8PLyIicnhzNnzvDjjz+SnJxMaGgoTz/9DKMflF0aBgy9h41rV/H42LvJSEtjzkfzGffghBKTMLu5uePlLkf+Ngxwx9tVnjLPnwJ+4NGnkAzZfPL268xISKBly5asXLmS27u2Q5IkFAoF//zzD08//TS39+xKw6bNef7VN/jfkxMA2QoYFBLKpi3bmPXaqwwcOBC9Xk+DBg0YPHgwSmVh+5SPjw9//vknb7zxBrm5uTRt2pRffvmFVq1aFfteBM5RSFLFlEdITk5mzJgxbN68GYVCwfnz52nUqBGPPvoovr6+fPRR8T5wVZH09HS8vb1JS0u7cT/GBbdB/HF5ObAFTN5T4i5pOUae++Uwd7UN5b7OYUQmZdH3wy24aVWcmjP4Jt5B7eH6T/9H/LvvAqB0d6fJ9m3cvXY00RmOeb2mdZrGxNYTyTBkMG/fPDw0HkzuMBkvbS30U62iKJVKFAqF1Wcp/8eme/fuLFq0yOpEXhoMBgNubm78/vvv3Hvvvdb2hx9+mNTUVP7++2+H/keOHKFDhw4Ofl35fmMNGzbkr7/+olmzZuh0OorDbDZz+PDhQseqEmQnQ2qUXLLSv3jfPUH5YLFIxKXnkpTpaK3zdtWQlmPE101LmJ9bEXvLVr0T14o2LHjpNKSXopJIUec5djUVgDo+rgR4lC4VUP4++cc8HZuO0WzB3UVNwwB3jGYLSRl6AjxdcFGXzXciNzeXy5cv07Bhw0LfyVv6/a4hVNgU8AsvvIBGoyEqKgo3N9sHauzYsaxdu7aihlF1MNtFXyWdc5wSLgJvVw0/PtqV+zrLpeF83WUfl2yDuVChbYEjnnfafLwsWVmk/f47k9tPLtTv44Mf88DqB+j5S09WXlzJ0jNLeXHLixU5VEEJXL58mUuXLnH58mWrFS0uLo5du3bdkPgD0Gq1dOrUiY12JQUtFgsbN260+inbExERwfHjxx0iWu+55x769evHX3/9hVpdA6zH+XWASyjzJig/EjL1hcQfQK4xr+yauvj/jbKEWTWlQlFiHygcRVwQd23pP+/5U8eeeTkA88+fpTeRlmMkMimb5CwDUckiqreiqLC71X///ce6deuoV6+eQ3vTpk1rZxqYuz+DX8bK07+SGaL2QNM7b+gQXjo5yspskUjJNhDqfWORf7UJTWgooe++y/UffkB/7hzXlyyhT73p7B+1g/2pxwj3CmfSxklEpkdyPOm4w767Y3eTlJNEgGvpp/wE5UeDBg2sy/m+ZCVZ3Ipj2rRpPPzww3Tu3JmuXbvy6aefkpWVZY0KnjBhAnXr1mXu3LnodLpC1Ubyow+bNWvmkHqk2mIVgFXMMlmL0BfxQG/IC/hTl0K8hfm5EX3duZhSKmQBVpLAK2pz82BPjBYJV23pPyPNgj3JNZrxcJFlh/07MFskawBKjjBmVBgV9oiXlZXlYPnL5/r167i4lE02+WpFgx7wyhVo/6C8fnnbDR9CoVBY6z6mZJVszq/t+IwcQd1P5YAj45Uork6aRPKsN+ldrzdhXmF80f+LIvft91s/TBZRuLwy2b17N6tWrXJo++UXOTVG48aNefLJJx2rWZSSsWPH8uGHHzJr1izat2/PkSNHWLt2rdVnLyoqitjY2Ft/A1UZkwGMeUFQVgFYC32zqzj5bg/OUrEUxFVTtDhTKhWUpmxvUQLRRaOyCrnSolEp8dRprD7/Zrtjm+zy05TGMikoGypMAPbu3dtaCg5k8WKxWHj//fdvKnlrjUChgHqd5OWkczd1CD932TE3OevGf/hqI9rwcBR2Dxzp/65ByvPhCvcO5+ehP1u3LRm6BHeNLRXB0+ufrriBCgoxZ84cTp48aV0/fvy4NdrzhRde4J9//mHu3Lk3dewpU6Zw5coV9Ho9e/fudcjltmXLFn744Yci9/3hhx9YsWLFTZ23SiBJkHASEk+D2SQsgNWAovLrlbaPQmGzJhZEZycc89O7lAcms030JaTbMjCURtwKyoYKE4Dvv/8+3377LUOGDMFgMPDyyy/TunVrtm3bxnvvvVdRw6h6uPrKf0vhA+iMuj7ytG+k8JsoFQqlEr9HHnFoS/52IdGTJnP12WcJ/mYV33T5kD/v+ZO2gW15p5ctsfDeuL20+bENkzZMQm8WgruiOXLkCHfccYd1/ddff7VWlJgyZQqff/45v/32W2UNr/oi2U25mQ3CAlgNUJXif6O2E1IFrWpF7e+uVdPAz41GAe74uGodMk+UNRLOrYuleW+CsqHCBGDr1q05d+4ct912G8OHDycrK4uRI0dy+PBhGjduXFHDqHq4yOXx0JcuL1ZBmoXI+++5mFxWI6rxBE6ZTPgfv+M9YgQAiZ9+SuamTWSs30DKzz/jO3IqoSfkHHB3NLiDRYMWOey//dp2xv4zll/O/ILBLMryVRQpKSkOqVS2bt3qkMC5S5cuREdHO9u1QrnZahKVhqnAZ9gigkCqOqUxzNmnV9OoFCjsvO6Ksg42CHCTp3d1Gur7u6EuRwtg0ZRdYpJq912sYCo0ZM3b25uZM2dW5CmrPi554ec3aQHsGu7HN1sv8d+pOFKzDfi43Vr2+9qAQqPBtVUrtK/OIO2vv5z2iX7iSQAa/v03XZp3Yd2odUzbMo2TyfIU5MW0i7y79122RG/h6wFf185clhVMcHAwly9fJiwsDIPBwKFDhxwqf2RkZDiU96potFotSqWSmJgYAgMD0Wq1RX4u8pPc5ubmVn4amJSrkF8nOzcH9EZ53WCGXJEcvTIwGvRIpqJ9jg16PYpS+CRLeeLeggqVRcKYJ4jMRqV1W8HjmiroXubs/ABGi9KheshNHVuSMBgMJCYmolQqb7kqTE2lwgTgtm3FBzn06dOngkZSxci3AObenAWwf0QQWpUSg9lCQoZeCMAbQOXpSeALL5CYV4lGFRiAOTHJoc/l4cNptHoVdRo35te7fiU6I5qhfw61bt8Vs4t5++Yxo9uMCh17bWTo0KFMnz6d9957jxUrVuDm5kZPu1qnx44dq9TZBKVSScOGDYmNjSUmJqbYvhaLhaSkJCIjI50mvK1Q0mPBkhdElqoAQyYYs8HVDC43d18S3BqJGXr0psLWK61KgUUCTbZLqR469blG0nNMBHhqSc8xWY9pSddiMFvIyDHhqlWRbZAfSLQ5FZdJQp8rp38piEIBqqyyGYebmxv169ev/O9YFaXCBGDfvn0Ltdl/gJ2VfakVWKeAM2Rn7Bt8+lIoFNTx0RGZnE26ky+ToHj8H50IkoRb1664dmgPQNzsN0i18yVL/nYhdd6bB0CYZxgvdHqBTw7ayhcuPbOUOxvcSeeQzhU69trGW2+9xciRI7n99tvx8PDgxx9/dHiyX7RoEQMHDqzEEcpWwPr162MymYq9p2VmZjJs2DAOHDiAh4fzEo8VgiTB1w+BOc/iMuQDuLwCruyE/rOg4T2VN7ZazHs/H+B8fGah9nVT+2CRJLQ3kCg512BGp1Ux489j7LssV6D67P72tKvrQ67BzL8nYpm/+QIAG//Xt0zGX1omLt5HlJNUNauf7Y3uBlLMOEOlUqFWq8XsTDFUmABMSUlxWDcajRw+fJjXX3+dd955p4i9agH5U8AWI5j0oLlxp1uvvBI9zp6mBMWj0GgIePoph7bQOW8i6fWk5VWBSPv7bzwH3olHv34gSTza+lFa+rckITuBObvnoDfr2XthC+5L/iX34GHCRz+I78hRlfF2ajQBAQFs27aNtLQ0PDw8UKlUDjVlly9fXrliKg+FQoFGoyl2OtpgMHDlyhW0Wu0t5TC8ZTITIe28bd2UDjlxkBkt/zpU5thqGSuPxmC2WBjRoR5RaSauZRR+gPBwL7r6R1Hk/wvNCo31mJ7u7uh0OnQ6yLGorO0V/VlM0eP0fepR4SM+e+VOhQlAb2/vQm133nknWq2WadOmcfDgwYoaStVCa/eDpU+/KQGYX6OxNKV9BKWjznvzCJ03l7MdOiLl5nJ18hTUoaEoXV1p+NefdA/tDkCmIZO5++aiX7AIDkrogLhDr2GMvIJbl6549L6tct9IDcTZvQTAz8+vgkdSA0i/5rhuzJZTwQCImtgVRq7RzHO/HAagX/Mgcg2FRVGrOrdWrszdLm9f/m8GQMvQyiuDNqBFMGfiCvu/Z+QaCfYSArC8qfRveHBwMGfPnq3sYVQeSiVoPcGQIU8DewTd8CG8dHkCMEckKi5LFAoFru3akb13LwCmvGTACR9+RMjMV5FMJjr8cZJhsRY6XXCMXEv+diHJ3y6k0T8rcWna1Nqec+QImdu2E/DM0ygqMWBBIAAgN9Vx3Zht8wdUic9nRaE32vz9MvUmsgtUw3h/dFv6R9z4b4M99uVCAz1tuVB7NPZn/gMdaRJU8dbzKf2b0MDfjWyDGVetis83nudqSg5p4resQqgwAZhfszMfSZKIjY1l3rx5tG/fvqKGUTVxyReAN+dw7eUq/xsPXEnh4Z7hZTgwQeicN7k4aLBDW9rKlbg0bkTcG28C8LDdtgv3dkC96zDhCfL6pbvvQdeqFe633Ubmpo3oz8u+NurgYHzHjqmItyAQOEeSIOawY1tWIqTmpdJRCgFYUdjP3lxJziangAVwTF7991s6h52LUME0MMPaht7y8W8GnUZlrW0P8OOuSK6m5IjZrAqiwgRg+/btUSgU1lI2+XTv3p1FixYVsVctQecFGTE3nwqmoR+/7Itm3Yk4MnKNeOrEjbus0DZoQMSpkyQv/I60FSswXL6MJS3NKv4KMmj2d7y4ezo5/21k2gr5qT735Ely7SpYAKQuX47PiHtRiPQEgsri5J+w4Q3Hth224CZUlT5BVGvo/9EW6/L47/aWyzm83ar+70L+bFZGrrAAVgQVFht9+fJlLl26xOXLl7l8+TJXrlwhOzubXbt2ERERUVHDqJrcYiqYe9vXpWGAOwazhe3nk0reQXBDKJRKAp56ksZr/kXboIHDNrfOtsjfJtu2onV14/P+nzPtpd95b7SS9CKyGeSeOEHS11+X57AFguIpKP4KohIPJxWF0ew8+XGH+j788UxPp9tulJcGNadXE3++eahTmRyvPMifzRIZLSqGCnvEa1Dgh1Ngh30qmJtAoVDQo7E/l5OyOHo1laFtKsecXxvwe+Rh4t6cA4AqIIA6H3/ElfEP4jlgAJogm49OC/8WpHZuyuNNLlDnOmS4Qq4G2mT48NrJpph27CF5+XICJk9GUdmJgAUCnTfkpjm2iSngSufPZ3qWWRqTUG9XljzevUyOVV7kz16JKeCKodwFYE5ODhs3buSuu+4CYMaMGej1tjqqKpWKt956q3JTIVQ2tygAAdrUlSMjT8WIxK3lic+YMSR+OR9zcjKBzz+HJiiIJuv/c9p3aqepPLvpWWL8bW2H/NIY02M/3x4Aj8QksnbvweO2XhU0eoGgCAJbQPQexzYRBFLunLiWxrTfjjjd5qpR1bocdvlTwO+vPUv7ej70bBJQySOq2ZT7FPCPP/7IN998Y13/8ssv2bVrF4cPH+bw4cP8/PPPLFiwoLyHUbWxloNLK75fMTQLliO4tp9P4vEfD5TFqAROUKhUNPj5/wj/bRm+991XbN++YX35buB3hdpNagU7W8o39ujHHyd9zRoyd+4ke/9+JFG7UlBRmO2sLO0fKLxdpIEpd34/eJVzThI+A+QYa19xhPwpYIAHyskXUmCj3L/hS5YscajXCbB06VIaNWoEwM8//8z8+fN54YUXynsoVZd8AXiTPoAA4f7u1uUNp+NJyMglyLMWW1XLEZeGDUvdt1toN44/fByApJwk/jj3B6n6VLbE/B+DDsk3+GsvTLMdu2kT6nz4Ebrmzcp20AKBPSYDZMTJy4+ug/rdYc8CSDxt6yMsgOVOQkbRNW9fGtS8AkdSNRABjBVLuVsAL1y4QJs2bazrOp3OoS5f165dOXXqVHkPo2qjy0tse5NpYAD83LV46Wx6XkwFVz0CXAN4qt1TvNL1FWY8/B3nnbhq6s9fIObFFwtFywsEZUr6NUACtQ7CusltBXOQiiCQcudaSk6R2+xz9dUW7H/DQPyOlTflLgBTU1MdfP4SExMJDw+3rlssFofttZJ8AVjQCfsGUCgUfDXeFt11OSnrVkclKEe61enOxpduZ/Z4FSu7KVjSV8nqzvK0sP78eZK/+Zaco0creZSCGktaXq4/7zBb/fGCFj8xBVzuxKUXbQEsKIZqA16ujp/Bx3/cX0kjqR2UuwCsV68eJ06cKHL7sWPHqFevXnkPo2qju/UpYIDbmgbwSF4i6MSMWi6qqwEz+85B0aEVP/dX8XcPJT/eqWJ3hPxjnPjpp0SOvZ/MrVsreZSCGkl+smcfuwTDBS1+Ygq43MnSF+3n51ULp0M9C4jemLSiBbLg1il3ATh06FBmzZpFbm7hf2ROTg5vvvkmw4YNK+9hVG3KwAKYT/60gRCAVZ8gtyCW3bWM7wd+z6R2kwBYfpuSU3a/ycmLFhP11FOcadOW+C+/qKSRCmocSXnlN73tPmwFLX4iDUyZk5ptID3XSEqWAUmSig30qI3+cLVR9FYm5W5jfvXVV/ntt99o3rw5U6ZMoVkz2bn97NmzfPnll5hMJl599dXyHkbVpgx8APMJ9MgTgJlCAFYXuoZ2paV/S3Zc28ExjvHGg2oaxkm8t9hsrUMMkDj/K7Lu7Eqj5t0qcbSCao/ZBDs/k5ftLYBufo79hAWwTNl1Icka2apRKVg55TbMlqJ9fTXq2pUCBsDbVXzmKpJytwAGBweza9cuWrRowfTp0xkxYgQjRoxgxowZtGzZkh07dhAcHFyuY5g3bx4KhYKpU6da23Jzc5k8eTL+/v54eHgwatQo4uPjy3UcRWKNAr51C2Cwtxz5u+VsIv+djLvl4wkqBg+tB0uGLeGTvnIprsshClLdHPuoJNjx5esiQERwa+Sk2JYj7rIt95vp2E8IwDLl3TW2CGujWeJodKp1XauSf4q/frAT47rW5/ZmgTQL8qzoIVY6BaeABeVLhZSCa9iwIWvXriUxMZE9e/awZ88eEhMTWbt2rTUdTHmxf/9+vvnmG9q2bevQ/sILL/DPP/+wfPlytm7dSkxMDCNHjizXsRRJGU4Bd2voh4ta/rc++X8HCxUVF1RtBjQYwO5xuxnRZATLbpf/jyfrwwej5OUW26M5dm5bZQ5RUN0xG+S/Sg0EtbC1ewTBiG9t62IKuEzxdXP0sUzNK3emVio4984QIucNY3DrEOaObMOPj3ZFqax9FkAPFyEAK5IKqwUM4OfnR9euXenatSt+fn4l73CLZGZmMn78eBYuXIivr6+1PS0tje+//56PP/6Y/v3706lTJxYvXsyuXbvYs2dPMUcsJ/IFoCkXTLc2davTqHDV2kqLxaYVnWZAUDXx0Howp9ccsob05H+PqfhwpIppzy0lJdQDj1yInzuvsocoqM7kC0C1kzQjniG2ZWEBvCXOxKXz6A/7OXFNfrAP8HC83vPWnAFwuF/XdtSqCpUktZ4afbUnT57MsGHDGDBggEP7wYMHMRqNDu0RERHUr1+f3bt3V/QwbaXg4JYjgQGGtLYlmCsuzYCgavNmzzcJ79CH5/vOpHVwOxSzpgIQtieS0+3a8c1j3Vmw/QOHfXbF7OLfS/9ikURFEUER5FcAcSbwQtvZlpVCmNwKD363l01nEnhgoWxUKGp601UjrnNxCJeX8qPG2lt//fVXDh06xP79hfMIxcXFodVq8fHxcWgPDg4mLq5ovzm9Xu+QszA9vYySVCpVsh+gPl1+eQTe0uFm3dWSX/ZFARAvBGC1pY5HHb4a8JV1vf1tI4nkbXlFb6DPTgOXLi5i4+J2nEu7yMW0i6yLXAfA5ujNfHD7B84OK6jtmPPuYc4SPbv6wNQTzq2DghsiKVO2tKbnmgDQG50/lKlq4VRvcSwY35FnlhyyrhvNEtpaGBBTEdRIC2B0dDTPP/88S5YsQacru3Joc+fOxdvb2/oKCwsreafSYg0ESb3lQ7lqVYzpLOdW3Hvp+i0fT1A1cNW48lcvR2tBozgwP/Q82/7+knWR6+h2xsKgAxbWXl7DuZRzlTRSQZUmfwq4qEofPmGFq4IISsXaE3F89N9Z9lxKdmjffCaB7CJSvhjNwsJlj3+BqXKjWcxmlBc1UgAePHiQhIQEOnbsiFqtRq1Ws3XrVj7//HPUajXBwcEYDAZSU1Md9ouPjyckJMT5QYEZM2aQlpZmfUVHR5fdoK2BIGVjVbyrbR0ANp1JKJPjCaoGHV//kGefVjHjYRWf3aPEooCwJJj1i4UX/zDzv78sPLbeQqsrEvevup/YzNgSjxmXFce0LdP44cQPmC0iaKjGY50CFqXeypLDUSk8/fNBvth0gfu/dfQln/jDfvZdlkWhtoCfm8kiBI49apWjtU8IwPKjRk4B33HHHRw/ftyhbeLEiURERPDKK68QFhaGRqNh48aNjBo1CpDzEkZFRdGjR48ij+vi4oKLSzlNjbj6yH+zk4vtVlra1JUFZUKGnlyjGZ3wM6kRDA4fTLenu/Ff5H8092tO0tBjBM9ZhBQTT9dzNkvC7F8sRAblsGD7AA4PacTiwYsJcA0gTZ/GxdSLdAjqgCKvBNjrO19nT+we1l9Zz964vfSq04sAtwAGhw8mw5BBck4y4d7hlfSOBWWOqZgpYMFNcz4hs9jt8enydfdx05Bgl6jfXVsjf4ZvGo3SUSAbhAAsN2rkJ8/T05PWrVs7tLm7u+Pv729tf+yxx5g2bRp+fn54eXnx7LPP0qNHD7p3714ZQwavuvLftKtlcjgfNw0eLmoy9SaupmTj66bFy1WDRkRZVXt8db6MjRgrrwS1x9xhOFGPPU7uiRO4NG+O/qxc5SE8AcITLFiUl5jn+TYebj78cf4PACa3n8zT7Z7meOJx9sTarBU7ru1gx7UdAPSu25vnNj3HoYRDLBq0iE7BnRDUAIoLAhHcNH5upRPUBQWgyH3nSMEE2Fl6M7k6YcQoD2rtJ++TTz5BqVQyatQo9Ho9gwYN4quvvip5x/LCO68eclrZTCsrFArq+bpyJi6Dv4/EsGDLRcZ1rc9b97YueWdBtULl7U3D35djSklB7etL1r59pO7eQfqChQCM32Ihfe863hutgnryzXX+kfnMPzLfegwfFx9mdJ3BK9tfsbZ1X2p7GHp95+u82u1Vuod2x2A2ICHhrnGvoHcoKFNK8gEU3BSmYqp62OPj6njdg7zKzk+9JqAuYAHs9+EWPFzUrHm+N2F+bkXsJbgZao0A3LJli8O6Tqdj/vz5zJ8/3/kOFU1+Saak85B4DgKb3fIhg710nInL4ItNFwD4vz1XhACswajzcl26d+2Ke9euaIwWkr/7HgCvHHjn/8z8NEjLus4qjBajw74vdn6RAQ0GwHbnx47OiOaZDc8Q5BpEjjkHJFg5YiUBrgHl+p4E5UBxeQAFN01ppypb1vFiX6QcnOeiVvLG3S3Lc1jVDmdR0Zl6E/suXxcCsIwR84FVBfe8qLvLW2F+F9j/3S0fMtir8A3+l31RIq9SLSFw6lSabNyAavWP1rYJ6wxsGrmeZzs8y2OtH6NXnV4sGbqEwbQi8aXpfN3oVe5pfA8dEj3wzZCok6llcHJd6/4JOQlkGDLIMGaw5vKaYs+fkpvCH+f+wJAvOARVA6sFUEwBlyX6IqJ87RnWNtSh/u+ZtwbTKNCjPIdV7Sjq9+laqihqUNbUGgtglSc/Cjif1f+DLo/f0iGDnUwtzPjzOG5aFcPb13Wyh6AmoVCr0dStSzPqkvbB+8S89LK8Yd02nhz5pEPfCw8MxBgdjd+/a3gwv9HDHSQJsq7QfPogFqi2Y7DYxNz7+9/nz/N/0rNOT17o9AJqpePt5K09b7H+ynrOp55netfp5fhOBcUSfwp2fQF9p4NvAzEFXE58tvE8ADqNktwicv7V83El6nq2dT0/EEtgoyjzxLUUIQDLGmEBrCrkRwGXIWG+zs3lO84nlfm5BFUb77vvxmfc/QDEvvoqV599FskkJ6jN3L4Do7OURplZkCX/WN2xJpaV967kwRYP8untH8vCELiQeoGfTv3EO3vfse5mtphZc3kN66+sB2DJ6SVkG7MLH19QMXw3AI4uhWV50l4IwDInNdvA1TyB0jCgsEVPm1efvUWoF4Nby6nGmgYJy58zgjyduyYIC2DZIyyAVYWCFkCAnNRbEobDO9QhMVOONmsU4G7Nrn48rzaloHYROGkSmRs3YUpIIGP9BlKXL8fn/vu5+vzzDv3UgYGYEhMd2nKPHsN013gmzXqdmEnT+aZ9BE/ddgbyLBi/n/udTEMmKqWKzVGbyTY5Cr4PD3zIrB6zShxjVHoUc/fNpVNwJ3rW6UlLf+EfdcsYs+S/ccfkvyIPYJmTkVftAyDAQ8uKyb3IzDXh667B00VDjtHMtdRsbm8WhAII8tTRMtSr8gZchfHUaVg3tQ/3f7ublGybr/LVFPEQWdYIC2BVQedTuC016pYO6aJWMblfEyb3a8KQNqFs/N/tAERfzy7kZ5GlN/H2qlPsuiCsgzUVdWAgTbZuQdukMQBxb84h+rHHkLLlG2vYN1/T4sxpmm7fRtPdu1AHBaF0c0MdHAyAKT6eq5OnYMnMxHfHSbZIL/ItDzHooAWFRWJt5FpWX1rtIP7aBrQFYPm55fx48keSc4rPc7n45GJ2RW/n7Lef8OFH9xGVeqU8LkXtJSdF5AHMIyXLwJ5LyUX6nEUlZ3OimIdlSZLYcymZlCyDgwDUGy20D/PhtqYBtKrjTX1/N5qHeNI/IhiVUoFSqaBHY3+83YQPZlE0D/GkbT0fh7aY1FwspYy0FpQOYQGsKmidTAdkJRZuuwXq+bqiUECWwUxyloGAvJI7vx+8yovLjwKwbH80x98cVKbnFVQdFAoFIa+9TtQjjwCQtWs3AMGvv4bH7bdb+6l9fWm06h8UajW5Z88S8+JLGK9dczhWwnvv4QM8BjRUhbL+di8upl3ES+vFggELaBojEff2O5w1evJ+v3Q+PPAhHx74kLHNxzKx9URC3UMxJySy58PpbDGe4rf2WSgleGa1hdtPyDf6q3UXU3/yG+V/YWoLi4ZAazn5fW0PAhn2+XZi0nL55qFODGpVuAJUnw82A7Bv5h0EeRb2p15zIo5JSw5R18eVj8e0s7brReLiMsHL1fHzaTBbSMrSO/1fCG4OIQCrCva5j/waw/WLkFW21jgXtYpQLx0xablcSc4mwMMFg8liFX8AGXoTOy8k0auJSO9RU3Hv3o0GP/8fV194AXNiEn4PT8Bv/PhC/VRe8hSVW4cONNm4gYSPPiJ54Xd4DR2K0tOT1GXLrH37r4llwLZ0LFkm4DqqryYTmSh/fhsA88/Aj3coWd1VyeGNv5Ky9BeONFIwfouFHmckRgOj1xYea/dHXiqHK1CLSTwNGTHysmfRZS9rAzFpuQD8dzK+kAA02Ym46OvZTkXHupNxgOyblu5gARTlFMsCZwmy07KNQgCWIUIAViVeOAW5qbDjkzwBWLYWQIA6Pq7EpOUyasEuLs8dyqGolEJ9Xl9xgk0v9i3zcwuqDm6dO9N02zaMUVFo6tUr1T4BU6aga9Uaj359Ubq44HPffag83Ll83xgsGRlYsrKsfc2JhR9eHt5ooXWkRKeLJU/jhLwxG5/77kOhEtn/y5zUvIAf77DKHUcVQaNynncuH20Rn0EXte2hPT3H5qumNwkLYFngpStsoU7PNTrpKbhZhA9gVcK7LgS3AvdAeb0cBGB+UAhAwxn/FipaDoiSO7UEhUKBtkGDUosspYsLXoMHocyrh+3auhXa8HDqf7cQl4gIp/uoAgLwHDLYul4a8ec9ciQ+Y8cK8VdeXJCjs/GpX7njqCKoVQpe/es4/T/aQkaukWyDia7vbLRuNxfhI6i1E4D/s5tFMYop4DLB3gLonTcd/NiPByprODUSYQGsirjnTb+W8RQwwJDWoXy99WKh9s/ub0+otytjvtlNhl48ZQlKj2u7djRa8ReSyYQpPh5N3bqY09Kw5OpRaDWofX1Jve02Yme+Zt0nbOFC4ufNQxkcSL2589AEByOZzVZXCJEfrQKoxQLQPphArVTyw65IQPbrc9eqHap6FDWl66J2/oDy4X3tnLYLboyuDf3QqpU0CnDnTFwGAKnZRkxmC2pR075MEAKwKlKOFsCpA5oWEoAbpvWhSZAn0XkJSqOv53AmLp2IEJGmQFB68hNPg1yfWGWX2ch7xAgUajVKTy88+vRGoVbj0XuV4/7C4lexeJdu6r8mkmE3xWs/3as3mvFxEnzgDPsp4Hweu60h3Rv5l9Eoazddwv04MutOXNQqGr/6r7U9Lj2XekXkuBXcGEJGV0WsAjChzA+t06j46dGu1vXBrUJoEuQJQF0fV0K9ZQfbPw9dc7q/QHAzKJRKvIcPx7N/PxRq8dxZJajFtYAz7HzJ7CtzJGboyS3gw6cvoqqH0omV2s+9dqfWKWvctOpCtYGvioogZYYQgFURqwAsn5x8fZoFWrOtj+5kswIolQr+N7A5AAfyipULBIIaiEvttu7b5+2LTLIFL8Wl5zoEdEDRQR3OLIPOIlcFt8497epYl+PTcytxJDULIQCrIlYfwERrya2y5o9nevLTo10Z0DLYob1HY3n64lBUKr/uu7VE1AKBoIryzM7KHkGlkm2w+fUlZ9nqW6fnmApFmupNzn0AnfkGOotcFdw674xobV0WUdZlhxCAVRGPEFC5gCnXVr6pjAnzc6NPs8BC7XV9XAnxkqeBf9gVyfGromycQFDjqMUBIAC5duLNbBcQcigqhVMx6Q59Y9NyMZgsnIpJd6ga4swC6OUqLIDlgadOw+C8XI0ZuSb2XkrmUmJmJY+q+iMEYFVEo4MmA+TlS1sq/PTfPdwZgDNxGdz95Q6OXU2t8DEIBAJBeWFvAbQnIUPPqmOxDm0frDvLi8uPMvTz7Sw/cNXa7sw3UFgAyw8XjSxX3lp1irHf7qH/R1u5bDd9L7hxhACsqgTKvnjWpK0VSESIp8P6fyfjK3wMAoGgjFCI6OqC5BRTrcPDRU2PApG8K4/K1VM+Xn/O2mY/FRnipeOutqG0C/Mp24EKrDiLut4vfNVvCSEAqyo+eVn60ypeAKpVSra/3M+6LrKvCwTVFIsFJFGarCC5RVgAAe5sGcwvT3bH1UlC/Ot2/oL5voHzRrZhz6t38OUDHdGI/HTlhrO8i4EetTeSvSwQn9aqSn6ZppQrlXL6MD833h/dFoBLicLMLhBUS8yGkvvUQrINpiK31fVxBZxbCQ1mC2tPxDHjz2NsOC2n6cqfmhSUL84sgIJbQ1zRqkpgXmmt5PNgrJy8R40D3QGEs61AUF0x20o/MuQD+e+wjypnLFWInCJy+4FcgQKwPgAX5OmfD/LLPtvMjKeL8PurCLROBGBRSboFpUOELFVVvOvJ+QCzEiFyJzQdUOFDaBTgAUBMWi7ZBhNuWvFxEQiqFWY7940uj0Pb+8DVt/LGU0XIcWIBfGFAM8Z1DSMoLwvCmM5h7LmYzJ+Hi0+KX9fXtVzGKHDEWfk3UXf51hAWwKqKQgGtRsrLG98ot3yAxeHrrsXXTX66FdFWAkE1xJRnAVRq5DrLtUD8SZLET7sjWbjtkkOKF3ucTe/6uWus4i+flnVKTpgtBGDF4EzsGUROwFtCCMCqzO2vyDfuuOOQdrXk/uVAo0DZCij8AAWCakj+FHAtKvt2MiadWX+f5J1/T7P3crLTPs4EoL+TgILGefe/4hCpXyoGZ2l3hAXw1hACsCrj7m8r2P5tX0iPLbZ7edAoQPYDvCj8AAWC6ocpLwhEVXtq1F6ym61IzNA77eMsD+AdLYIKtTlLlp/PPe3qsPzpHjcxQsHNYDDb/mferpq8toqfGatJCAFY1fFtIP/NToJfx1X46YUFUCCoxtRCC+C1FFvQXHqu82jfyAIuLREhnk7TjKiUikJ5UfN5d2QbuoT73cJIBTeCvQXw9jxhbhRTwLeEEIBVHfuSTQmnK/z0jfIjgZOEBVBQc5k/fz7h4eHodDq6devGvn37iuy7cOFCevfuja+vL76+vgwYMKDY/pVKLbQAXk3Jti6n5xTOYbrhVDyHolId2uoV48fnLLefj5sGDxcRFFeRuNtd7/z/iYgCvjWEAKzq5OcDhEp5im8aJFsAz8VnioTQghrJsmXLmDZtGrNnz+bQoUO0a9eOQYMGkZCQ4LT/li1bGDduHJs3b2b37t2EhYUxcOBArl0rPlq0UqiNFsBUmwUww4kF8K3Vp6zLDfzdaBLkwZN9Ghd5PGf+gvm5AgUVx5T+TegS7sv7o9uiVSsAYQG8VYQArOq42EWhVcJTfMMAd5oGeWAwWdh8xvkPokBQnfn444954oknmDhxIi1btuTrr7/Gzc2NRYsWOe2/ZMkSJk2aRPv27YmIiOC7777DYrGwcePGCh55KciPAlbVIgHoMAVc+KE1zc4qOOuulmyYdrs1958znFkRi7MYCsqHAA8Xlj/dkzGdw9DmWQBFEMitIQRgVccz2LZcCQmhFQoFtzUNAOC77Zcr/PwCQXliMBg4ePAgAwbY8mwqlUoGDBjA7t27S3WM7OxsjEYjfn5Fiwi9Xk96errDq0LIrwSirh1TwJFJWZxPsLmrLN0bhVQghZa9oHPVllwn2ZkVsa6P2y2MUnCr5E8Bf77pQiWPpHojBGBVJ+IuCO8tLxsywVx0CaPyomN9OXfY8Wtp7LyQVOHnFwjKi6SkJMxmM8HBwQ7twcHBxMXFleoYr7zyCnXq1HEQkQWZO3cu3t7e1ldYWFiRfcuUWmYBXHuy8P8sLj3XYd0+NaCzer8FmTmsBQBD24RY20Tuv8olJdsm4tOyhWvSzSIEYFVHpYEH/7St6yvIcmDHoFYhBHjIFoTx3+0l14lPjEBQG5k3bx6//vorf/31Fzqdrsh+M2bMIC0tzfqKjo4usm+Zkm8BVNWOXHX51r3h7etY24pLFlwaC+D4bvXZ/GJfZt/dytomfAArl+QsW3of4Zt+8wgBWB1Qa0Gbl5A0q+ItcFq1kq8f7GRd3yR8AQU1hICAAFQqFfHx8Q7t8fHxhISEFLGXzIcffsi8efP477//aNvWed3YfFxcXPDy8nJ4lTsZ8bD/e3m5lgSB5IuBBv7u1ipGxfmJqRSKEo+pUChoGOCOzs5aGOJdtNgXlD8J6UIAlgVCAFYX/JvIf5POVsrpO4f78UA3OSXNyZi0ShmDQFDWaLVaOnXq5BDAkR/Q0aNH0Ul+33//fd566y3Wrl1L586dK2KoN87SMRC9R16uJWlg8v31vHRqq5+Y3s4CWNAa6KErfSoXncb2cymCQCqXtvW8rcvOfDQFpUMIwOpCYIT8N/5U8f3KkSZ5SaEvJoik0IKaw7Rp01i4cCE//vgjp0+f5plnniErK4uJEycCMGHCBGbMmGHt/9577/H666+zaNEiwsPDiYuLIy4ujszMKpYrM/aIbVnrXmnDqEjyp4C9dBqrADTaVYvIsLMWzR3ZhlDv0gs5F7WKz+5vzydj2xHgpGycoOKYPiTCuuwsSltQOoQArC6EdZX/nl9XaUNoGiwLwBPCAiioQYwdO5YPP/yQWbNm0b59e44cOcLatWutgSFRUVHExtrKMC5YsACDwcDo0aMJDQ21vj788MPKegslo6kdFqv8yh9ermq0avnnLT3HiCRJZBtMRCbLD68eLmrGda1f5HGKYnj7uozoUK/sBiy4KXzctPTOy06RJgTgTSNSmVcXIu6C1dPg2iHIvg5uFV+CqGN9XzQqBVdTcohKzqa+v0iFIKgZTJkyhSlTpjjdtmXLFof1yMjI8h/QrWIu8KOoqV0WQE+dxporbsKiwlVaPG9g6ldQNfHSyT6eb68+zX2dKyiqvoYhLIDVBc9gCGgGSBC9t1KG4O6iJiJEdl4/FVvx0cgCgaCU5KQ4rtcSC2B8XsqXAA8XNOqiAzx0pUj/Iqja1POTP9NCzN88QgBWJwKayX/TrlbaEPJrAz/980GWH6igVBYCgeDGyC3gplELBGBGrtE6BVzX19VpDd98pg5oWlHDEpQTd7eVU/2YzFIJPQVFIaRzdSJ/2rfg030F0ijAw7r80u/HaBToTqcGFT8dLRAInJB4TnYVidzu2F6D08BsOZvAd9svsyMvSb2PmwYPF3WxAjB/+lBQfVGrZAuvySIE4M0iLIDVCdc8oZV9vdKG0KWhr8P6D7uuVNJIBAJBIXZ/UVj8AVhqbqqM//121Cr+APzd5ZQ3Luqif97EtGH1R63MF4CiHvDNIgRgdcJqAaw8Adi5gR+NA20O5f8cjWHXRcfk1FdTskXJOIGgMkgp4oGsEkpIVhTJWQaHdWVecudiLYCuwgJY3VEr5f+vWUwB3zRCAFYnqoAFUKtWsnZqH86/M4SHujcA4IGFezkdm44lzxR/z5c7Gf/dXvZcSq60cQoEtZLsIr5zZoPz9krmXHwGh6Jkl5aULAMbT8djKqZyhz1mi8TqY7GF2vPlgEZVdBCImAKu/qjyLIBGYQG8aWqsAJw7dy5dunTB09OToKAg7r33Xs6edayikZuby+TJk/H398fDw4NRo0YVKglVpXDzl/9mV651TaNSolEpebZ/E2vbkM+2M+KrnSRm6Lme90S+8XQVvpYCQU0ks4jvnHfdih1HKZAkiYGfbGPkV7tIyMhlzDe7eezHA/zfntK5lWw5m8DkpYcKtXcI8wGKtwDeSAUQQdUk//9rFj6AN02NFYBbt25l8uTJ7Nmzh/Xr12M0Ghk4cCBZWbYqFi+88AL//PMPy5cvZ+vWrcTExDBy5MhKHHUJ5N/EYw5XSk3gggR5OdbDPHo1jS7vbLCubz+fhMUiIUkSB69ct6ZoEAgE5YQ+w3HdtyH0mALtx1fOeIohMdNWzzUyKZvzCXIlFWdWPWdEX8922v7q0BYA1kTQ9twREcSb97TCw0UIwOqO1QJoln9jBDdOjf0WrF271mH9hx9+ICgoiIMHD9KnTx/S0tL4/vvvWbp0Kf379wdg8eLFtGjRgj179tC9e/fKGHbxeNlloP+qB/zvDCgrN5/Vb0/14Mn/O0BqduFs7GfiMuj49nrrNj93LRun3Y6ve+2oSyoQVCgWC5gKPGS1Hgl3zKqc8ZTA1ZQc67L9w6G6mKnbfCRJIi2nsF/jwJbB1vuLtoAFcObQFjzRp9HNDldQxcgPAgHZCliaz43AkRprASxIWpqcF8vPT/ajO3jwIEajkQEDBlj7REREUL9+fXbv3u30GHq9nvT0dIdXhWJf/SMrAdIqPw9f14Z+HJk1kGVPdifP95o7IoLo1zwQwEEYXs8ysPdy5fkvCgQ1GqMTi5hr1UzRtPxANCO/2mVdf+WPY9bl4qZuQRZ/93+7h082nCu0zVVreyAueBx/D/HgWZOwF3wiFczNUWMtgPZYLBamTp1Kr169aN26NQBxcXFotVp8fHwc+gYHBxMXF+f0OHPnzuXNN98s7+EWjUIB3SfBnq/k9aQL4BteeeOxo1sjf3a+0h9/Dy0uahXn4zM4F5/JtdQch37Hr6UyuHVIJY1SIKjBGHMKt3WoelO/IOcQtUdvsjnylyQAk4t5kHTT2n7SGgU6lr8T952aRX4UMAgBeLPUCgvg5MmTOXHiBL/++ustHWfGjBmkpaVZX9HRlWCBGzwXWtwjLycVfgKuTOr4uOKilp/AmwZ7sv3lfux99Q4uvjuU90e3BWD+5otMW3aEzWcSKnOoAkHNw5jluF63M7j6Ou9bxbB35Lef2nPGtRQnQjcPfzv3kpEdbC4zLwxo5iAOBdUfewugSAVzc9R4AThlyhRWrVrF5s2bqVfPdkMICQnBYDCQmprq0D8+Pp6QEOdPii4uLnh5eTm8KoWAvDJGyecr5/ylRKlUEOylQ6VUcGeLYGv7n4evMfGH/aw/JaKEBYJb5sJGWDujcPk3RfW4vdfzdSxTdyEhk5l/HScxQ1+o73fbLzF8/s4ij2XvX2wf6as3mctgpIKqhP2DgkgFc3NUjzvETSBJElOmTOGvv/5i06ZNNGzY0GF7p06d0Gg0bNy40dp29uxZoqKi6NGjR0UP98bwzxOASVVbANrj66615g3MZ9MZIQAFglvm55GyW8j2jxzbW4+qnPHcIGG+bg7rl5KyWLI3itkrTzi0J2fqeXv16WKP1bOxv3VZZScQcoxCANY0FAqF9X8sUsHcHDXWJj558mSWLl3K33//jaenp9Wvz9vbG1dXV7y9vXnssceYNm0afn5+eHl58eyzz9KjR4+qGQFsT0Az+W81EoAAT/dtzIrD18jQy9F7Z+IySthDIBCUmsvb5L/ugTB8PjQZUHz/SiTI04WEDD1fP9iRPw5dc9pnf6RjzfNMvfNqJgsndCbMz5W0bCMtQp3PyuQahYWoJqJSKjBbJIylTB4ucKTGCsAFCxYA0LdvX4f2xYsX88gjjwDwySefoFQqGTVqFHq9nkGDBvHVV19V8EhvgoC8BMyZcZCbDrpKmoq+Qer6uHJ41p1cTMxi0KfbOBuXgcUioSzB50cgEJSCnDzB5F0Pmg2q3LGUQHqunB2gVR1vfj/oXAAW5FJiltP2O1sGO223Ry8sgDUSjVKBAWEBvFlqrAAsTWJInU7H/PnzmT9/fgWMqAzReYNHsJz1P/k81O1U2SMqNWqVksaB7mjVSrINZjaeSSjVDVwgEJQSjVvJfSoRg8litch56TRytG7xM7sATPxhf6G2lkVY/AoS4OlyQ2MUVA/sk0ELbpwa6wNY47H6AV6o3HHcBGqV0jpV88zPB0nNrpp1SgWCaolH1X6gysi15Qb10KmZYldS0p7MXJP1Qd5ZfeAhrUP4+sHiH34XP9KF4e3rMLmf83MIqjeiHNytIQRgdSU/Ejj+RPH9qijzRrbBw0WNySLRfs56Vh6NqewhCQQ1A5+wyh5BsaTnyr58Hi5qVEoFXjqN0345RrO1rni8k4jgBQ92or5/8dbOfhFBfHZ/B7xdnZ9DUL2xWQCFD+DNIARgdSVETmjNwR9Bn1m5Y7kJWoR6MWNohHX9uV8OM/KrnXyy/pyo6ygQlBZn3xXvyhOAB6+k0Pv9Taw9Ubie7xsrT9Jhzn/8cfAqAF66kj2QrqXmMOPP4/Sat6nMxyqovpxMPkn/3/qTFTwbpe4qd32xg/2RosrUjSIEYHWl3Tjwrg/6NDi9srJHc1Pc1ymM0Z1suRkPRaXy2cbzfL/jciWOSiCoRpgL1+DGp37FjyOP5345TPT1HJ7++VChbT/siiQl28iXm2W3FU87y9/8Bzo6Pd61lBx+2RdVqH14+zplNGJBdWRT1CYScxJRatJRu58F4DEnPqKC4hECsLqidYdOE+TlI0srdyw3iVat5MP72rHv1TsY3832o7XxtFwlZM3xWA5FpRS1u0AgMOUWbqtEAegseTOAxYmPlperzQI4rG0op+YUjlyOuu5Y3/iutqGcnjOYT8e2v7WBCqo1sZk2C7NCJX/m7MsJCkqHEIDVmVYj5b/Re8Hk/MZbHQjy0vHOiDb8M+U2AM7GZ3AmLp1nlhxi5Fe7iE938iMnENRmDv0ku39smF14WwVPAecazXy64RwnY9KK7JNpKJzDr6Dvn32pNleNXFLyYqKje0uTIA9ctSoUCpE6qqZyPfc6Xxz+gjm753Aw/qDTPtcy7VIHKeXfB1etqiKGV6OosWlgagV+jcDNH7KT4fx/0OLuyh7RLdE02AOtWsn1LAODP91ube/27kae6duYlwc1Fzd+gSAzAVY+63ybize4eFTocL7ddolPN5zn0w3n0djVZ801mtHlCbmM3MICMMRbV+QxB7YK5u8jMZxPcBSAdX1ci9hDUFOYvXM2W65uAWD5ueUcf/h4oT6xWfYWQLk2tE4tBOCNIiyA1RmFAiKGycsHf6zcsZQBOo2KR3qGO922YMtFNp9NqNgBCQRVEWOO8/ZBc2HCigodCuDgfG+fj+1aqm2c6TmFfRWfH9C0UNvaqb2Z/0BH7u1QF4AL8QUEoK8QgDWdvXF7HdYLBgWaLCYSsm2/BQphAbxphACs7rQdK/9Nrn75AJ3xyuAIxnSuh5dOzUuDmvOsXY6wf4/HVeLIBIIqQnZy4Tb3QOgxCeo6D6YoCyRJ4lJiptWfT5IkLidlFZmD7YKd9a7gVG7TIA+CPAtbACNCvBjWNpR6eZa+jALl3wrWDRYUjSRJRKZFYpEcfeMSsxPJMspVVbKN2cRlxZFrynWcVi0FyTnJpOlt0/7phnQOxB0g01B0VopMQyYJ2QlkG7M5mXSy0NgA1ArHiclrmdc4c/0Mp5NPI0kS8dnxmCVbZRelTk4hplYqMFqMHEk4wqnkUzf0XmorYgq4uuPXWP6bchkOL4EO4yt3PLeISqng/dHteH90O0BOAGu2SHy15SIbT8dzLTWH3ReT+XzjecZ0rsdjtzUST36C2sXCfpVy2sU7I5mz6hQvDGjG8wOasu5kPE//7NxHC+D1FScY1CqE61kGpiw97LCtQQn5+4qy9BU3bSxwZPm55by15y2eaPMEz3V8DpBFW//l/fHSerFz3E5GrRzF1cyrhLqHEpsVy+93/05zv+YlHjvbmE3f3/riqnZlzwN7UKDggdUPcCX9Cs18m/HHPX843W/QH4NIN6QT7hVOZHokk9tP5ul2Tzv0USsdZcmQP4dYl1/r9hqNfRo7bFeqs1BoE1EoPHhv33ssO7uMlv4tWXbXslJdp9qMsABWdzxDQOcjL/89CfZ/V6nDKWvUKiUv3NkMT52alGwjveZt4sXlR4m6ns2H/52jxay1HLwiIoUFtQRnaV8AzOVfTWfOKtmq8smGcwB89N/ZYvvn5tXfPXo11aG9Z2N/Zt/dqth93bRq/Ny1Dm3P9m9irfwgKJm39rwFwMLjC61tRxOPArK1TpIkrmbKORnzfepWXFhRqmNfSb8CQI4ph6ScJNIN6da2cynn0JsLByXmmnJJN6QDEJkeCcD2a9sL9SsoAO05GH+QDEMGABF+tjyySm0i9f3cOZ4k+wtGZ0SX6n3UdsS3qbqjUMDoRbb1/d9X3ljKCY1KyYwhLVArnQeAvL7ihEgeLagdpN/YNF1l8MczPQG54od92TeAvs0DWfpEd8L8Sp7KtQ/4aFPXm/8NLNkyJSg9cVk371Jj74MXkxnjEJQBjmlarG1ZhdtcVYUtvcUJwOu518k1yz5/XlovuoZ0BUChNGKyWKznXTxocSnehUBMAdcEmtwBT2yChf0h4RTEn4LglpU9qjLlgW71GdYmlKjr2ZyLz2BY21AuJmYy7PMdnIpNZ8vZRPpFBFX2MAWC8iX15i0bh6NSGPHVLgB2Te9PnVJE1EZfz2bSkkMcv1Z0ipeCNA/xxMdNQ2q2kWupORhvMj9bPV9X63ldNSrOXj/L9O3TuZAq+zsPazSMMc3G8MH+D3il6yu0D2pf6mMbLUae2/QcrfxbMaXDlFLvl2vKZcrGKeSac5l/x3y8Xbxv6D1VBIcTDjNhzQSHtlErR3Eu5ZxD28A/Bhba9+fTP3NXo7toFSBbaLON2UzeOJkD8QdwU7thsph4s9ebViscwO/nfufvi387HCcmKwaTxcTL21/GS+tFljGLaxmFH15yTI4BTasurXIqFPPZG7fXGiQy7JfLSMC+vhKudX/hIL9AnuEx1CO0yGMIbAgLYE0htANo8p6qvxsAKVcqdzzlgLebhjb1vBnVqR46jYpWdbytUcOilrCgVpAZ77z9rk9K3PWh7/dZl2evPFmq081Zdcqp+Ms1mskxmgu1Nw/2xMNFbbXeXUvJcUgBcyOG+k4NfG3HDfFkzeU1VvEHsPrSah5e+zAnkk8weePk0h8Y2Bu7lx3XdvDNsW9uaPbgaOJR9sbt5WjiUXbH7L6hc1YUz2x4plBbQfFXHKsvr7YuH008yoH4AwBkm7IxWAzM2D6DmEzb/bag+ANIykliTeQazqec52D8Qc5cP0OGMaNQv4JtM7bPcFgPdXcu5NxzJFrsiaXlnlh8ncSceGm9in6DAivCAlhTUCphwBuw5mUwZsHfk+GRVZU9qnLnnvZ1+GFXJCuPxjC5X2OaBHlW9pAEgvIj14kl7oVT4F23xF0z7SJqI5OySnW62DTnKWeir2cTlyZPxX37UCf6NAvkWmoOoXlBGnV9XDkZk87VlBwsN+me8dhtDbm9WSBGs0REiCfTt/9UZN9837LSYi/6UvQp+On8SrWfvfCJyaqaD535Eb43Sl2PulzLvOYwfWtv6bPH/jrkM6n9JM6nnGf9lfVkGDJKNcVc1PEBPu33Kd1DuxOZHomnxpNhfw2zbnOzczH0T4cUcdu/KYQArEl0ewqaDoTP20PkDshMBI/Ayh5VudKxvi93tgxm/al4Bny8jZ3T+4tksYKaizMBWIT4i03L4b+T8YzuVA93F8dbfXpuEcEkBThxzSas2tT1Rm8ycy4+k082nMNkkVArFdzRIhiVUkHjQFsC6vwo3tJaGp2hUChoGiz/slskC2si1xTbf9qWaaiVagJdA+ka0hVXtSvHko5htBgZFD6IRt6NADh7/SxLziyx7hebGesgAHde24nerMdT68mRhCMMDh/MuivrAPjs0GfWftEZ0fx65le6hna1Hhtk69fqS6sxWozc2eBOGng1YGv0Vo4kHgGgTUAb+tfv7/Q9XEi5wPqo9dzV6C7CPG+8ostf5/+64X3yGdV0FJ8f/pxjScf47NBneGg8+L9T/+e0b34wiT196/W1+gbO2zevVOdMykli2pZpDAofVCgNjb/OH3eNO638CwcMtYm0Cfh79lqI94HAdMjVgFEF19VL8BtfvTNiVARCANY0/BpCSFuIOwYfNoHnDssVQ2owrw5twZazCRjNEr3mbeKte1vzUPcGlT0sgaDs0Zfe0jX2mz1Wn9l3RrRx2JaeU7gyR0EKlnbLNZrxzYvMzc/J6eOmReUkOKtRgLvTY3YJ93XaXhL74/aX2Gf9lfXW5Z9OOVoLN0VtYvndywEY/c9oh20xWTFWnzeD2cDTGxzTknx++HOn5/v93O8AqBQqjkw4Ym1/edvL1vF+dugz9jywh6lbpmKymKz9t47d6tR/8OXtL3M+5Tz7YvexePCNBTKcSDrBrF2zbmgfe3rU6cHnhz8nITuB744Xn00iPtvRFUGlUFHXsy6emhs3xa2/st7hf5ePq7roB/mn19j8SrufLWxhvp78kxCApUAIwJpIxDBZAAIc/AHunFOpwylvGga4M3dkW15cLj+Vvr7iBGuOx/LJ2PYEe4m8YYIaRAEL4N36t/mniK5R17MBWHcynndGtEGpgPyczc789wpyKdFxKjE918iTfRqx77Kt8sfw9nWc7juiYz3eXn0afV4ASAN/N4a0DuWJPjf3MGofdTq963Q+PvAxBkvpU9+cuX4GKFxVAhynM0szbemmdiPblG1dt09KDIXFakpuCiaLCbVSjU6lI9OYydWMq04F4PmU8wBWv7sbwdm0bEH6hfUjKScJSZIIcgtiU/Qm67bWAa2Z3WM2F1Mv8vPpnwvt2yWkC0qFkkbejVApVDT0bkiuKZfYrFjaB7XHS+uFp9ZRADbxaUKfen0wWUxolBqMFiPXc6/jp/Mjwi+CV3e8WuRYg92CHdb/vOdPtl/bzmf7P3ZoX9NJwZCDjv9XbQNhACgNQgDWRHpNhTOrIO447PwMjLlwx6wKrxFakYzuVI+72oby0u/H+OdoDLsuJtPt3Y28MKAZU/o3cWqlEAiqHQUE4HGpZEGVlKknS2+iYMGOyKQs/Dy0eOk0Tve7muLo/5eeY6J9mI9D293tnAtADxc1r9/VktdWnABg5eTb8HZzfp58so3ZZJuy8dB4kG5IJ8gtiCxjFi4qF6vFaUjoHYzWduewb1fWJe9AZZYI1QZy1ZyEe45Elqv8PQ9MlUj2Aovd995gNnAx9WKh8x6KP8SQhkNwU7txMrnkKesPb/+QSRsnObRdSLmAUuEYU6kyS2hMkGaQ/2d+Gh8aKwI5m3yK45d3U18ThKtJicrXlwxTJu5qR6upJElF1j6/nnsdF5UL7hp3az6/4qJnAWb3mM3oZo7Wz/v+uc8qjgHr9oICsJV/KxYNWkRJFBSAH97+YaHEzfZ8c+wba/7AghQUyE19m9LUtyk/bXIUgIsHqhhy0NGiHfyqYzCJwDlCANZENDq4/xf4sjOYcmHfNxB/Eh76E9QulT26ckOnUfHp2Pb0bhrAy7/LFtBPNpxDq1byTN+ib0ICQbXBiQ+gxSKhLOEBp9XsdYXa+n64BSg6Jcy11GyHdS9XNZ4FxGJx/rYedn6HXq7F/9TkmHIY+udQknNtZe7eve1dZu6YSUPvhlxKu4TKLDHmje1cur6OR9RKDjyl4H9/mKmTkcTKOzy4f2Uq39+pRGuGhzZZONRIwbyxtipBY1eNdYgizmdT9CYHS1hJ+OoKT2OPWDmiUNsH35upcx2u9JATaE/+JZ0WZ/MsjPM/4iofAaAa3I9RHbYzsIFjWpY/z//JqGajCh33dPJpxq0eh7vGnbWj1rLk9BLmH5lf4rglCls/tSqtk56y/539/6K46Vh7CgrAoqJ48wlyCypSABYlfgNL4QWhqeP8wUTgiEgDU1PxCYPnjkCPvBxXV3bAikmQlVSpwypvVEoFYzqH8dF97axt7609Q1RydjF7CQTVhAzbFOWDBtnKUbBe7o1SVAqllCw5UGRAi2CaBXvw1fiOhYRcgIdzAQEwuHUIXcJ9eaZv4yJ/zPO5kn7FQXCA7D8nIXEp7RIAQangnjetrTJZ6JzkTZM4cMsycf/KVAAeW29h0Cn5IbfjJckh74wz8VccPi4+Dn99XXx5vuPzeGgKz6T4uPjg4+KDuybPiidJ1EsGpQRJ++Tciy3OOr8HmdduBuC/K/85tBc13tPXT2OWzKQb0rmQeoEDcYWni/+45w9a+LVgRtcZTG4/mbYBbRnacGihfrO6z6Kpb1M+7fepQ/sLnV4gyC2I9oHtaerblJndZjodS0G6hnSlsbf8sH1Xo7tw0xSf8Pu1bq85bZ/edXqR+0wOuc+6rH1/FuFeDfluoJIrARqywurjM2YMSpeaa+goS4QFsCbjFQqD3gG1DrZ/CCd+l1+vJYK66Bt3TWBUp3q0CPVi4g/7iE/X89TPB1n97G1WS0m+L1BJP0wCQZUiLxH0HfoPuCjJ0b8ZuUa8XYufXrVnxeRebDwdzxebZIHhqXP+M5AfKTy0TQgjO9YDHH3onurTqNjvj06jYvnTPUs1Jmf+awUDDQLTHC1YL/uNIZmvC+1XzyUIA7JVyTMHMpxokDsb3Mnk9pO59+97nY5n17hdhaxZ+STnOArVf+79h3DvcACOJR5j/L/jcbELsr6Scw0Xl+JT4SgkCanAtSwqRYp9e0xmTKF0NPc1u49mvs347e7frG0F6+3m09yvOX/e82eh9uFNhjO8yfBix+yMQLdAVty7otT9G/k0Yma3mbyz9x0A/hv1X4lJnFubgkkEvEeMoM494/iHcUzOOcTTbrHMvrslnXs1vOFx11aEAKwNdJ4oC8B8DiyC7s5vCDWJlnW8mDeyLRN/2M/p2HSe/eUwJouFdSdtPyzvjGjNve3rFkqTIRBUOXJSQS9PAcdI/tbm/07G8+htjj96/7en6ETwrhqVw/qbK08xvpvsNP/9jsss2HKRMD9XDkelAjj4CNoLPq8bEJ3FseHKBl7Y8kKJ/QILzH4nLygs/gAM12zpRALTnAvAuh51i5yeVFok0me+RdyePWgbNCB45qvoWrSwbs8Xhu45EpNWW1Cf+5aLR46BJOHqomZeiolEb9t1Onn9JLeXMPHS+ZzEoSYweZWFM/UU/NdJ6SD0jGYj07dPJy47zkGEf3LwExJzEh2O5a52I/b119HUr0/AE08Uf+IqgBYVz640UzdZIufvyVzGUQi7demC1913Ez9vLlJ2DsY42QpuP83rlfcQY590XFAy4levNuBdD6ZHw4KekBYNu76QcwbWAutXv4ggXhjQjE82nGP18cJO0jP/OsHMv2RH9bVTexMRIjLIC6ooeVVAspUe5GCLbt91MamQAHw9L/iiIFqVkhBvHUPbhFotgAazhejr2dTzdeWtVbK/WlKmLdNuQQthmJ8r0ddzGNQq5NbfE7DoRMnBBQCB6aVMKG2yiYDANIlLoYXvc6Huobhp3HDXuBdKnDwoPpj0f+TYalNiItd/+j/qzH3Xuj3fb+6evRa6nJfIKJB7rxHQKM42Vq0ROlwqfuzdzkpoTXDbKYnbTkn810lJptFW4uJgwsFCU8Rgs5L66fwwS2bS9Gm0ilGTulxOUVMdBGCn6z5kn5Svjz7udKHtuadOYYiKIufAQYd2XWtbfsD8h5T0nNLltxTICAFYW9B5weR98F4DSL8Kx3+HtveVvF8NYEr/JgR4ajkSlYpZkohLy8VNq2bDaccppgcW7uXFgc3pFxFIqLdIJi2oYuQFgGSrZAtUsJcL8el64tJzHbrlGBzTknw3oTNerhqMZgv1/dzwdtXg7aohxEtn3Tc+PbfIqeCClr7Vz/UmKUNPo8CyySqQnwA43CucyPTIIvt1lhoARW/PJ+jF/5GxeQs5Bw/yXOg46o2dxJX0KxxNOMpHB+XAizoesvVozcg1ZBozyTXlkpSThFqppsHhOJJ52Xo80/XkQufYMmYLV4+/BmxxaPceOZL0//5DyrSJNxcjhGSoAJswrfP9t8Q89qR1PSBdwivbJlRdDFKhqd6C9Avrx5jmYwBo5tsMpULJ1YyrNDqRTL4NVLJYUCirtqu/z3UD2YC6eVNCX3zJYVvMq69iTkwie7+cWsfv0Udx79EdlY+vowDM+4yWNsG5QEYIwNqE1g06Pgz7F8Lmd6DN6FphBVQpFYzv1sA6zZVPjsHM5rMJbDmbwG8HrnI9y8Crfx3HVaNiw/9uFxVFBFWLyO0AZCvlQIM7WgSzdG8UJ66ls/pYLEPbhKBQKApF73Zs4Iufu83nV3/xIsZr1wj0dLEKwHf/Pc19nW2VJ+pmJtIl7jRZGh2eyl4Ox/PSabiYfoKjiUraBbbDGYYrV9BfuIDnHXcU+5ZyTDlcz5XzCo6LGMfcfXOdd5Qk6u+OBEDbpDGGC3I6F/9nni40Fex5552Y09LIOXiQoHQFfjo//HR+hLiFFBKAvjpfa1RvU9+mAKQr1jocL2vrNlJ++w1LtnxddS1boo6NRbf9MBYcce/RnewDBzDaCcC791qoG+/Y07tXb+wlXb0kyNHarIQjd1lIqBfJb01+ZUjjYay5XLgKSp96fbit7m0Obf5aX679ZZtOT/vzTzwHDkTlVb4zGxmbN2O8FoPnwDsxp6aSsWEDaj9/fEaNxBgXR+6ZM3gOGIBCoSBrzx6ydu5C5S9XX0lf/S8A7hEt8Ojd2+G42gYNyElMwpJ3PT369MG9e7dC5xdTwDeHEIC1jTtmyQIw5TJ80xue3lHZI6o0XLUqhrYJZWibUN68pzX3L9zD0ehUcoxmes3bxLyRbbi/a/3KHqZAILNRTujuZpItQ40DPazJnScvPcTyp3vQJdyPmFRHi6Cb1tHn79KwuwDo+8w7HEeOljwUlcqhPJ8/gBn7/4/GabJEUa1tAg8/ZN2WbcxmwpoJAOwbv89pipCLgwYDUH/xItx79CjyLeXnrnNTu9HQ2zaNHeEX4ZCfrmWUTRy5tm9vFYBunTpT0D6nDg21+ocZY2wyyz6oI8S96Olrc3rhPCNxs2YX2d8ebVgYKk9P7O1QTQt6njh56PbKgU4Xbe9xxG4JyGBB1ltcvv8Ke2L3FNrHWTqajE2byFhvq6oR+9rrZGzcRNiCr0o1/ptBf/EiV5+R8yLmHDpI1t59mJPl/4olK4uETz8Fo5F6Xy9AFxFB1CMTnR5HU7dwSUNtWH2HqV9tWD2n++ZbAFOzhQXwRhACsLah8wL/JpB8QU4UnRIJvuGVPapKx1Wr4renurP9XBKTlx5Cb7Iw/c/jfLbxPM/0bcxD3RuIiGFB5WGxWZACzLLrQoCHlrkj2/DKH8cBOB+fSZdwP7Ls0sK8N6oNugJBH/mM1SRwtmVn/jvl6AqhkCzUz7BruxLpsD3dYBNIcVlxDsINHCOFs/btK14AZsrqqI5HHbqEdOGx1o+hVWkZ32I8b+x6g10xu2gf1J7hCVpgAwDBL76I0tUNl0YN8bitF0Evv4ymTiim5GRUHh4otVqnAtBD68HsHrNRKVR4aYu2iFkyMp22uzRtgiH6KlJurtPtAZMno2vXDqVX0eXQ/B57FO/hcnSt+bt5bP+/99D5BWI8J1cAaZAgUd8upqPrOYlfrm4rdBxvF+9C1j8Aw4XCqWMyN28ucjxlgf6CLbl2zomTVvEHkPbPP2CURVnWrl0odTbfVYWLCyofH0zx8mfNWe4+/yeeQKFWYcnV49q6lVORCFgrPsWm5TjdLnCOEIC1kft+hK/zpnU+awcjvoEWd4PWef3O2oKLWsWAlsH89lQPXlx+lPMJmcSm5TLr75PMXnmSMZ3C6NHYn5RsA+tOxpGQruete1vTq0lAZQ9dUNMxFf5hc1ErGd6+Lidj0vlp9xWupWaTkWu0ToP1bhrA2C6yBVsyGpHMZhQqmxhUJyXy9eNtiUwzcM/cNZgVSnI0OrZMbE323zY/QmNkpMN5Mw02gRSdEV1IAJpTU20reT/+kskEKhUKhUIWiCYTCo3G6v9Xx6MOaqWaqZ2mWnf9pN8n1uX4A/O4Dvg9PAGVjw8hM20lxPwfLWxRyhcKhitXMKemovLxQTIaGRl+DwqVCsliAYsFhVr+CTSnpaF0dUWh1WJOk30tfR98EJWXF0lfydYzr7vuJn3tWvSnCwcqAAQ+K+dcVXk4F4BKT0+CX7L5uLW+bTitb5PFYJsfbbWa/1nfzmr10pggJf4Krkrw9A0mIUcuiffube+ilVRYcnJQurpiMchl8ewjoG8UU0oKSBJKNzcHoVYQyWhEodFYl3NPnbJuM0ZFOfTNv5YAUk4uhkhbdLquRQtcmjaxBqw4E4AujRoS+tZbJY69nq9shY5JzS1VYnSBjBCAtZGQ1vDoOlg0SF7/6yk4tgwe+qv4/WoJ7cJ8WDe1D6uPxzLzr+Ok55qQJFh2IJplB6Id+j74/V6Gt6tDv4gg7mlXR1gJBeWD0SYAcyX5x1erlp37831V52++yPzNNmtM/tRv6p9/EfuqLJjUIbapz5SlS0lbtQrXnj1ZvnYtZoWSDzqNI3ukLQACZMuN4eo1tPXy8g4abcEJkzdOZv3o9Q5TqsZrNqtb8nffo2vblvh583Bp2Ij6339H7PQZZG7dSqPVq6xTwMVVjLDk5nL9xx+B0ld4yO8n5eRwrrujBVIdGoomJARzaiqNVv5N0sKFJH3+BSp/fxqvXkXyt98CoPT0QFPXdr789aIEoLVfERZAS6Zzy6I9oe6hDgKybaTE95/JYnzzGDcW5BU08sWdi4OHYEpMpN6XXxDz4ktIgCYoqMRzOCN29hukLlsGgMLNjYa/L8elUeEyg+lr1xHz8svUef99jNeukfDBB8Ue1xRrm/9OXb4cli+3ris9PR0sekVZ90pDiJcOlVKBwWwhMVMvasCXkqodHiQoP+p3l/0B87m4CXZ+XnnjqWIolQrubleHo7MH8tqwFvRrHkiXcF983DR4uKgZ1bEeHev7IEmw4kgMz/96hA5vrWf+5gtOC84LBLeEwZaq5EnjNEC2WAPU9XUerJSf7y9f/AGY4uIc+ljS08laKwc9qCQLD8fvt043b2xmm2LMOWTzwyqYoHjZ2WUO68YYRyvUteeexxQTS9bOnVgMBtL+/htzairpa9Zao1vrehT9428/xeh+W+FpT2co3YquQGGKjSXn8GEMly+jv3iRzC1bATAnJ5O1b5+1n1unzg6iROXphdegwSi0WlTe3qh8fKzb6n5hu3d69O2LwpkFrZj7wsMtH8ZF5cKk9pPwuW+00z5dL6vpF9aPtoFtaZiqwXj1KpJez/XFizGnpWFJS0N//nyh/RSuJQezZW7ZYhtmdjbZ+/Y77Xdt6lQkg4FrU6eWKP5KIvjll/Do0weVtzeuHTqgqefcv680qFVKQvJEX8Ea1oKiERbA2kzv/0GbMfBpa3l9/euwez5M2g1ufpU7tiqCQqHg8d6NeLy3/DRstkhIkoRapUSSJDaeTuDf47GsOHKN1GwjH6w7y/n4DD4e015MQwjKjjwL4HXJg20WOfLWpYAFsCCu2hu/vYdGn5OP3bw5U/5eSMz0GaStWOHgS1dQAJosjpGX9hbAgtgLFKWrq80CWEz1h3xBqWvbFpfGZVvTWzKbHd6b/qz8/lGrce/VE2O0zeKv9HDHs18/vO++q9hjet15J15HDlvXT0e0KKa3zItdXuTFLi/KK02gxRmblTFz+w6in3gCv1Qzn/eXhWa6XaBHbv6Yi0BZggCUDAZMCfLUsuegQWSsW+dwTUpDs/37iJr4KLkn5PyTvuPHE/K68zJvhfbdWzjA5Wao6+PKtdQcrqZk06lB4QAZQWGEAKzt+ITB7FT4YRhc2QmZcXKlkD4vVvbIqiQqpQLyMtUrFAoGtAxmQMtgHr2tIZ+sP8fGMwmsOBLDqmOx3NOuDqfjMri/SxgTeoggEsFNEnccvr4NSYL0ozoG6fYSkJOK7p9rJLsq8TpynABVV5JcfRx2c9WoyNiw4aZOGeWey4KtLzFAk0x94PqyZSR++hnGTq2I809lYrKZP3sqeXijBb/Te0m59BtpK1eSc+AgUmgQRX3SY2fYrJGxM2fSs72G9iozYdFbubD1Y4zR0bi2b48kWVC5e5C1a5e1v/10bFkROcrR2pY0fz4AKk9PFAoFGrspc8lUOSlG8t+3ITKSqMceByBr507rdvugC2eYr1/n8tixqHx8UKDA94FxpP+7BlOSXJ5EMhhAklC4uODapjUZ69aR9vffJH/7Lbo2bVB5eaHQaNC1LFrIqvKmc/MF4K1M594s9Xxd2RcJ11KFBbC0CAEokNMSjPk/+GUsXN0Pm96CoBbQsA+4FB3RJrDRuq43H49tz9RfD7P5bCImi8Sfh2XLxeyVJ/li03mGtA6lfZgPWQYT7lo1A1sF46krm3JaghrMdwMAyE3RkHNGy1Ty/KjOrichr8s9bVQsauyYc0+jVpD4+RelPo1JCeq8YOMduijWRl4j2WzhJcAcm1d+6+BJ8qv7Nkgw0zIaOHWcuBXHrcdRxCZQFPpzjtaq24/kpe04uMKaOiXnyBGn+5bW/y8fnzFjSP3tt5I7OiHfj0+hteVP1DVrdlPHys9V6HVX8ZbDotDUrYvSwwNLZqaD8CsOt65dybabzs49esy6nLl1q9N9XJo1wyXPWpkfmZt73PZ/tZ8mtid/KlwX0ZyMdevkYzW/uWt1K4R4y1PA8WnOyaCe6QAAGW1JREFUo7QFhRECUCDj7g+P/Aufd5Arhfz6AOh84OGVEOo82avAEW9XDYse6cKy/dHEpedyPcvAvsvXuZSURVKmgf/bc8WhRmvjLe48278p97SrI6aLBUVjkn/QJFPRn5H762uof3dL3vjHFpGZlm20RuT6PjCOtFWrseTluNN7uPDhMCMzl8mKz6iC1x5SMUzRhv2pxzjcWD7XgWYKLB+8Svpn8/G56liMt6VjPJSVBG/4fqASZZ0Q2sRpuZgVhdoCc257G4VKSeQbr6O9iZJdNyoAg1+dgeeggWjq1CH32DEsuXriZjvP5+f74IO4NG5E3JtyrkWlmy0jQpPNmzAlJaFt0MDpviUROGkS7l274tq+/U3tr3RxIfzXX8g9eRKQU9skfla8v3bdjz4k++AhVD4+RD3yiNM+bl274jNqpLyiUODWrRvqoCDCvvuO5G++sVXfeHgC13/8qdD+AVOmoAkNwT0vebPfxIloGzdG6eaOe8+ehfqXN97WaiAiGXRpEQJQYEOthXs+gz+egJzrkJsK3/QBn/rQbhx0eFBeFhSJQqEolDw612jml31RfLjuLGqVkogQTw5cSeFiYhZTlx3h7dWnGdO5HoNbh9C2nk/lDFxQ5UjIyGXNqXMEmDxpkmvE+6q2yL7m1Su5+5VnecOuLTnLgClP8PlNnCgHYPz+h7zNzczRRkqwq2VxOVTBl5yAEFtsoKRQcKmlDwYvMz6lHHeyJxxuokSnSicm0JfYLPl429qpUKDgWg8fbt+UWMJRCqO+wQhXpU6HRy853ZVLQzlVTcbGDWRt216or3uvnnj06WMVgFKObRpRExqKJrRoH8WSUGg0xeZCLA0uTZrg0qQJINcnzheAfo89yvXvC9dSVgcG4jVYzvLgM3asNcLXHreuXa05Ce3xuK0XGevWWgWgz333ORWAPveNRhMcbF1X6nR4DRx4E++ubMhPBp0hysGVGiEABY40GQAvX4KcFPi/ERB7BFKjYOt78ss3HLzD5JJytaSU3K2i06iY2Kshj/QMt/oBXk3J5r21Z/nnaAxJmXq+2nKRr7ZcJMBDS/swX94Z0VqkMqjlXEzI4uNjE/n1d08sQEoJ/Q88Ogpa2yL763ioIS9p8fHcS4TZRa1maEyAmmwtuBngbN2iv8ev7niVcd5mWpZy3FFB8rFyzbnWIA+AmTtmAjBMYeH2Uh7LHnXArefbtI/ctUdbr55jjsTAwFs+V3mhsrsOpbEqauo598crqh1AGx5u61eE5bWqXSOvPHea9BxhASwtQgAKCqNQyFHAj66D1Cuw92s5MATkyiEpkXJd0jUvw8R/If6knETaYoJG/cClbIrE1zTsg0Dq+brxxbgOzBzagj8PX+VodCrrTsaTlGlgw+l4tp1PpHsjfzxd1MSn5zKwVTDD29cVorAW4e2qwT9bX2yfyCAIz3O5q38hnfffaMuv+6JoHOjBs11DyLezbbu+n8mjR3P48FqSUmJY21n+LM5+UMU9+xX80tuWoqRtQFt8dD5ss6tA8VcPJb1OmQmymwVWdOvI2ehDZLoqUCtVnA0yE5ABv/aRLX4968jTgLtidtExqCM6tfzZ1ffTE5UZR/29NncITViYNeLWtX17NHVC8R4xkvRVq1AHB4PFfNNTqPb4TXgYS2YW+nPnMCUnI+Xk4PfYo2jzrGt1P/uMtJUrCXjyiVs+V3mhUCgIeWsOhshIPAcMIHDq82Tt2Yvh0iWQJIJecgzg8x4+HP3p07g0a47S04PMTZtRB/jjOWBAkefwHT8eQ+QVPPr3Q+nmRtD0V0hfs8bqSxjy5psolFUri5xnXj3gdGEBLDUKSSQtu2nS09Px9vYmLS0Nr3Iutl3pWMyy6Eu5AgmnZFFYFO0egO5PC9/BGyQqOZsTMWl8sekCp2ML1yMFCPBwQakACejcwJdGge70bR5E6zreuBao+VobqK7fwdKOe+ycVryx1OJ0m6FHOyb0OcGv79mqdtinD0m5cJq4u0aSo4XNix7nf53/x+s7X2fFhRUOxxndbDS/n/vdun78Ydnxf9mZZby9921re6NYiXk/yOfyGXc/mpcn0++3fgCMajqKP87/4XDc/OMURfQzk6xlyuzHLRDcDEeiU7l3/k7q+riyc3r/EvtX13tHWSIsgILSoVRBo7629ZA2sOU9SItGliN2HF0qv+p2lhNO+4bLlsGsRAhtW+tLzhVFfX836vu7MbBlMPsuX2fv5et46tRcSMhk+/kk4tJzScq0WYTWnJAjM+dvvohWpaSerysh3jrah/mgVirwctUQ4OFCoKcLRrMFV40KP3ctAR4u+LhpqlxaGkmSiE/Xk5ZjJD3XSFq2kS7hfni7lX+k9Pz58/nggw+Ii4ujXbt2fPHFF3Tt2rXI/suXL+f1118nMjKSpk2b8t577zF06NAyHZPRYmTCJufiD+BEwjEsSkfRn59zTuXtbS3DleUCP5z8gR9O/uD0ON5ab6ftBXPzZbnYljV16uCptWUIaOFXcq67gkj64q2bAsGN4JVnAbyWmsO5+AyaBYsMFiUhBKDg5ujwoPzKTARjFmjcQZ8O59bByb/kdDLXDsgve1x9IeIuaDUC6nWRxaCy9lmuikOtUtKzSQA9C9QYztKbuJCQiVqlICFdz3+n4lh7Io6UbCMGs4VLSVlcSspi18Xi84IBaFQKtColJouEh4uaQE9ZKPq4aXFRK3HTqjCaJWvlE4VC9rHRaVToTWZMZokgTxeMFgmLRSI914iHixqdRoVaqUChUJBtMJFtMJNtMJNjMJFlMJNjMJOea+R6lgFJAr3JTK7RwqXETLIMZswWx4eJ35/uQefw8k1KvmzZMqZNm8bXX39Nt27d+PTTTxk0aBBnz54lyEngwa5duxg3bhxz587lrrvuYunSpdx7770cOnSI1q1bl9m4sgxZ+GY432ZRwK4WsoBf21HB4EOO182+BuuJ8KKFvpfWi/EtxtPcrzkzd8xkXu951m1tAtrgp/Pjeu51AFJ91SjCgiEhGffu3XFRudA+sD2p+lSGNRrG4pOLrbV9Z/dwHm1rT+C0aWQfPkzAU0+V2FcgKAn7ijjbziUKAVgKxBTwLSBMyMVw/RIcWQrpMRC9D5ILlygCQOMGPg0gOwnMBnAPhLBu4N8YXLxkcahU5700eesquZ9aBwFNa32uQotFIup6NhcSMjkSnUpqjoFrKTkYzBZSs42kZMmF4hUKBZl6E2k3kYKjolApFXi7avDSqfF21TD7nlZ0rF90Vv+y+A5269aNLl268OWXXwJgsVgICwvj2WefZfr06YX6jx07lqysLFatWmVt6969O+3bt+frr4txjbjBcUfFniKr3ygAJj+jItVDFn5mJegMkOsiC7uJrSZyz+yNGC9eKnSMTB1k/P0l7YM7WNtc1a5YJAtuajdMkgmNUrawGi1G63I+RrORDGMGbmq5tJoLaiSj0VpdwiJZkCQJlVKFyWIiy5iFm8at0HGKQjIaUWhELkxB2fDO6lMs3H6ZR3qG88Y9rYrtK36/hQXwhqd+BKXErxH0zysFZNLLgtCnARz/TbYOnl0riz5jNiTa+f/kpkHyhRs7l2uehUjrIRfpUChlYZkvHFUa+a9aJy8XWaeAUkQ1l7D9VqZVb/LcSiAcCFcosLp158+yW+9rtn3NkoTBZMEigUWSMFkkjGYJvdGM0SxhtkgYzRbUKgUGk4TBbEGhUGAyWzBLoMorhpKtN6NWKVAqFKBQYMmz3pnzHinVSgUqpQKVUmm3rECtUuCiVqJQKFApFCiVSrRqJa4aFS5qZV61lTxcXwLKr6yTwWDg4MGDzJgxw9qmVCoZMGAAu3fvdrrP7t27mTZtmkPboEGDWLFiRZHn0ev16O2mPNPTnft42pN5/iAKIEcLid44fD5y7aZjvV280fj5OxWAl4MVdPVpjJ/OuRVVo7CJL2eiTaPS4Kdy3Fehtv1sKBVK60dLrVTj7eJ8OrkohPgTlCX1/eUbn6gHXDpqtQC80akfwU2idpEriwB0ekR+SZKc4Dbliiz4vEJlAZd4BhLOQMplWRxazHJ0sdko/zVmyylqMhNkUZdzXX6B7a+gWFRAyeXhS0l5GxM7PQKBzcvt8ElJSZjNZoLt8pkBBAcHc+bMGaf7xMXFOe0fFxdX5Hnmzp3Lm2++eUNjy750CncgyYtiHw46BHVA1yrJmrfNYaxB6mLr7AoENYl6eXWxRTm40lGrBeDHH3/ME088wcSJEwH4+uuvWb16NYsWLXI69SMoQxQK0LhCUIT8yiewObQsnJy0SHJS5TyFKg0YsgFJFo2mnDzhaJL/WoxgzJGFZLGU4BFRosdEKTwqbvUYZeG1UV3eh2/DkvtUA2bMmOFgNUxPTycsLKzYfULb9eXk6NOoXXTMv2MSkWmRZBmz0Kg0mCwmMg2ZtA9qT8fgjpgnNyW3ji+Xzu4lrUUYqhw96vQcbh8+GheVS7HnEQhqCi1CvXhxYDMaBYpUZKWh1grAm5n6uZlpHEE54+ojvwSCmyAgIACVSkV8Xu3TfOLj4wkJCXG6T0hIyA31B3BxccHF5caEWN0Og6jbYZB1vU+9PkX2VXl60mDCkzTgyRs6h0BQkwjx1jGlf9PKHka1oWplcqxAipv6KWoqZ+7cuXh7e1tfJT3BCwSCqo1Wq6VTp05s3LjR2maxWNi4cSM9iijf1aNHD4f+AOvXry+yv0AgEFRFaq0AvBlmzJhBWlqa9RUdXUQ1dIFAUG2YNm0aCxcu5Mcff+T06dM888wzZGVlWV1DJkyY4DBT8Pzzz7N27Vo++ugjzpw5wxtvvMGBAweYMmVKZb0FgUAguGFq7RTwzUz93Mw0jkAgqNqMHTuWxMREZs2aRVxcHO3bt2ft2rXW2YGoqCiUdmWvevbsydKlS3nttdd49dVXadq0KStWrCjTHIACgUBQ3tTqPIDdunWja9eufPHFF4A89VO/fn2mTJlSqiAQkUdIIKhcqut3sLqOWyCoKYjvYC22AII89fPwww/TuXNnunbtyqeffuow9SMQCAQCgUBQE6nVArCkqR+BQCAQCASCmkitFoAAU6ZMEc7bAoFAIBAIahUiClggEAgEAoGgliEEoEAgEAgEAkEtQwhAgUAgEAgEglqGEIACgUAgEAgEtQwhAAUCgUAgEAhqGUIACgQCgUAgENQyan0amFshv4hKenp6JY9EIKid5H/3qltBI3HvEAgql+p67yhLhAC8BTIyMgAICwur5JEIBLWbjIwMvL29K3sYpUbcOwSCqkF1u3eUJbW6FvCtYrFYiImJwdPTE4VCUWS/9PR0wsLCiI6OrrU1B8sCcR3LjppyLSVJIiMjgzp16qBUVh+PFnHvqFjEdSwbatJ1rK73jrJEWABvAaVSSb169Urd38vLq9p/aaoC4jqWHTXhWlbHp3dx76gcxHUsG2rKdayO946ypHbKXoFAIBAIBIJajBCAAoFAIBAIBLUMIQArABcXF2bPno2Li0tlD6VaI65j2SGuZfVA/J/KBnEdywZxHWsWIghEIBAIBAKBoJYhLIACgUAgEAgEtQwhAAUCgUAgEAhqGUIACgQCgUAgENQyhAAUCAQCgUAgqGUIAVgBzJ8/n/DwcHQ6Hd26dWPfvn2VPaQqw9y5c+nSpQuenp4EBQVx7733cvbsWYc+ubm5TJ48GX9/fzw8PBg1ahTx8fEOfaKiohg2bBhubm4EBQXx0ksvYTKZKvKtVCnmzZuHQqFg6tSp1jZxHasX4r5RPOLeUT6Ie0ctQhKUK7/++quk1WqlRYsWSSdPnpSeeOIJycfHR4qPj6/soVUJBg0aJC1evFg6ceKEdOTIEWno0KFS/fr1pczMTGufp59+WgoLC5M2btwoHThwQOrevbvUs2dP63aTySS1bt1aGjBggHT48GHp33//lQICAqQZM2ZUxluqdPbt2yeFh4dLbdu2lZ5//nlru7iO1Qdx3ygZce8oe8S9o3YhBGA507VrV2ny5MnWdbPZLNWpU0eaO3duJY6q6pKQkCAB0tatWyVJkqTU1FRJo9FIy5cvt/Y5ffq0BEi7d++WJEmS/v33X0mpVEpxcXHWPgsWLJC8vLwkvV5fsW+gksnIyJCaNm0qrV+/Xrr99tutN3FxHasX4r5x44h7x60h7h21DzEFXI4YDAYOHjzIgAEDrG1KpZIBAwawe/fuShxZ1SUtLQ0APz8/AA4ePIjRaHS4hhEREdSvX996DXfv3k2bNm0IDg629hk0aBDp6emcPHmyAkdf+UyePJlhw4Y5XC8Q17E6Ie4bN4e4d9wa4t5R+1BX9gBqMklJSZjNZocvBUBwcDBnzpyppFFVXSwWC1OnTqVXr160bt0agLi4OLRaLT4+Pg59g4ODiYuLs/Zxdo3zt9UWfv31Vw4dOsT+/fsLbRPXsfog7hs3jrh33Bri3lE7EQJQUGWYPHkyJ06cYMeOHZU9lGpHdHQ0zz//POvXr0en01X2cASCCkXcO24ece+ovYgp4HIkICAAlUpVKFoqPj6ekJCQShpV1WTKlCmsWrWKzZs3U69ePWt7SEgIBoOB1NRUh/721zAkJMTpNc7fVhs4ePAgCQkJdOzYEbVajVqtZuvWrXz++eeo1WqCg4PFdawmiPvGjSHuHbeGuHfUXoQALEe0Wi2dOnVi48aN1jaLxcLGjRvp0aNHJY6s6iBJElOmTOGvv/5i06ZNNGzY0GF7p06d0Gg0Dtfw7NmzREVFWa9hjx49OH78OAkJCdY+69evx8vLi5YtW1bMG6lk7rjjDo4fP86RI0esr86dOzN+/HjrsriO1QNx3ygd4t5RNoh7Ry2msqNQajq//vqr5OLiIv3www/SqVOnpCeffFLy8fFxiJaqzTzzzDOSt7e3tGXLFik2Ntb6ys7OtvZ5+umnpfr160ubNm2SDhw4IPXo0UPq0aOHdXt+CoKBAwdKR44ckdauXSsFBgbW+hQE9pF8kiSuY3VC3DdKRtw7yg9x76gdCAFYAXzxxRdS/fr1Ja1WK3Xt2lXas2dPZQ+pygA4fS1evNjaJycnR5o0aZLk6+srubm5SSNGjJBiY2MdjhMZGSkNGTJEcnV1lQICAqT//e9/ktForOB3U7UoeBMX17F6Ie4bxSPuHeWHuHfUDhSSJEmVY3sUCAQCgUAgEFQGwgdQIBAIBAKBoJYhBKBAIBAIBAJBLUMIQIFAIBAIBIJahhCAAoFAIBAIBLUMIQAFAoFAIBAIahlCAAoEAoFAIBDUMoQAFAgEAoFAIKhlCAEoEAgEAoFAUMsQAlBQbVAoFMW+3njjjcoeokAgqGKI+4ZA4Bx1ZQ9AICgtsbGx1uVly5Yxa9Yszp49a23z8PCwLkuShNlsRq0WH3GBoDYj7hsCgXOEBVBQbQgJCbG+vL29USgU1vUzZ87g6enJmjVr6NSpEy4uLuzYsYOLFy8yfPhwgoOD8fDwoEuXLmzYsMHhuHq9nldeeYWwsDBcXFxo0qQJ33//vXX7iRMnGDJkCB4eHgQHB/PQQw+RlJRk3f7777/Tpk0bXF1d8ff3Z8CAAWRlZVXYdREIBEUj7hsCgXOEABTUKKZPn868efM4ffo0bdu2JTMzk6FDh7Jx40YOHz7M4MGDufvuu4mKirLuM2HCBH755Rc+//xzTp8+zTfffGO1CqSmptK/f386dOjAgQMHWLt2LfHx8YwZMwaQrQvjxo3j0Ucf5fTp02zZsoWRI0ciSmwLBNUHcd8Q1EokgaAasnjxYsnb29u6vnnzZgmQVqxYUeK+rVq1kr744gtJkiTp7NmzEiCtX7/ead+33npLGjhwoENbdHS0BEhnz56VDh48KAFSZGTkzb8ZgUBQIYj7hkBgQ1gABTWKzp07O6xnZmby4osv0qJFC3x8fPDw8OD06dPWJ/kjR46gUqm4/fbbnR7v6NGjbN68GQ8PD+srIiICgIsXL9KuXTvuuOMO2rRpw3333cfChQtJSUkp3zcpEAjKFHHfENRGhKeroEbh7u7usP7iiy+yfv16PvzwQ5o0aYKrqyujR4/GYDAA4OrqWuzxMjMzufvuu3nvvfcKbQsNDUWlUrF+/Xp27drFf//9xxdffMHMmTPZu3cvDRs2LLs3JhAIyg1x3xDURoQFUFCj2blzJ4888ggjRoygTZs2hISEEBkZad3epk0bLBYLW7dudbp/x44dOXnyJOHh4TRp0sThlf+joVAo6NWrF2+++SaHDx9Gq9Xy119/VcTbEwgE5YC4bwhqA0IACmo0TZs25c8//+TIkSMcPXqUBx54AIvFYt0eHh7Oww8/zKOPPsqKFSu4fPkyW7Zs4bfffgNg8uTJXL9+nXHjxrF//34uXrzIunXrmDhxImazmb179/Luu+9y4MABoqKi+PPPP0lMTKRFixaV9ZYFAsEtIu4bgtqAEICCGs3HH3+Mr68vPXv25O6772bQoEF07NjRoc+CBQsYPXo0kyZNIiIigieeeMKajqFOnTrs3LkTs9nMwIEDadOmDVOnTsXHxwelUomXlxfbtm1j6NChNGvWjNdee42PPvqIIUOGVMbbFQgEZYC4bwhqAwpJEnHnAoFAIBAIBLUJYQEUCAQCgUAgqGUIASgQ/H+7dSAAAAAAIMjfepCLIgCYEUAAgBkBBACYEUAAgBkBBACYEUAAgBkBBACYEUAAgBkBBACYEUAAgBkBBACYCSY+MjBmsvP1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image('yyz_cnn_plot.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGPBdUXtJHHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7aa8c9-7b42-4101-aac4-11f5ab963718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "pwd: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/268"
      ],
      "metadata": {
        "id": "-5T2TKJa7Z0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a52e4a-0f94-4a44-f15d-e35689ee1f70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yyu233/EnsembleSCA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlNeUOcE7BIb",
        "outputId": "3f6df94a-1137-419d-9656-85313ecd293c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EnsembleSCA' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSHNjyaR7HIy",
        "outputId": "03bdfb6a-9627-4805-fd36-206224326e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkUl6eDl7K0Q",
        "outputId": "c5abd7fa-b76b-4f39-db41-498b7a36e8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd EnsembleSCA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgWQbefW7SoG",
        "outputId": "d2e6f853-bab5-4035-c7b7-601f645a632b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/268/EnsembleSCA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git fetch"
      ],
      "metadata": {
        "id": "jO27V5uQ7T-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -v -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09gLMAHO7YYG",
        "outputId": "b8952285-263f-434d-f15d-221fbee6f9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mfeature/268/yyz               \u001b[m 1a82de1 Update README.md\n",
            "  master                        \u001b[m 1a82de1 Update README.md\n",
            "  \u001b[31mremotes/origin/268/feature/yyz\u001b[m 81440aa Added LSTM model\n",
            "  \u001b[31mremotes/origin/HEAD           \u001b[m -> origin/master\n",
            "  \u001b[31mremotes/origin/master         \u001b[m 1a82de1 Update README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uVdoqb373kl",
        "outputId": "c725ecd2-dc71-4900-8ee5-b540d7f5a54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M\tcommons/ensemble_aes.py\n",
            "M\trun_ensemble.py\n",
            "Switched to branch 'master'\n",
            "Your branch is up to date with 'origin/master'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -D feature/268/yyz "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYxnP2yG78JY",
        "outputId": "4129ecd6-4459-4d77-fc27-ebf692af0306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted branch feature/268/yyz (was 1a82de1).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -v -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68wdNxgM8HSW",
        "outputId": "c71dac65-c819-4d9f-96e8-5a2109403d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mmaster                        \u001b[m 1a82de1 Update README.md\n",
            "  \u001b[31mremotes/origin/268/feature/yyz\u001b[m 81440aa Added LSTM model\n",
            "  \u001b[31mremotes/origin/HEAD           \u001b[m -> origin/master\n",
            "  \u001b[31mremotes/origin/master         \u001b[m 1a82de1 Update README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch 268/feature/yyz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB5JnVtx8Jig",
        "outputId": "d7fe22df-1336-4af5-e636-fe00838f53fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch '268/feature/yyz' set up to track remote branch '268/feature/yyz' from 'origin'.\n",
            "Switched to a new branch '268/feature/yyz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git stash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUOTIUFX8NDU",
        "outputId": "20546c3f-eaf1-4b90-f994-71341c1af76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved working directory and index state WIP on master: 1a82de1 Update README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_ensemble.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ-vI17P8UIg",
        "outputId": "1fe6c36a-00bf-467c-f142-770ef1c9a914"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-24 16:42:32.278633: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-24 16:42:33.576026: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-24 16:42:33.576136: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-24 16:42:33.576157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Num GPUs Available:  1\n",
            "2023-03-24 16:42:43.588376: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 700, 1)           4         \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 700, 100)          40800     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 700, 100)          80400     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 700, 100)          80400     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 909       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 282,913\n",
            "Trainable params: 282,911\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 364s 3s/step - loss: nan - accuracy: 0.0039 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 368s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 388s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 367s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 374s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 369s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 367s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 359s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 350s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 353s 3s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "157/157 [==============================] - 58s 369ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "157/157 [==============================] - 58s 371ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 700, 1)           4         \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 700, 300)          362400    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 700, 300)          721200    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 2709      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,807,513\n",
            "Trainable params: 1,807,511\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 292s 2s/step - loss: nan - accuracy: 0.0040 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 286s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 284s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 281s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 278s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 278s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 277s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 277s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 282s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 281s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "157/157 [==============================] - 43s 273ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "157/157 [==============================] - 44s 280ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 700, 1)           4         \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 700, 300)          362400    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 700, 300)          721200    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 300)               721200    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 2709      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,807,513\n",
            "Trainable params: 1,807,511\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 291s 2s/step - loss: nan - accuracy: 0.0043 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 288s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 287s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 283s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 285s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 280s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 281s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 281s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 287s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 295s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "157/157 [==============================] - 45s 284ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "157/157 [==============================] - 43s 277ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 700, 1)           4         \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 700, 100)          40800     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 700, 100)          80400     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 909       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 202,513\n",
            "Trainable params: 202,511\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 273s 2s/step - loss: nan - accuracy: 0.0043 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 279s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 287s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 291s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 286s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 281s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 270s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 270s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 271s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 278s 2s/step - loss: nan - accuracy: 0.0033 - val_loss: nan - val_accuracy: 0.0042\n",
            "157/157 [==============================] - 48s 307ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "157/157 [==============================] - 49s 314ms/step\n",
            "KR: 0 | GE for correct key (224): 161.0)\n",
            "KR: 1 | GE for correct key (224): 161.0)\n",
            "KR: 2 | GE for correct key (224): 161.0)\n",
            "KR: 3 | GE for correct key (224): 161.0)\n",
            "KR: 4 | GE for correct key (224): 161.0)\n",
            "KR: 5 | GE for correct key (224): 161.0)\n",
            "KR: 6 | GE for correct key (224): 161.0)\n",
            "KR: 7 | GE for correct key (224): 161.0)\n",
            "KR: 8 | GE for correct key (224): 161.0)\n",
            "KR: 9 | GE for correct key (224): 161.0)\n",
            "KR: 10 | GE for correct key (224): 161.0)\n",
            "KR: 11 | GE for correct key (224): 161.0)\n",
            "KR: 12 | GE for correct key (224): 161.0)\n",
            "KR: 13 | GE for correct key (224): 161.0)\n",
            "KR: 14 | GE for correct key (224): 161.0)\n",
            "KR: 15 | GE for correct key (224): 161.0)\n",
            "KR: 16 | GE for correct key (224): 161.0)\n",
            "KR: 17 | GE for correct key (224): 161.0)\n",
            "KR: 18 | GE for correct key (224): 161.0)\n",
            "KR: 19 | GE for correct key (224): 161.0)\n",
            "KR: 20 | GE for correct key (224): 161.0)\n",
            "KR: 21 | GE for correct key (224): 161.0)\n",
            "KR: 22 | GE for correct key (224): 161.0)\n",
            "KR: 23 | GE for correct key (224): 161.0)\n",
            "KR: 24 | GE for correct key (224): 161.0)\n",
            "KR: 25 | GE for correct key (224): 161.0)\n",
            "KR: 26 | GE for correct key (224): 161.0)\n",
            "KR: 27 | GE for correct key (224): 161.0)\n",
            "KR: 28 | GE for correct key (224): 161.0)\n",
            "KR: 29 | GE for correct key (224): 161.0)\n",
            "KR: 30 | GE for correct key (224): 161.0)\n",
            "KR: 31 | GE for correct key (224): 161.0)\n",
            "KR: 32 | GE for correct key (224): 161.0)\n",
            "KR: 33 | GE for correct key (224): 161.0)\n",
            "KR: 34 | GE for correct key (224): 161.0)\n",
            "KR: 35 | GE for correct key (224): 161.0)\n",
            "KR: 36 | GE for correct key (224): 161.0)\n",
            "KR: 37 | GE for correct key (224): 161.0)\n",
            "KR: 38 | GE for correct key (224): 161.0)\n",
            "KR: 39 | GE for correct key (224): 161.0)\n",
            "KR: 40 | GE for correct key (224): 161.0)\n",
            "KR: 41 | GE for correct key (224): 161.0)\n",
            "KR: 42 | GE for correct key (224): 161.0)\n",
            "KR: 43 | GE for correct key (224): 161.0)\n",
            "KR: 44 | GE for correct key (224): 161.0)\n",
            "KR: 45 | GE for correct key (224): 161.0)\n",
            "KR: 46 | GE for correct key (224): 161.0)\n",
            "KR: 47 | GE for correct key (224): 161.0)\n",
            "KR: 48 | GE for correct key (224): 161.0)\n",
            "KR: 49 | GE for correct key (224): 161.0)\n",
            "KR: 50 | GE for correct key (224): 161.0)\n",
            "KR: 51 | GE for correct key (224): 161.0)\n",
            "KR: 52 | GE for correct key (224): 161.0)\n",
            "KR: 53 | GE for correct key (224): 161.0)\n",
            "KR: 54 | GE for correct key (224): 161.0)\n",
            "KR: 55 | GE for correct key (224): 161.0)\n",
            "KR: 56 | GE for correct key (224): 161.0)\n",
            "KR: 57 | GE for correct key (224): 161.0)\n",
            "KR: 58 | GE for correct key (224): 161.0)\n",
            "KR: 59 | GE for correct key (224): 161.0)\n",
            "KR: 60 | GE for correct key (224): 161.0)\n",
            "KR: 61 | GE for correct key (224): 161.0)\n",
            "KR: 62 | GE for correct key (224): 161.0)\n",
            "KR: 63 | GE for correct key (224): 161.0)\n",
            "KR: 64 | GE for correct key (224): 161.0)\n",
            "KR: 65 | GE for correct key (224): 161.0)\n",
            "KR: 66 | GE for correct key (224): 161.0)\n",
            "KR: 67 | GE for correct key (224): 161.0)\n",
            "KR: 68 | GE for correct key (224): 161.0)\n",
            "KR: 69 | GE for correct key (224): 161.0)\n",
            "KR: 70 | GE for correct key (224): 161.0)\n",
            "KR: 71 | GE for correct key (224): 161.0)\n",
            "KR: 72 | GE for correct key (224): 161.0)\n",
            "KR: 73 | GE for correct key (224): 161.0)\n",
            "KR: 74 | GE for correct key (224): 161.0)\n",
            "KR: 75 | GE for correct key (224): 161.0)\n",
            "KR: 76 | GE for correct key (224): 161.0)\n",
            "KR: 77 | GE for correct key (224): 161.0)\n",
            "KR: 78 | GE for correct key (224): 161.0)\n",
            "KR: 79 | GE for correct key (224): 161.0)\n",
            "KR: 80 | GE for correct key (224): 161.0)\n",
            "KR: 81 | GE for correct key (224): 161.0)\n",
            "KR: 82 | GE for correct key (224): 161.0)\n",
            "KR: 83 | GE for correct key (224): 161.0)\n",
            "KR: 84 | GE for correct key (224): 161.0)\n",
            "KR: 85 | GE for correct key (224): 161.0)\n",
            "KR: 86 | GE for correct key (224): 161.0)\n",
            "KR: 87 | GE for correct key (224): 161.0)\n",
            "KR: 88 | GE for correct key (224): 161.0)\n",
            "KR: 89 | GE for correct key (224): 161.0)\n",
            "KR: 90 | GE for correct key (224): 161.0)\n",
            "KR: 91 | GE for correct key (224): 161.0)\n",
            "KR: 92 | GE for correct key (224): 161.0)\n",
            "KR: 93 | GE for correct key (224): 161.0)\n",
            "KR: 94 | GE for correct key (224): 161.0)\n",
            "KR: 95 | GE for correct key (224): 161.0)\n",
            "KR: 96 | GE for correct key (224): 161.0)\n",
            "KR: 97 | GE for correct key (224): 161.0)\n",
            "KR: 98 | GE for correct key (224): 161.0)\n",
            "KR: 99 | GE for correct key (224): 161.0)\n",
            "Run 0 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 1 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 2 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 3 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 4 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 5 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 6 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 7 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 8 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 9 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 10 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 11 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 12 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 13 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 14 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 15 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 16 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 17 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 18 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 19 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 20 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 21 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 22 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 23 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 24 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 25 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 26 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 27 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 28 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 29 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 30 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 31 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 32 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 33 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 34 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 35 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 36 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 37 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 38 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 39 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 40 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 41 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 42 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 43 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 44 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 45 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 46 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 47 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 48 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 49 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 50 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 51 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 52 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 53 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 54 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 55 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 56 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 57 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 58 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 59 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 60 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 61 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 62 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 63 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 64 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 65 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 66 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 67 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 68 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 69 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 70 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 71 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 72 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 73 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 74 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 75 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 76 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 77 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 78 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 79 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 80 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 81 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 82 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 83 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 84 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 85 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 86 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 87 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 88 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 89 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 90 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 91 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 92 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 93 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 94 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 95 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 96 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 97 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 98 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Run 99 - GE 4 models: 161 | GE 2 models: 161 | \n",
            "Figure(640x480)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pycat commons/ensemble_aes.py"
      ],
      "metadata": {
        "id": "sJkY1nDE8aUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile commons/ensemble_aes.py\n",
        "\n",
        "from tensorflow.keras import backend as backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from commons.neural_networks import NeuralNetwork\n",
        "from commons.sca_metrics import SCAMetrics\n",
        "from commons.datasets import SCADatasets\n",
        "from commons.load_datasets import LoadDatasets\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class EnsembleAES:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.number_of_models = 50\n",
        "        self.number_of_best_models = 10\n",
        "        self.ge_all_validation = []\n",
        "        self.ge_all_attack = []\n",
        "        self.sr_all_validation = []\n",
        "        self.sr_all_attack = []\n",
        "        self.k_ps_all = []\n",
        "        self.ge_ensemble = None\n",
        "        self.ge_ensemble_best_models = None\n",
        "        self.ge_best_model_validation = None\n",
        "        self.ge_best_model_attack = None\n",
        "        self.sr_ensemble = None\n",
        "        self.sr_ensemble_best_models = None\n",
        "        self.sr_best_model_validation = None\n",
        "        self.sr_best_model_attack = None\n",
        "        self.target_dataset = None\n",
        "        self.l_model = None\n",
        "        self.target_byte = None\n",
        "        self.classes = None\n",
        "        self.epochs = None\n",
        "        self.mini_batch = None\n",
        "\n",
        "    def set_dataset(self, target):\n",
        "        self.target_dataset = target\n",
        "\n",
        "    def set_leakage_model(self, leakage_model):\n",
        "        self.l_model = leakage_model\n",
        "        if leakage_model == \"HW\":\n",
        "            self.classes = 9\n",
        "        else:\n",
        "            self.classes = 256\n",
        "\n",
        "    def set_target_byte(self, target_byte):\n",
        "        self.target_byte = target_byte\n",
        "\n",
        "    def set_epochs(self, epochs):\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def set_mini_batch(self, mini_batch):\n",
        "        self.mini_batch = mini_batch\n",
        "\n",
        "    def __add_if_one(self, value):\n",
        "        return 1 if value == 1 else 0\n",
        "\n",
        "    def get_best_models(self, n_models, result_models_validation, n_traces):\n",
        "        result_number_of_traces_val = []\n",
        "        for model_index in range(n_models):\n",
        "            if result_models_validation[model_index][n_traces - 1] == 1:\n",
        "                for index in range(n_traces - 1, -1, -1):\n",
        "                    if result_models_validation[model_index][index] != 1:\n",
        "                        result_number_of_traces_val.append(\n",
        "                            [result_models_validation[model_index][n_traces - 1], index + 1,\n",
        "                             model_index])\n",
        "                        break\n",
        "            else:\n",
        "                result_number_of_traces_val.append(\n",
        "                    [result_models_validation[model_index][n_traces - 1], n_traces,\n",
        "                     model_index])\n",
        "\n",
        "        sorted_models = sorted(result_number_of_traces_val, key=lambda l: l[:])\n",
        "\n",
        "        list_of_best_models = []\n",
        "        for model_index in range(n_models):\n",
        "            list_of_best_models.append(sorted_models[model_index][2])\n",
        "\n",
        "        return list_of_best_models\n",
        "\n",
        "    def run_mlp(self, X_profiling, Y_profiling, X_validation, Y_validation, X_attack, Y_attack, plt_validation, plt_attack, params,\n",
        "                step, fraction):\n",
        "        mini_batch = random.randrange(500, 1000, 100)\n",
        "        learning_rate = random.uniform(0.0001, 0.001)\n",
        "        activation = ['relu', 'tanh', 'elu', 'selu'][random.randint(0, 3)]\n",
        "        layers = random.randrange(2, 8, 1)\n",
        "        neurons = random.randrange(500, 800, 100)\n",
        "\n",
        "        model = NeuralNetwork().mlp_random(self.classes, params[\"number_of_samples\"], activation, neurons, layers, learning_rate)\n",
        "        model.fit(\n",
        "            x=X_profiling,\n",
        "            y=Y_profiling,\n",
        "            batch_size=self.mini_batch,\n",
        "            verbose=1,\n",
        "            epochs=self.epochs,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_validation, Y_validation),\n",
        "            callbacks=[])\n",
        "\n",
        "        ge_validation, sr_validation, kp_krs = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte,\n",
        "                                                                      X_validation, plt_validation, step, fraction)\n",
        "        ge_attack, sr_attack, _ = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte, X_attack, plt_attack, step,\n",
        "                                                         fraction)\n",
        "\n",
        "        backend.clear_session()\n",
        "\n",
        "        return ge_validation, ge_attack, sr_validation, sr_attack, kp_krs\n",
        "\n",
        "    def run_cnn(self, X_profiling, Y_profiling, X_validation, Y_validation, X_attack, Y_attack, plt_validation, plt_attack, params,\n",
        "                step, fraction):\n",
        "        X_profiling = X_profiling.reshape((X_profiling.shape[0], X_profiling.shape[1], 1))\n",
        "        X_validation = X_validation.reshape((X_validation.shape[0], X_validation.shape[1], 1))\n",
        "        X_attack = X_attack.reshape((X_attack.shape[0], X_attack.shape[1], 1))\n",
        "\n",
        "        mini_batch = random.randrange(500, 1000, 100)\n",
        "        learning_rate = random.uniform(0.0001, 0.001)\n",
        "        activation = ['relu', 'tanh', 'elu', 'selu'][random.randint(0, 3)]\n",
        "        dense_layers = random.randrange(2, 8, 1)\n",
        "        neurons = random.randrange(500, 800, 100)\n",
        "        conv_layers = random.randrange(1, 2, 1)\n",
        "        filters = random.randrange(8, 32, 4)\n",
        "        kernel_size = random.randrange(10, 20, 2)\n",
        "        stride = random.randrange(5, 10, 5)\n",
        "\n",
        "        model = NeuralNetwork().cnn_random(self.classes, params[\"number_of_samples\"], activation, neurons, conv_layers, filters,\n",
        "                                           kernel_size, stride, dense_layers, learning_rate)\n",
        "        model.fit(\n",
        "            x=X_profiling,\n",
        "            y=Y_profiling,\n",
        "            batch_size=self.mini_batch,\n",
        "            verbose=1,\n",
        "            epochs=self.epochs,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_validation, Y_validation),\n",
        "            callbacks=[])\n",
        "\n",
        "        ge_validation, sr_validation, kp_krs = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte,\n",
        "                                                                      X_validation, plt_validation,\n",
        "                                                                      step, fraction)\n",
        "        ge_attack, sr_attack, _ = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte, X_attack, plt_attack, step,\n",
        "                                                         fraction)\n",
        "\n",
        "        backend.clear_session()\n",
        "\n",
        "        return ge_validation, ge_attack, sr_validation, sr_attack, kp_krs\n",
        "    \n",
        "    def run_lstm(self, X_profiling, Y_profiling, X_validation, Y_validation, X_attack, Y_attack, plt_validation, plt_attack, params, step, fraction):\n",
        "        X_profiling = X_profiling.reshape((X_profiling.shape[0], X_profiling.shape[1], 1))\n",
        "        X_validation = X_validation.reshape((X_validation.shape[0], X_validation.shape[1], 1))\n",
        "        X_attack = X_attack.reshape((X_attack.shape[0], X_attack.shape[1], 1))\n",
        "\n",
        "        mini_batch = random.randrange(500, 1000, 100)\n",
        "        learning_rate = random.uniform(0.0001, 0.001)\n",
        "        activation = ['relu', 'tanh', 'elu', 'selu'][random.randint(0, 3)]\n",
        "        layers = random.randrange(2, 4, 1)\n",
        "        neurons = random.randrange(100, 400, 100)\n",
        "\n",
        "        model = NeuralNetwork().lstm_random(self.classes, params[\"number_of_samples\"], activation, neurons, layers, learning_rate)\n",
        "        model.fit(\n",
        "            x=X_profiling,\n",
        "            y=Y_profiling,\n",
        "            batch_size=self.mini_batch,\n",
        "            verbose=1,\n",
        "            epochs=self.epochs,\n",
        "            shuffle=True,\n",
        "            validation_data=(X_validation, Y_validation),\n",
        "            callbacks=[])\n",
        "\n",
        "        ge_validation, sr_validation, kp_krs = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte,\n",
        "                                                                      X_validation, plt_validation,\n",
        "                                                                      step, fraction)\n",
        "        ge_attack, sr_attack, _ = SCAMetrics().ge_and_sr(100, model, params, self.l_model, self.target_byte, X_attack, plt_attack, step,\n",
        "                                                         fraction)\n",
        "\n",
        "        backend.clear_session()\n",
        "\n",
        "        return ge_validation, ge_attack, sr_validation, sr_attack, kp_krs\n",
        "\n",
        "    \n",
        "    def compute_ensembles(self, kr_nt, correct_key):\n",
        "\n",
        "        list_of_best_models = self.get_best_models(self.number_of_models, self.ge_all_validation, kr_nt)\n",
        "\n",
        "        self.ge_best_model_validation = self.ge_all_validation[list_of_best_models[0]]\n",
        "        self.ge_best_model_attack = self.ge_all_attack[list_of_best_models[0]]\n",
        "        self.sr_best_model_validation = self.sr_all_validation[list_of_best_models[0]]\n",
        "        self.sr_best_model_attack = self.sr_all_attack[list_of_best_models[0]]\n",
        "\n",
        "        kr_ensemble = np.zeros(kr_nt)\n",
        "        krs_ensemble = np.zeros((100, kr_nt))\n",
        "        kr_ensemble_best_models = np.zeros(kr_nt)\n",
        "        krs_ensemble_best_models = np.zeros((100, kr_nt))\n",
        "\n",
        "        for run in range(100):\n",
        "\n",
        "            key_p_ensemble = np.zeros(256)\n",
        "            key_p_ensemble_best_models = np.zeros(256)\n",
        "\n",
        "            for index in range(kr_nt):\n",
        "                for model_index in range(self.number_of_models):\n",
        "                    key_p_ensemble += np.log(self.k_ps_all[list_of_best_models[model_index]][run][index] + 1e-36)\n",
        "                for model_index in range(self.number_of_best_models):\n",
        "                    key_p_ensemble_best_models += np.log(self.k_ps_all[list_of_best_models[model_index]][run][index] + 1e-36)\n",
        "\n",
        "                key_p_ensemble_sorted = np.argsort(key_p_ensemble)[::-1]\n",
        "                key_p_ensemble_best_models_sorted = np.argsort(key_p_ensemble_best_models)[::-1]\n",
        "\n",
        "                kr_position = list(key_p_ensemble_sorted).index(correct_key) + 1\n",
        "                kr_ensemble[index] += kr_position\n",
        "                krs_ensemble[run][index] = kr_position\n",
        "\n",
        "                kr_position = list(key_p_ensemble_best_models_sorted).index(correct_key) + 1\n",
        "                kr_ensemble_best_models[index] += kr_position\n",
        "                krs_ensemble_best_models[run][index] = kr_position\n",
        "\n",
        "            print(\"Run {} - GE {} models: {} | GE {} models: {} | \".format(run, self.number_of_models,\n",
        "                                                                           int(kr_ensemble[kr_nt - 1] / (run + 1)),\n",
        "                                                                           self.number_of_best_models,\n",
        "                                                                           int(kr_ensemble_best_models[kr_nt - 1] / (run + 1))))\n",
        "\n",
        "        ge_ensemble = kr_ensemble / 100\n",
        "        ge_ensemble_best_models = kr_ensemble_best_models / 100\n",
        "\n",
        "        sr_ensemble = np.zeros(kr_nt)\n",
        "        sr_ensemble_best_models = np.zeros(kr_nt)\n",
        "\n",
        "        for index in range(kr_nt):\n",
        "            for run in range(100):\n",
        "                sr_ensemble[index] += self.__add_if_one(krs_ensemble[run][index])\n",
        "                sr_ensemble_best_models[index] += self.__add_if_one(krs_ensemble_best_models[run][index])\n",
        "\n",
        "        return ge_ensemble, ge_ensemble_best_models, sr_ensemble/100, sr_ensemble_best_models/100\n",
        "\n",
        "    def create_z_score_norm(self, dataset):\n",
        "        z_score_mean = np.mean(dataset, axis=0)\n",
        "        z_score_std = np.std(dataset, axis=0)\n",
        "        return z_score_mean, z_score_std\n",
        "\n",
        "    def apply_z_score_norm(self, dataset, z_score_mean, z_score_std):\n",
        "        for index in range(len(dataset)):\n",
        "            dataset[index] = (dataset[index] - z_score_mean) / z_score_std\n",
        "\n",
        "    def run_ensemble(self, number_of_models, number_of_best_models):\n",
        "\n",
        "        self.number_of_models = number_of_models\n",
        "        self.number_of_best_models = number_of_best_models\n",
        "\n",
        "        target_params = SCADatasets().get_trace_set(self.target_dataset)\n",
        "\n",
        "        root_folder = \"/content/drive/MyDrive/268/EnsembleSCA/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/\"\n",
        "\n",
        "        (X_profiling, Y_profiling), (X_validation, Y_validation), (X_attack, Y_attack), (\n",
        "            _, plt_validation, plt_attack) = LoadDatasets().load_dataset(\n",
        "            root_folder + target_params[\"file\"], target_params[\"n_profiling\"], target_params[\"n_attack\"], self.target_byte, self.l_model)\n",
        "\n",
        "        # normalize with z-score\n",
        "        z_score_mean, z_score_std = self.create_z_score_norm(X_profiling)\n",
        "        self.apply_z_score_norm(X_profiling, z_score_mean, z_score_std)\n",
        "        self.apply_z_score_norm(X_validation, z_score_mean, z_score_std)\n",
        "        self.apply_z_score_norm(X_attack, z_score_mean, z_score_std)\n",
        "\n",
        "        # convert labels to categorical labels\n",
        "        Y_profiling = to_categorical(Y_profiling, num_classes=self.classes)\n",
        "        Y_validation = to_categorical(Y_validation, num_classes=self.classes)\n",
        "        Y_attack = to_categorical(Y_attack, num_classes=self.classes)\n",
        "\n",
        "        X_profiling = X_profiling.astype('float32')\n",
        "        X_validation = X_validation.astype('float32')\n",
        "        X_attack = X_attack.astype('float32')\n",
        "\n",
        "        kr_step = 10  # key rank processed for each kr_step traces\n",
        "        kr_fraction = 1  # validation or attack sets are divided by kr_fraction before computing key rank\n",
        "\n",
        "        self.ge_all_validation = []\n",
        "        self.sr_all_validation = []\n",
        "        self.ge_all_attack = []\n",
        "        self.k_ps_all = []\n",
        "\n",
        "        kr_nt = int(len(X_validation) / (kr_step * kr_fraction))\n",
        "\n",
        "        # train random MLP\n",
        "        # for model_index in range(self.number_of_models):\n",
        "        #    ge_validation, ge_attack, sr_validation, sr_attack, kp_krs = self.run_mlp(X_profiling, Y_profiling,\n",
        "        #                                                                              X_validation, Y_validation,\n",
        "        #                                                                              X_attack, Y_attack,\n",
        "        #                                                                              plt_validation, plt_attack,\n",
        "        #                                                                              target_params, kr_step, kr_fraction)\n",
        "        #    self.ge_all_validation.append(ge_validation)\n",
        "        #    self.ge_all_attack.append(ge_attack)\n",
        "        #    self.sr_all_validation.append(sr_validation)\n",
        "        #    self.sr_all_attack.append(sr_attack)\n",
        "        #    self.k_ps_all.append(kp_krs)\n",
        "\n",
        "        # train random CNN\n",
        "        # for model_index in range(self.number_of_models):\n",
        "        #    ge_validation, ge_attack, sr_validation, sr_attack, kp_krs = self.run_cnn(X_profiling, Y_profiling,\n",
        "        #                                                                              X_validation, Y_validation,\n",
        "        #                                                                              X_attack, Y_attack,\n",
        "        #                                                                              plt_validation, plt_attack,\n",
        "        #                                                                              target_params, kr_step, kr_fraction)\n",
        "        #    self.ge_all_validation.append(ge_validation)\n",
        "        #    self.ge_all_attack.append(ge_attack)\n",
        "        #    self.sr_all_validation.append(sr_validation)\n",
        "        #    self.sr_all_attack.append(sr_attack)\n",
        "        #    self.k_ps_all.append(kp_krs)\n",
        "            \n",
        "        for model_index in range(self.number_of_models):\n",
        "            ge_validation, ge_attack, sr_validation, sr_attack, kp_krs = self.run_lstm(X_profiling, Y_profiling,\n",
        "                                                                                      X_validation, Y_validation,\n",
        "                                                                                      X_attack, Y_attack,\n",
        "                                                                                      plt_validation, plt_attack,\n",
        "                                                                                      target_params, kr_step, kr_fraction)\n",
        "            self.ge_all_validation.append(ge_validation)\n",
        "            self.ge_all_attack.append(ge_attack)\n",
        "            self.sr_all_validation.append(sr_validation)\n",
        "            self.sr_all_attack.append(sr_attack)\n",
        "            self.k_ps_all.append(kp_krs)\n",
        "\n",
        "        ge_ensemble, ge_ensemble_best_models, sr_ensemble, sr_ensemble_best_models = self.compute_ensembles(kr_nt,\n",
        "                                                                                                            target_params[\"good_key\"])\n",
        "\n",
        "        self.ge_ensemble = ge_ensemble\n",
        "        self.ge_ensemble_best_models = ge_ensemble_best_models\n",
        "        self.sr_ensemble = sr_ensemble\n",
        "        self.sr_ensemble_best_models = sr_ensemble_best_models\n",
        "\n",
        "    def get_ge_ensemble(self):\n",
        "        return self.ge_ensemble\n",
        "\n",
        "    def get_ge_ensemble_best_models(self):\n",
        "        return self.ge_ensemble_best_models\n",
        "\n",
        "    def get_ge_best_model_validation(self):\n",
        "        return self.ge_best_model_validation\n",
        "\n",
        "    def get_ge_best_model_attack(self):\n",
        "        return self.ge_best_model_attack\n",
        "\n",
        "    def get_sr_ensemble(self):\n",
        "        return self.sr_ensemble\n",
        "\n",
        "    def get_sr_ensemble_best_models(self):\n",
        "        return self.sr_ensemble_best_models\n",
        "\n",
        "    def get_sr_best_model_validation(self):\n",
        "        return self.sr_best_model_validation\n",
        "\n",
        "    def get_sr_best_model_attack(self):\n",
        "        return self.sr_best_model_attack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7nHYX488rSc",
        "outputId": "54209c5e-7e1d-4667-9859-60eccfda46c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting commons/ensemble_aes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/268/EnsembleSCA/run_ensemble.py\n",
        "\n",
        "from commons.ensemble_aes import EnsembleAES\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "ensemble_aes = EnsembleAES()\n",
        "ensemble_aes.set_dataset(\"ascad_fixed_key\")  # \"ascad_fixed_key\", \"ascad_random_key\" or \"ches_ctf\"\n",
        "ensemble_aes.set_leakage_model(\"HW\")\n",
        "ensemble_aes.set_target_byte(2)\n",
        "ensemble_aes.set_mini_batch(400)\n",
        "ensemble_aes.set_epochs(10)\n",
        "ensemble_aes.run_ensemble(\n",
        "    number_of_models=4,\n",
        "    number_of_best_models=2\n",
        ")\n",
        "\n",
        "# plotting GE and SR\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ensemble_aes.get_ge_best_model_validation(), label=\"GE best validation\")\n",
        "plt.plot(ensemble_aes.get_ge_best_model_attack(), label=\"GE best attack\")\n",
        "plt.plot(ensemble_aes.get_ge_ensemble(), label=\"GE Ensemble All Models\")\n",
        "plt.plot(ensemble_aes.get_ge_ensemble_best_models(), label=\"GE Ensemble Best Models\")\n",
        "plt.xlabel(\"Traces\")\n",
        "plt.ylabel(\"Guessing Entropy\")\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ensemble_aes.get_sr_best_model_validation(), label=\"SR best validation\")\n",
        "plt.plot(ensemble_aes.get_sr_best_model_attack(), label=\"SR best attack\")\n",
        "plt.plot(ensemble_aes.get_sr_ensemble(), label=\"SR Ensemble All Models\")\n",
        "plt.plot(ensemble_aes.get_sr_ensemble_best_models(), label=\"SR Ensemble Best Models\")\n",
        "plt.xlabel(\"Traces\")\n",
        "plt.ylabel(\"Success Rate\")\n",
        "plt.legend()\n",
        "plt.savefig(\"yyz_cnn_plot\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfdXatCz86r4",
        "outputId": "95c4748c-e859-4afb-d66d-e4f02b43ed1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/268/EnsembleSCA/run_ensemble.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('yyz_cnn_plot.png')"
      ],
      "metadata": {
        "id": "981sTg5E90xz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "3c9ceeff-087b-490f-a92a-a6ee8bd5cd3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEo5TqVaQ0Z-",
        "outputId": "eaeacf81-e10e-4242-e72a-ce80c98bdff6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refresh index: 100% (9/9), done.\n",
            "On branch 268/feature/yyz\n",
            "Your branch is up to date with 'origin/268/feature/yyz'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   commons/custom_loss.py\u001b[m\n",
            "\t\u001b[31mmodified:   commons/ensemble_aes.py\u001b[m\n",
            "\t\u001b[31mmodified:   run_ensemble.py\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git diff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URcGxCyLRo1Z",
        "outputId": "f568f580-ae68-4b3b-ab1d-aa95c893b32f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mdiff --git a/commons/custom_loss.py b/commons/custom_loss.py\u001b[m\n",
            "\u001b[1mold mode 100755\u001b[m\n",
            "\u001b[1mnew mode 100644\u001b[m\n",
            "\u001b[1mdiff --git a/commons/ensemble_aes.py b/commons/ensemble_aes.py\u001b[m\n",
            "\u001b[1mindex 6fabf7b..194216e 100644\u001b[m\n",
            "\u001b[1m--- a/commons/ensemble_aes.py\u001b[m\n",
            "\u001b[1m+++ b/commons/ensemble_aes.py\u001b[m\n",
            "\u001b[36m@@ -1,3 +1,4 @@\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            " from tensorflow.keras import backend as backend\u001b[m\n",
            " from tensorflow.keras.utils import to_categorical\u001b[m\n",
            " from commons.neural_networks import NeuralNetwork\u001b[m\n",
            "\u001b[36m@@ -152,8 +153,8 @@\u001b[m \u001b[mclass EnsembleAES:\u001b[m\n",
            "         mini_batch = random.randrange(500, 1000, 100)\u001b[m\n",
            "         learning_rate = random.uniform(0.0001, 0.001)\u001b[m\n",
            "         activation = ['relu', 'tanh', 'elu', 'selu'][random.randint(0, 3)]\u001b[m\n",
            "\u001b[31m-        layers = random.randrange(2, 8, 1)\u001b[m\n",
            "\u001b[31m-        neurons = random.randrange(500, 800, 100)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        layers = random.randrange(2, 4, 1)\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        neurons = random.randrange(100, 400, 100)\u001b[m\n",
            " \u001b[m\n",
            "         model = NeuralNetwork().lstm_random(self.classes, params[\"number_of_samples\"], activation, neurons, layers, learning_rate)\u001b[m\n",
            "         model.fit(\u001b[m\n",
            "\u001b[36m@@ -247,7 +248,7 @@\u001b[m \u001b[mclass EnsembleAES:\u001b[m\n",
            " \u001b[m\n",
            "         target_params = SCADatasets().get_trace_set(self.target_dataset)\u001b[m\n",
            " \u001b[m\n",
            "\u001b[31m-        root_folder = \"/home/yiy003/private/ECE268/EnsembleSCA/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/\"\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        root_folder = \"/content/drive/MyDrive/268/EnsembleSCA/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/\"\u001b[m\n",
            " \u001b[m\n",
            "         (X_profiling, Y_profiling), (X_validation, Y_validation), (X_attack, Y_attack), (\u001b[m\n",
            "             _, plt_validation, plt_attack) = LoadDatasets().load_dataset(\u001b[m\n",
            "\u001b[1mdiff --git a/run_ensemble.py b/run_ensemble.py\u001b[m\n",
            "\u001b[1mindex f065e8a..5bb5b92 100644\u001b[m\n",
            "\u001b[1m--- a/run_ensemble.py\u001b[m\n",
            "\u001b[1m+++ b/run_ensemble.py\u001b[m\n",
            "\u001b[36m@@ -1,3 +1,4 @@\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            " from commons.ensemble_aes import EnsembleAES\u001b[m\n",
            " import matplotlib.pyplot as plt\u001b[m\n",
            " \u001b[m\n",
            "\u001b[36m@@ -12,8 +13,8 @@\u001b[m \u001b[mensemble_aes.set_target_byte(2)\u001b[m\n",
            " ensemble_aes.set_mini_batch(400)\u001b[m\n",
            " ensemble_aes.set_epochs(10)\u001b[m\n",
            " ensemble_aes.run_ensemble(\u001b[m\n",
            "\u001b[31m-    number_of_models=10,\u001b[m\n",
            "\u001b[31m-    number_of_best_models=7\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    number_of_models=4,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    number_of_best_models=2\u001b[m\n",
            " )\u001b[m\n",
            " \u001b[m\n",
            " # plotting GE and SR\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/268/EnsembleSCA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42jd6EcSRsh6",
        "outputId": "4231e324-d4fe-4db7-d1b2-5e46e72fccd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/268/EnsembleSCA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "E-100uzcSFQT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Changed LSTM parameters. Generated entropy and success rate plots successfully\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkro2QRKS4mH",
        "outputId": "63489e96-9df8-4ad3-c393-807474a007c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[268/feature/yyz 1c61b20] Changed LSTM parameters. Generated entropy and success rate plots successfully\n",
            " 3 files changed, 7 insertions(+), 5 deletions(-)\n",
            " mode change 100755 => 100644 commons/custom_loss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"yalex778@163.com\""
      ],
      "metadata": {
        "id": "ulYASLrITJK-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"yyz\""
      ],
      "metadata": {
        "id": "wAgM_jSCTQ0u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push https://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@github.com/yyu233/EnsembleSCA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cctC7v5QTV0w",
        "outputId": "022cf903-e046-4a91-d1f8-94b5a1ec0a74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 9, done.\n",
            "Counting objects:  11% (1/9)\rCounting objects:  22% (2/9)\rCounting objects:  33% (3/9)\rCounting objects:  44% (4/9)\rCounting objects:  55% (5/9)\rCounting objects:  66% (6/9)\rCounting objects:  77% (7/9)\rCounting objects:  88% (8/9)\rCounting objects: 100% (9/9)\rCounting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  20% (1/5)\rCompressing objects:  40% (2/5)\rCompressing objects:  60% (3/5)\rCompressing objects:  80% (4/5)\rCompressing objects: 100% (5/5)\rCompressing objects: 100% (5/5), done.\n",
            "Writing objects:  20% (1/5)\rWriting objects:  40% (2/5)\rWriting objects:  60% (3/5)\rWriting objects:  80% (4/5)\rWriting objects: 100% (5/5)\rWriting objects: 100% (5/5), 536 bytes | 67.00 KiB/s, done.\n",
            "Total 5 (delta 4), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/yyu233/EnsembleSCA.git\n",
            "   81440aa..1c61b20  268/feature/yyz -> 268/feature/yyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_HCRtgSTZig"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "173DBEcIVwk2ZmbOYd5XygxCzYedo9uiu",
      "authorship_tag": "ABX9TyN+dqCFHYn0sacX7CZdpKPH",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}